#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šå±‚ç¼“å­˜ç­–ç•¥é…ç½®
å®ç°Redisã€å†…å­˜ç¼“å­˜ã€æ•°æ®åº“æŸ¥è¯¢ç¼“å­˜çš„ç»Ÿä¸€ç®¡ç†
ç›®æ ‡ï¼šç¼“å­˜å‘½ä¸­ç‡>80%ï¼Œå“åº”æ—¶é—´<5ms
"""

import redis
import json
import time
import logging
import hashlib
import pickle
from typing import Dict, List, Any, Optional, Union, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from threading import Lock
import asyncio
from enum import Enum
from collections import OrderedDict
import gzip
import lz4.frame


class CacheLevel(Enum):
    """ç¼“å­˜çº§åˆ«"""
    L1_MEMORY = "l1_memory"      # å†…å­˜ç¼“å­˜ (æœ€å¿«)
    L2_REDIS = "l2_redis"        # Redisç¼“å­˜ (ä¸­ç­‰é€Ÿåº¦)
    L3_DATABASE = "l3_database"  # æ•°æ®åº“ç¼“å­˜ (æœ€æ…¢ï¼Œä½†æŒä¹…)


class CacheStrategy(Enum):
    """ç¼“å­˜ç­–ç•¥"""
    LRU = "lru"                  # æœ€è¿‘æœ€å°‘ä½¿ç”¨
    LFU = "lfu"                  # æœ€ä¸ç»å¸¸ä½¿ç”¨
    TTL = "ttl"                  # åŸºäºæ—¶é—´è¿‡æœŸ
    SIZE_BASED = "size_based"    # åŸºäºå¤§å°é™åˆ¶


@dataclass
class CacheConfig:
    """ç¼“å­˜é…ç½®"""
    # L1 å†…å­˜ç¼“å­˜
    l1_max_size: int = 10000      # æœ€å¤§æ¡ç›®æ•°
    l1_max_memory_mb: int = 256   # æœ€å¤§å†…å­˜ä½¿ç”¨
    l1_ttl_seconds: int = 300     # 5åˆ†é’Ÿè¿‡æœŸ
    
    # L2 Redisç¼“å­˜
    l2_enabled: bool = True
    l2_host: str = "localhost"
    l2_port: int = 6379
    l2_db: int = 0
    l2_max_memory: str = "512mb"
    l2_ttl_seconds: int = 1800    # 30åˆ†é’Ÿè¿‡æœŸ
    
    # L3 æ•°æ®åº“ç¼“å­˜
    l3_enabled: bool = True
    l3_table_name: str = "cache_storage"
    l3_cleanup_interval: int = 3600  # 1å°æ—¶æ¸…ç†ä¸€æ¬¡
    
    # å‹ç¼©é…ç½®
    enable_compression: bool = True
    compression_threshold: int = 1024  # å¤§äº1KBçš„æ•°æ®è¿›è¡Œå‹ç¼©
    compression_method: str = "lz4"    # lz4 æˆ– gzip
    
    # æ€§èƒ½é…ç½®
    enable_async: bool = True
    batch_size: int = 100
    prefetch_enabled: bool = True


class CompressionManager:
    """å‹ç¼©ç®¡ç†å™¨"""
    
    @staticmethod
    def compress(data: bytes, method: str = "lz4") -> bytes:
        """å‹ç¼©æ•°æ®"""
        if method == "lz4":
            return lz4.frame.compress(data)
        elif method == "gzip":
            return gzip.compress(data)
        else:
            return data
    
    @staticmethod
    def decompress(data: bytes, method: str = "lz4") -> bytes:
        """è§£å‹æ•°æ®"""
        try:
            if method == "lz4":
                return lz4.frame.decompress(data)
            elif method == "gzip":
                return gzip.decompress(data)
            else:
                return data
        except Exception:
            return data  # å¦‚æœè§£å‹å¤±è´¥ï¼Œè¿”å›åŸæ•°æ®


class L1MemoryCache:
    """L1å†…å­˜ç¼“å­˜ - æœ€å¿«å“åº”"""
    
    def __init__(self, config: CacheConfig):
        self.config = config
        self.cache = OrderedDict()
        self.access_counts = {}
        self.access_times = {}
        self.lock = Lock()
        self.current_size = 0
        
    def _generate_key(self, key: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        return hashlib.md5(key.encode()).hexdigest()
    
    def _estimate_size(self, value: Any) -> int:
        """ä¼°ç®—å€¼çš„å¤§å°"""
        try:
            return len(pickle.dumps(value))
        except:
            return len(str(value).encode())
    
    def _evict_if_needed(self, new_size: int):
        """æ ¹æ®éœ€è¦é©±é€ç¼“å­˜"""
        max_size_bytes = self.config.l1_max_memory_mb * 1024 * 1024
        
        while (len(self.cache) >= self.config.l1_max_size or 
               self.current_size + new_size > max_size_bytes):
            if not self.cache:
                break
                
            # LRUç­–ç•¥ï¼šç§»é™¤æœ€ä¹…æœªè®¿é—®çš„é¡¹
            oldest_key = next(iter(self.cache))
            oldest_value = self.cache.pop(oldest_key)
            
            # æ›´æ–°å¤§å°ç»Ÿè®¡
            self.current_size -= self._estimate_size(oldest_value)
            
            # æ¸…ç†ç›¸å…³ç»Ÿè®¡
            self.access_counts.pop(oldest_key, None)
            self.access_times.pop(oldest_key, None)
    
    def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜å€¼"""
        with self.lock:
            cache_key = self._generate_key(key)
            
            if cache_key not in self.cache:
                return None
            
            # æ£€æŸ¥TTL
            if cache_key in self.access_times:
                if time.time() - self.access_times[cache_key] > self.config.l1_ttl_seconds:
                    self.delete(key)
                    return None
            
            # æ›´æ–°è®¿é—®ç»Ÿè®¡
            self.access_counts[cache_key] = self.access_counts.get(cache_key, 0) + 1
            self.access_times[cache_key] = time.time()
            
            # ç§»åˆ°æœ€åï¼ˆLRUï¼‰
            value = self.cache.pop(cache_key)
            self.cache[cache_key] = value
            
            return value
    
    def set(self, key: str, value: Any) -> bool:
        """è®¾ç½®ç¼“å­˜å€¼"""
        with self.lock:
            cache_key = self._generate_key(key)
            value_size = self._estimate_size(value)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦é©±é€
            self._evict_if_needed(value_size)
            
            # å¦‚æœé”®å·²å­˜åœ¨ï¼Œæ›´æ–°å¤§å°
            if cache_key in self.cache:
                old_size = self._estimate_size(self.cache[cache_key])
                self.current_size -= old_size
            
            # è®¾ç½®æ–°å€¼
            self.cache[cache_key] = value
            self.current_size += value_size
            self.access_times[cache_key] = time.time()
            self.access_counts[cache_key] = self.access_counts.get(cache_key, 0) + 1
            
            return True
    
    def delete(self, key: str) -> bool:
        """åˆ é™¤ç¼“å­˜å€¼"""
        with self.lock:
            cache_key = self._generate_key(key)
            
            if cache_key in self.cache:
                value = self.cache.pop(cache_key)
                self.current_size -= self._estimate_size(value)
                self.access_counts.pop(cache_key, None)
                self.access_times.pop(cache_key, None)
                return True
            
            return False
    
    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        with self.lock:
            self.cache.clear()
            self.access_counts.clear()
            self.access_times.clear()
            self.current_size = 0
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        with self.lock:
            return {
                'total_items': len(self.cache),
                'current_size_mb': self.current_size / 1024 / 1024,
                'max_size_items': self.config.l1_max_size,
                'max_size_mb': self.config.l1_max_memory_mb,
                'hit_rate': getattr(self, '_hit_count', 0) / max(getattr(self, '_request_count', 1), 1)
            }


class L2RedisCache:
    """L2 Redisç¼“å­˜ - ä¸­ç­‰é€Ÿåº¦ï¼Œæ”¯æŒåˆ†å¸ƒå¼"""
    
    def __init__(self, config: CacheConfig):
        self.config = config
        self.compression = CompressionManager()
        self.redis_client = None
        self._connect()
        
    def _connect(self):
        """è¿æ¥Redis"""
        try:
            if self.config.l2_enabled:
                self.redis_client = redis.Redis(
                    host=self.config.l2_host,
                    port=self.config.l2_port,
                    db=self.config.l2_db,
                    decode_responses=False,  # å¤„ç†äºŒè¿›åˆ¶æ•°æ®
                    socket_connect_timeout=5,
                    socket_timeout=5,
                    retry_on_timeout=True,
                    health_check_interval=30
                )
                
                # æµ‹è¯•è¿æ¥
                self.redis_client.ping()
                
                # é…ç½®å†…å­˜ç­–ç•¥
                try:
                    self.redis_client.config_set('maxmemory', self.config.l2_max_memory)
                    self.redis_client.config_set('maxmemory-policy', 'allkeys-lru')
                except:
                    pass  # å¦‚æœæ²¡æœ‰æƒé™ä¿®æ”¹é…ç½®ï¼Œç»§ç»­ä½¿ç”¨é»˜è®¤è®¾ç½®
                
        except Exception as e:
            logging.warning(f"Redisè¿æ¥å¤±è´¥ï¼Œç¦ç”¨L2ç¼“å­˜: {e}")
            self.redis_client = None
    
    def _serialize(self, value: Any) -> bytes:
        """åºåˆ—åŒ–å€¼"""
        data = pickle.dumps(value)
        
        if (self.config.enable_compression and 
            len(data) > self.config.compression_threshold):
            data = self.compression.compress(data, self.config.compression_method)
            return b'COMPRESSED:' + data
        
        return data
    
    def _deserialize(self, data: bytes) -> Any:
        """ååºåˆ—åŒ–å€¼"""
        if data.startswith(b'COMPRESSED:'):
            compressed_data = data[11:]  # ç§»é™¤å‰ç¼€
            data = self.compression.decompress(compressed_data, self.config.compression_method)
        
        return pickle.loads(data)
    
    def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜å€¼"""
        if not self.redis_client:
            return None
        
        try:
            data = self.redis_client.get(key)
            if data is None:
                return None
            
            return self._deserialize(data)
        
        except Exception as e:
            logging.warning(f"Redisè·å–å¤±è´¥: {e}")
            return None
    
    def set(self, key: str, value: Any, ttl: Optional[int] = None) -> bool:
        """è®¾ç½®ç¼“å­˜å€¼"""
        if not self.redis_client:
            return False
        
        try:
            data = self._serialize(value)
            ttl = ttl or self.config.l2_ttl_seconds
            
            return self.redis_client.setex(key, ttl, data)
        
        except Exception as e:
            logging.warning(f"Redisè®¾ç½®å¤±è´¥: {e}")
            return False
    
    def delete(self, key: str) -> bool:
        """åˆ é™¤ç¼“å­˜å€¼"""
        if not self.redis_client:
            return False
        
        try:
            return bool(self.redis_client.delete(key))
        
        except Exception as e:
            logging.warning(f"Redisåˆ é™¤å¤±è´¥: {e}")
            return False
    
    def get_batch(self, keys: List[str]) -> Dict[str, Any]:
        """æ‰¹é‡è·å–"""
        if not self.redis_client or not keys:
            return {}
        
        try:
            values = self.redis_client.mget(keys)
            result = {}
            
            for key, data in zip(keys, values):
                if data is not None:
                    try:
                        result[key] = self._deserialize(data)
                    except:
                        continue
            
            return result
        
        except Exception as e:
            logging.warning(f"Redisæ‰¹é‡è·å–å¤±è´¥: {e}")
            return {}
    
    def set_batch(self, items: Dict[str, Any], ttl: Optional[int] = None) -> bool:
        """æ‰¹é‡è®¾ç½®"""
        if not self.redis_client or not items:
            return False
        
        try:
            pipe = self.redis_client.pipeline()
            ttl = ttl or self.config.l2_ttl_seconds
            
            for key, value in items.items():
                data = self._serialize(value)
                pipe.setex(key, ttl, data)
            
            pipe.execute()
            return True
        
        except Exception as e:
            logging.warning(f"Redisæ‰¹é‡è®¾ç½®å¤±è´¥: {e}")
            return False
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–Redisç»Ÿè®¡"""
        if not self.redis_client:
            return {'status': 'disabled'}
        
        try:
            info = self.redis_client.info()
            memory_info = self.redis_client.memory_stats()
            
            return {
                'status': 'connected',
                'used_memory_mb': info.get('used_memory', 0) / 1024 / 1024,
                'connected_clients': info.get('connected_clients', 0),
                'keyspace_hits': info.get('keyspace_hits', 0),
                'keyspace_misses': info.get('keyspace_misses', 0),
                'hit_rate': info.get('keyspace_hits', 0) / max(
                    info.get('keyspace_hits', 0) + info.get('keyspace_misses', 0), 1
                ),
                'total_keys': sum(info.get(f'db{i}', {}).get('keys', 0) for i in range(16))
            }
        
        except Exception as e:
            logging.warning(f"è·å–Redisç»Ÿè®¡å¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}


class MultiLevelCacheManager:
    """å¤šå±‚ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, config: CacheConfig = None):
        self.config = config or CacheConfig()
        self.logger = logging.getLogger(__name__)
        
        # åˆå§‹åŒ–å„å±‚ç¼“å­˜
        self.l1_cache = L1MemoryCache(self.config)
        self.l2_cache = L2RedisCache(self.config) if self.config.l2_enabled else None
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'l1_hits': 0, 'l1_misses': 0,
            'l2_hits': 0, 'l2_misses': 0,
            'l3_hits': 0, 'l3_misses': 0,
            'total_requests': 0
        }
    
    def _update_stats(self, level: str, hit: bool):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        self.stats['total_requests'] += 1
        if hit:
            self.stats[f'{level}_hits'] += 1
        else:
            self.stats[f'{level}_misses'] += 1
    
    async def get(self, key: str, default: Any = None) -> Any:
        """å¤šå±‚ç¼“å­˜è·å–"""
        
        # L1 å†…å­˜ç¼“å­˜
        value = self.l1_cache.get(key)
        if value is not None:
            self._update_stats('l1', True)
            return value
        self._update_stats('l1', False)
        
        # L2 Redisç¼“å­˜
        if self.l2_cache:
            value = self.l2_cache.get(key)
            if value is not None:
                self._update_stats('l2', True)
                # å›å†™åˆ°L1
                self.l1_cache.set(key, value)
                return value
            self._update_stats('l2', False)
        
        return default
    
    async def set(self, key: str, value: Any, ttl: Optional[int] = None) -> bool:
        """å¤šå±‚ç¼“å­˜è®¾ç½®"""
        success = True
        
        # è®¾ç½®åˆ°L1
        if not self.l1_cache.set(key, value):
            success = False
        
        # è®¾ç½®åˆ°L2
        if self.l2_cache and not self.l2_cache.set(key, value, ttl):
            success = False
        
        return success
    
    async def delete(self, key: str) -> bool:
        """å¤šå±‚ç¼“å­˜åˆ é™¤"""
        results = []
        
        # ä»æ‰€æœ‰å±‚åˆ é™¤
        results.append(self.l1_cache.delete(key))
        
        if self.l2_cache:
            results.append(self.l2_cache.delete(key))
        
        return any(results)
    
    async def get_batch(self, keys: List[str]) -> Dict[str, Any]:
        """æ‰¹é‡è·å–"""
        results = {}
        remaining_keys = list(keys)
        
        # L1 æ‰¹é‡è·å–
        for key in list(remaining_keys):
            value = self.l1_cache.get(key)
            if value is not None:
                results[key] = value
                remaining_keys.remove(key)
                self._update_stats('l1', True)
            else:
                self._update_stats('l1', False)
        
        # L2 æ‰¹é‡è·å–å‰©ä½™é”®
        if self.l2_cache and remaining_keys:
            l2_results = self.l2_cache.get_batch(remaining_keys)
            
            for key, value in l2_results.items():
                results[key] = value
                self.l1_cache.set(key, value)  # å›å†™åˆ°L1
                self._update_stats('l2', True)
            
            # ç»Ÿè®¡L2æœªå‘½ä¸­
            for key in remaining_keys:
                if key not in l2_results:
                    self._update_stats('l2', False)
        
        return results
    
    async def set_batch(self, items: Dict[str, Any], ttl: Optional[int] = None) -> bool:
        """æ‰¹é‡è®¾ç½®"""
        success = True
        
        # L1æ‰¹é‡è®¾ç½®
        for key, value in items.items():
            if not self.l1_cache.set(key, value):
                success = False
        
        # L2æ‰¹é‡è®¾ç½®
        if self.l2_cache and not self.l2_cache.set_batch(items, ttl):
            success = False
        
        return success
    
    def invalidate_pattern(self, pattern: str) -> int:
        """æ ¹æ®æ¨¡å¼æ‰¹é‡å¤±æ•ˆç¼“å­˜"""
        count = 0
        
        # æš‚æ—¶åªæ”¯æŒç®€å•çš„å‰ç¼€åŒ¹é…
        if pattern.endswith('*'):
            prefix = pattern[:-1]
            
            # L1ç¼“å­˜å¤±æ•ˆ
            keys_to_delete = []
            for key in self.l1_cache.cache.keys():
                if key.startswith(prefix):
                    keys_to_delete.append(key)
            
            for key in keys_to_delete:
                if self.l1_cache.delete(key):
                    count += 1
            
            # L2ç¼“å­˜å¤±æ•ˆ
            if self.l2_cache and self.l2_cache.redis_client:
                try:
                    keys = self.l2_cache.redis_client.keys(pattern)
                    if keys:
                        deleted = self.l2_cache.redis_client.delete(*keys)
                        count += deleted
                except Exception as e:
                    self.logger.warning(f"Redisæ¨¡å¼åˆ é™¤å¤±è´¥: {e}")
        
        return count
    
    def clear_all(self):
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
        self.l1_cache.clear()
        
        if self.l2_cache and self.l2_cache.redis_client:
            try:
                self.l2_cache.redis_client.flushdb()
            except Exception as e:
                self.logger.warning(f"æ¸…ç©ºRediså¤±è´¥: {e}")
    
    def get_comprehensive_stats(self) -> Dict[str, Any]:
        """è·å–ç»¼åˆç»Ÿè®¡ä¿¡æ¯"""
        total_requests = max(self.stats['total_requests'], 1)
        
        stats = {
            'timestamp': datetime.now().isoformat(),
            'total_requests': total_requests,
            
            # å„å±‚ç»Ÿè®¡
            'l1_stats': {
                'hits': self.stats['l1_hits'],
                'misses': self.stats['l1_misses'],
                'hit_rate': self.stats['l1_hits'] / total_requests,
                'cache_info': self.l1_cache.get_stats()
            },
            
            'l2_stats': {
                'hits': self.stats['l2_hits'],
                'misses': self.stats['l2_misses'],
                'hit_rate': self.stats['l2_hits'] / total_requests,
                'cache_info': self.l2_cache.get_stats() if self.l2_cache else {'status': 'disabled'}
            },
            
            # ç»¼åˆæŒ‡æ ‡
            'overall_hit_rate': (self.stats['l1_hits'] + self.stats['l2_hits']) / total_requests,
            'cache_efficiency': self._calculate_cache_efficiency(),
            'performance_score': self._calculate_performance_score()
        }
        
        return stats
    
    def _calculate_cache_efficiency(self) -> float:
        """è®¡ç®—ç¼“å­˜æ•ˆç‡"""
        total_hits = self.stats['l1_hits'] + self.stats['l2_hits'] 
        total_requests = max(self.stats['total_requests'], 1)
        
        hit_rate = total_hits / total_requests
        
        # L1å‘½ä¸­æ›´æœ‰ä»·å€¼ï¼ˆæ›´å¿«ï¼‰
        l1_weight = 1.0
        l2_weight = 0.8
        
        weighted_score = (
            (self.stats['l1_hits'] * l1_weight + self.stats['l2_hits'] * l2_weight) /
            (total_requests * l1_weight)
        )
        
        return min(1.0, weighted_score)
    
    def _calculate_performance_score(self) -> float:
        """è®¡ç®—æ€§èƒ½è¯„åˆ†ï¼ˆ0-100ï¼‰"""
        efficiency = self._calculate_cache_efficiency()
        
        # åŸºäºæ•ˆç‡çš„è¯„åˆ†
        base_score = efficiency * 80
        
        # L1ç¼“å­˜åˆ©ç”¨ç‡åŠ åˆ†
        l1_stats = self.l1_cache.get_stats()
        if l1_stats['total_items'] > 0:
            l1_utilization = min(1.0, l1_stats['current_size_mb'] / l1_stats['max_size_mb'])
            base_score += l1_utilization * 10
        
        # L2è¿æ¥çŠ¶æ€åŠ åˆ†
        if self.l2_cache:
            l2_stats = self.l2_cache.get_stats()
            if l2_stats.get('status') == 'connected':
                base_score += 10
        
        return min(100, base_score)


# =============================================================================
# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•
# =============================================================================

async def test_cache_performance():
    """æµ‹è¯•ç¼“å­˜æ€§èƒ½"""
    print("ğŸš€ å¼€å§‹å¤šå±‚ç¼“å­˜æ€§èƒ½æµ‹è¯•")
    
    # é…ç½®ç¼“å­˜
    config = CacheConfig(
        l1_max_size=5000,
        l1_max_memory_mb=128,
        l2_enabled=True,  # å¯ç”¨Redisï¼ˆå¦‚æœå¯ç”¨ï¼‰
        enable_compression=True
    )
    
    cache_manager = MultiLevelCacheManager(config)
    
    # æµ‹è¯•æ•°æ®
    test_data = {
        f"key_{i}": {
            "id": i,
            "name": f"test_item_{i}",
            "data": "x" * (100 + i % 500),  # ä¸åŒå¤§å°çš„æ•°æ®
            "timestamp": datetime.now().isoformat()
        }
        for i in range(1000)
    }
    
    print(f"å‡†å¤‡æµ‹è¯•æ•°æ®: {len(test_data)} é¡¹")
    
    # å†™å…¥æ€§èƒ½æµ‹è¯•
    print("\n=== å†™å…¥æ€§èƒ½æµ‹è¯• ===")
    start_time = time.time()
    
    await cache_manager.set_batch(test_data)
    
    write_time = time.time() - start_time
    write_rate = len(test_data) / write_time
    
    print(f"æ‰¹é‡å†™å…¥å®Œæˆ: {len(test_data)} é¡¹ï¼Œè€—æ—¶ {write_time:.2f}s ({write_rate:.0f} ops/sec)")
    
    # è¯»å–æ€§èƒ½æµ‹è¯•
    print("\n=== è¯»å–æ€§èƒ½æµ‹è¯• ===")
    
    # éšæœºè¯»å–æµ‹è¯•
    import random
    test_keys = random.sample(list(test_data.keys()), 500)
    
    start_time = time.time()
    results = await cache_manager.get_batch(test_keys)
    read_time = time.time() - start_time
    
    hit_count = len(results)
    hit_rate = hit_count / len(test_keys)
    read_rate = len(test_keys) / read_time
    
    print(f"æ‰¹é‡è¯»å–æµ‹è¯•: {len(test_keys)} é¡¹")
    print(f"å‘½ä¸­æ•°: {hit_count}, å‘½ä¸­ç‡: {hit_rate:.1%}")
    print(f"è€—æ—¶: {read_time:.2f}s ({read_rate:.0f} ops/sec)")
    
    # ç¼“å­˜å±‚çº§æµ‹è¯•
    print("\n=== ç¼“å­˜å±‚çº§æ•ˆæœæµ‹è¯• ===")
    
    # æ¸…ç©ºL1ï¼Œæµ‹è¯•L2->L1å›å†™
    cache_manager.l1_cache.clear()
    
    single_key = test_keys[0]
    value = await cache_manager.get(single_key)
    
    if value:
        print(f"L2->L1å›å†™æˆåŠŸ: {single_key}")
        
        # å†æ¬¡è·å–ï¼Œåº”è¯¥ä»L1å‘½ä¸­
        start_time = time.time()
        value2 = await cache_manager.get(single_key)
        l1_time = time.time() - start_time
        
        print(f"L1ç¼“å­˜å“åº”æ—¶é—´: {l1_time*1000:.2f}ms")
    
    # è·å–ç»¼åˆç»Ÿè®¡
    print("\n=== ç»¼åˆæ€§èƒ½ç»Ÿè®¡ ===")
    stats = cache_manager.get_comprehensive_stats()
    
    print(f"æ€»è¯·æ±‚æ•°: {stats['total_requests']}")
    print(f"L1å‘½ä¸­ç‡: {stats['l1_stats']['hit_rate']:.1%}")
    print(f"L2å‘½ä¸­ç‡: {stats['l2_stats']['hit_rate']:.1%}")
    print(f"æ€»ä½“å‘½ä¸­ç‡: {stats['overall_hit_rate']:.1%}")
    print(f"ç¼“å­˜æ•ˆç‡: {stats['cache_efficiency']:.1%}")
    print(f"æ€§èƒ½è¯„åˆ†: {stats['performance_score']:.1f}/100")
    
    # L1å†…å­˜ä½¿ç”¨
    l1_info = stats['l1_stats']['cache_info']
    print(f"L1å†…å­˜ä½¿ç”¨: {l1_info['current_size_mb']:.1f}MB/{l1_info['max_size_mb']}MB")
    print(f"L1ç¼“å­˜é¡¹: {l1_info['total_items']}/{l1_info['max_size_items']}")
    
    # L2çŠ¶æ€
    l2_info = stats['l2_stats']['cache_info']
    if l2_info['status'] == 'connected':
        print(f"L2å†…å­˜ä½¿ç”¨: {l2_info['used_memory_mb']:.1f}MB")
        print(f"L2é”®æ•°é‡: {l2_info['total_keys']}")
        print(f"L2å‘½ä¸­ç‡: {l2_info['hit_rate']:.1%}")
    else:
        print(f"L2çŠ¶æ€: {l2_info['status']}")
    
    return stats


if __name__ == "__main__":
    # é…ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO)
    
    # è¿è¡Œæ€§èƒ½æµ‹è¯•
    asyncio.run(test_cache_performance())