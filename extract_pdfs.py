#!/usr/bin/env python3
"""
é«˜æ•ˆæ‰¹é‡æ˜“å­¦PDFæ–‡ä»¶å¤„ç†è„šæœ¬
æ”¯æŒ200+PDFæ–‡ä»¶çš„å¹¶è¡Œå¤„ç†ã€æ–‡æœ¬æå–ã€è‡ªåŠ¨åˆ†ç±»å’Œæ•°æ®ç»“æ„åŒ–
"""

import os
import json
import re
import logging
from concurrent.futures import ProcessPoolExecutor, as_completed
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from datetime import datetime
import hashlib
import pickle
from collections import defaultdict

try:
    import pdfplumber
except ImportError:
    print("è¯·å®‰è£…pdfplumber: pip install pdfplumber")
    exit(1)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('pdf_processing.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class PDFInfo:
    """PDFæ–‡ä»¶ä¿¡æ¯"""
    file_path: str
    file_name: str
    file_size: int
    pages: int
    category: str
    confidence: float
    priority: int  # 1-5, 1æœ€é«˜
    processed_at: str
    text_length: int
    
@dataclass 
class ExtractedContent:
    """æå–çš„å†…å®¹"""
    hexagrams: List[Dict[str, Any]]  # 64å¦ä¿¡æ¯
    yao_ci: List[Dict[str, Any]]     # 384çˆ»è¾
    annotations: List[Dict[str, Any]] # æ³¨è§£
    cases: List[Dict[str, Any]]      # æ¡ˆä¾‹
    keywords: List[str]              # å…³é”®è¯
    author: Optional[str]            # ä½œè€…
    dynasty: Optional[str]           # æœä»£
    
class PDFProcessor:
    """PDFå¤„ç†å™¨"""
    
    def __init__(self, data_dir: str, output_dir: str):
        self.data_dir = Path(data_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        (self.output_dir / "raw_texts").mkdir(exist_ok=True)
        (self.output_dir / "structured_data").mkdir(exist_ok=True)
        (self.output_dir / "categories").mkdir(exist_ok=True)
        (self.output_dir / "cache").mkdir(exist_ok=True)
        
        # åˆ†ç±»æ¨¡å¼
        self.category_patterns = {
            "å…­çˆ»": {
                "keywords": ["å…­çˆ»", "åœæ˜“", "å¢åˆ ", "ç«ç æ—", "é»„é‡‘ç­–", "ç­®", "å¦è±¡", "çˆ»è¾"],
                "priority": 1,
                "patterns": [r"å…­çˆ»", r"ç­®\w*", r"å¦è±¡", r"çˆ»\w+", r"åŠ¨çˆ»", r"å˜çˆ»"]
            },
            "æ¢…èŠ±æ˜“æ•°": {
                "keywords": ["æ¢…èŠ±", "æ˜“æ•°", "æ¢…èŠ±æ˜“", "è§‚æ¢…", "æ•°ç†"],
                "priority": 2,
                "patterns": [r"æ¢…èŠ±\w*æ˜“\w*", r"è§‚æ¢…", r"æ˜“æ•°", r"æ•°ç†"]
            },
            "å¤§å…­å£¬": {
                "keywords": ["å…­å£¬", "å£¬å­¦", "å£¬å ", "è¯¾ä¼ ", "ç¥å°†", "åäºŒå°†"],
                "priority": 1,
                "patterns": [r"å…­å£¬", r"å£¬å ", r"è¯¾ä¼ ", r"ç¥å°†", r"åäºŒå°†"]
            },
            "ç´«å¾®æ–—æ•°": {
                "keywords": ["ç´«å¾®", "æ–—æ•°", "å‘½ç›˜", "å®«ä½", "æ˜Ÿæ›œ"],
                "priority": 2,
                "patterns": [r"ç´«å¾®\w*æ–—æ•°", r"å‘½ç›˜", r"å®«ä½", r"æ˜Ÿæ›œ"]
            },
            "å¥‡é—¨éç”²": {
                "keywords": ["å¥‡é—¨", "éç”²", "ä¹å®«", "å…«é—¨", "ç¥ç…"],
                "priority": 2,
                "patterns": [r"å¥‡é—¨\w*éç”²", r"ä¹å®«", r"å…«é—¨", r"ç¥ç…"]
            },
            "å…«å­—å‘½ç†": {
                "keywords": ["å…«å­—", "å››æŸ±", "å‘½ç†", "å¹²æ”¯", "çº³éŸ³"],
                "priority": 2,
                "patterns": [r"å…«å­—", r"å››æŸ±", r"å‘½ç†", r"å¹²æ”¯", r"çº³éŸ³"]
            },
            "é‡‘å£è¯€": {
                "keywords": ["é‡‘å£è¯€", "é‡‘å£", "è¯¾å¼"],
                "priority": 3,
                "patterns": [r"é‡‘å£\w*è¯€", r"é‡‘å£", r"è¯¾å¼"]
            },
            "å¤ªä¹™ç¥æ•°": {
                "keywords": ["å¤ªä¹™", "ç¥æ•°", "å¤ªä¹™ç¥æ•°"],
                "priority": 3,
                "patterns": [r"å¤ªä¹™\w*ç¥æ•°", r"å¤ªä¹™"]
            },
            "æ²³æ´›ç†æ•°": {
                "keywords": ["æ²³æ´›", "ç†æ•°", "æ²³å›¾", "æ´›ä¹¦"],
                "priority": 3,
                "patterns": [r"æ²³æ´›\w*ç†æ•°", r"æ²³å›¾", r"æ´›ä¹¦"]
            },
            "å‘¨æ˜“åŸºç¡€": {
                "keywords": ["å‘¨æ˜“", "æ˜“ç»", "å…«å¦", "å…­åå››å¦", "å¦è±¡"],
                "priority": 1,
                "patterns": [r"å‘¨æ˜“", r"æ˜“ç»", r"å…«å¦", r"å…­åå››å¦"]
            }
        }
        
        # 64å¦åç§°
        self.hexagram_names = [
            "ä¹¾", "å¤", "å±¯", "è’™", "éœ€", "è®¼", "å¸ˆ", "æ¯”",
            "å°ç•œ", "å±¥", "æ³°", "å¦", "åŒäºº", "å¤§æœ‰", "è°¦", "è±«",
            "éš", "è›Š", "ä¸´", "è§‚", "å™¬å—‘", "è´²", "å‰¥", "å¤",
            "æ— å¦„", "å¤§ç•œ", "é¢", "å¤§è¿‡", "å", "ç¦»", "å’¸", "æ’",
            "é", "å¤§å£®", "æ™‹", "æ˜å¤·", "å®¶äºº", "ç½", "è¹‡", "è§£",
            "æŸ", "ç›Š", "å¤¬", "å§¤", "èƒ", "å‡", "å›°", "äº•",
            "é©", "é¼", "éœ‡", "è‰®", "æ¸", "å½’å¦¹", "ä¸°", "æ—…",
            "å·½", "å…‘", "æ¶£", "èŠ‚", "ä¸­å­š", "å°è¿‡", "æ—¢æµ", "æœªæµ"
        ]
        
        # ç¼“å­˜æ–‡ä»¶è·¯å¾„
        self.cache_file = self.output_dir / "cache" / "processing_cache.pkl"
        self.processed_files = self.load_cache()
    
    def load_cache(self) -> Dict[str, Any]:
        """åŠ è½½ç¼“å­˜"""
        if self.cache_file.exists():
            try:
                with open(self.cache_file, 'rb') as f:
                    return pickle.load(f)
            except Exception as e:
                logger.warning(f"ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
        return {}
    
    def save_cache(self):
        """ä¿å­˜ç¼“å­˜"""
        try:
            with open(self.cache_file, 'wb') as f:
                pickle.dump(self.processed_files, f)
        except Exception as e:
            logger.error(f"ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
    
    def get_file_hash(self, file_path: Path) -> str:
        """è·å–æ–‡ä»¶å“ˆå¸Œ"""
        stat = file_path.stat()
        return hashlib.md5(f"{file_path}_{stat.st_size}_{stat.st_mtime}".encode()).hexdigest()
    
    def classify_pdf(self, text: str, file_name: str) -> Tuple[str, float, int]:
        """åˆ†ç±»PDFæ–‡ä»¶"""
        text_lower = text.lower()
        file_lower = file_name.lower()
        
        best_category = "å…¶ä»–"
        best_confidence = 0.0
        best_priority = 5
        
        for category, config in self.category_patterns.items():
            confidence = 0.0
            
            # æ–‡ä»¶ååŒ¹é…
            for keyword in config["keywords"]:
                if keyword in file_lower:
                    confidence += 2.0
            
            # å†…å®¹æ¨¡å¼åŒ¹é…
            for pattern in config["patterns"]:
                matches = len(re.findall(pattern, text, re.IGNORECASE))
                confidence += matches * 0.5
            
            # å…³é”®è¯å¯†åº¦
            total_keywords = sum(text_lower.count(kw) for kw in config["keywords"])
            if len(text) > 0:
                confidence += (total_keywords / len(text)) * 1000
            
            if confidence > best_confidence:
                best_confidence = confidence
                best_category = category
                best_priority = config["priority"]
        
        # å½’ä¸€åŒ–ç½®ä¿¡åº¦
        confidence_normalized = min(best_confidence / 10.0, 1.0)
        
        return best_category, confidence_normalized, best_priority
    
    def extract_hexagrams(self, text: str) -> List[Dict[str, Any]]:
        """æå–64å¦ä¿¡æ¯"""
        hexagrams = []
        
        for i, name in enumerate(self.hexagram_names):
            # æŸ¥æ‰¾å¦ååŠå…¶æè¿°
            pattern = rf"{name}[å¦]?[ï¼š:]\s*([^ã€‚]+[ã€‚]?)"
            matches = re.finditer(pattern, text)
            
            for match in matches:
                hexagram = {
                    "number": i + 1,
                    "name": name,
                    "description": match.group(1).strip(),
                    "position": match.start()
                }
                hexagrams.append(hexagram)
        
        return hexagrams
    
    def extract_yao_ci(self, text: str) -> List[Dict[str, Any]]:
        """æå–çˆ»è¾"""
        yao_positions = ["åˆ", "äºŒ", "ä¸‰", "å››", "äº”", "ä¸Š"]
        yao_types = ["å…­", "ä¹"]
        yao_ci = []
        
        for pos in yao_positions:
            for yao_type in yao_types:
                # åŒ¹é…çˆ»è¾æ ¼å¼ï¼šåˆå…­ã€ä¹äºŒç­‰
                pattern = rf"({pos}{yao_type})[ï¼š:]([^ã€‚]+[ã€‚]?)"
                matches = re.finditer(pattern, text)
                
                for match in matches:
                    yao = {
                        "position": pos,
                        "type": yao_type,
                        "full_name": match.group(1),
                        "text": match.group(2).strip(),
                        "location": match.start()
                    }
                    yao_ci.append(yao)
        
        return yao_ci
    
    def extract_annotations(self, text: str) -> List[Dict[str, Any]]:
        """æå–æ³¨è§£"""
        annotations = []
        
        # æ³¨è§£æ¨¡å¼
        annotation_patterns = [
            r"æ³¨[ï¼š:]([^ã€‚]+[ã€‚]?)",
            r"è§£[ï¼š:]([^ã€‚]+[ã€‚]?)",
            r"é‡Š[ï¼š:]([^ã€‚]+[ã€‚]?)",
            r"æŒ‰[ï¼š:]([^ã€‚]+[ã€‚]?)",
            r"æ›°[ï¼š:]([^ã€‚]+[ã€‚]?)"
        ]
        
        for pattern in annotation_patterns:
            matches = re.finditer(pattern, text)
            for match in matches:
                annotation = {
                    "type": match.group(0)[0],
                    "content": match.group(1).strip(),
                    "position": match.start()
                }
                annotations.append(annotation)
        
        return annotations
    
    def extract_cases(self, text: str) -> List[Dict[str, Any]]:
        """æå–æ¡ˆä¾‹"""
        cases = []
        
        # æ¡ˆä¾‹æ¨¡å¼
        case_patterns = [
            r"ä¾‹[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d+][ï¼š:]([^ã€‚]{20,}[ã€‚])",
            r"æ¡ˆä¾‹[ï¼š:]([^ã€‚]{20,}[ã€‚])",
            r"å®ä¾‹[ï¼š:]([^ã€‚]{20,}[ã€‚])",
            r"å ä¾‹[ï¼š:]([^ã€‚]{20,}[ã€‚])"
        ]
        
        for pattern in case_patterns:
            matches = re.finditer(pattern, text, re.DOTALL)
            for match in matches:
                case = {
                    "content": match.group(1).strip(),
                    "position": match.start(),
                    "length": len(match.group(1))
                }
                cases.append(case)
        
        return cases
    
    def extract_keywords(self, text: str) -> List[str]:
        """æå–å…³é”®è¯"""
        keywords = set()
        
        # å¸¸è§æ˜“å­¦æœ¯è¯­
        common_terms = [
            "é˜´é˜³", "äº”è¡Œ", "å…«å¦", "å…­çˆ»", "å åœ", "é¢„æµ‹", "å‘½ç†", 
            "é£æ°´", "å‘¨æ˜“", "å¤ªæ", "ç¥ç…", "åç¥", "å¤©å¹²", "åœ°æ”¯",
            "å¦è±¡", "çˆ»å˜", "åŠ¨çˆ»", "é™çˆ»", "ä¸–åº”", "å…­äº²", "ç”¨ç¥"
        ]
        
        for term in common_terms:
            if term in text:
                keywords.add(term)
        
        # æå–ä¸“æœ‰åè¯
        proper_nouns = re.findall(r'ã€Š([^ã€‹]+)ã€‹', text)
        keywords.update(proper_nouns[:10])  # é™åˆ¶æ•°é‡
        
        return sorted(list(keywords))
    
    def extract_author_dynasty(self, text: str, filename: str) -> Tuple[Optional[str], Optional[str]]:
        """æå–ä½œè€…å’Œæœä»£"""
        author = None
        dynasty = None
        
        # ä»æ–‡ä»¶åæå–
        author_match = re.search(r'([^-_\s]+)[_-]', filename)
        if author_match:
            potential_author = author_match.group(1)
            if not any(char.isdigit() for char in potential_author):
                author = potential_author
        
        # æœä»£æ¨¡å¼
        dynasty_patterns = [
            r'(å®‹|æ˜|æ¸…|å…ƒ|å”|æ±‰)[æœä»£]?',
            r'[(ï¼ˆ](å®‹|æ˜|æ¸…|å…ƒ|å”|æ±‰)[)ï¼‰]'
        ]
        
        for pattern in dynasty_patterns:
            match = re.search(pattern, text)
            if match:
                dynasty = match.group(1)
                break
        
        return author, dynasty
    
    def process_single_pdf(self, file_path: Path) -> Optional[Dict[str, Any]]:
        """å¤„ç†å•ä¸ªPDFæ–‡ä»¶"""
        try:
            # æ£€æŸ¥ç¼“å­˜
            file_hash = self.get_file_hash(file_path)
            if file_hash in self.processed_files:
                logger.info(f"è·³è¿‡å·²å¤„ç†æ–‡ä»¶: {file_path.name}")
                return self.processed_files[file_hash]
            
            logger.info(f"å¤„ç†æ–‡ä»¶: {file_path.name}")
            
            # æå–æ–‡æœ¬
            text = ""
            page_count = 0
            
            with pdfplumber.open(file_path) as pdf:
                page_count = len(pdf.pages)
                for page in pdf.pages:
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + "\n"
            
            if not text.strip():
                logger.warning(f"æ— æ³•æå–æ–‡æœ¬: {file_path.name}")
                return None
            
            # åˆ†ç±»
            category, confidence, priority = self.classify_pdf(text, file_path.name)
            
            # åˆ›å»ºPDFä¿¡æ¯
            pdf_info = PDFInfo(
                file_path=str(file_path),
                file_name=file_path.name,
                file_size=file_path.stat().st_size,
                pages=page_count,
                category=category,
                confidence=confidence,
                priority=priority,
                processed_at=datetime.now().isoformat(),
                text_length=len(text)
            )
            
            # æå–ç»“æ„åŒ–å†…å®¹
            extracted_content = ExtractedContent(
                hexagrams=self.extract_hexagrams(text),
                yao_ci=self.extract_yao_ci(text),
                annotations=self.extract_annotations(text),
                cases=self.extract_cases(text),
                keywords=self.extract_keywords(text),
                *self.extract_author_dynasty(text, file_path.name)
            )
            
            # æ„å»ºç»“æœ
            result = {
                "pdf_info": asdict(pdf_info),
                "content": asdict(extracted_content),
                "raw_text": text[:5000] + "..." if len(text) > 5000 else text  # æˆªå–å‰5000å­—ç¬¦
            }
            
            # ä¿å­˜åˆ°ç¼“å­˜
            self.processed_files[file_hash] = result
            
            # ä¿å­˜åŸå§‹æ–‡æœ¬
            text_file = self.output_dir / "raw_texts" / f"{file_path.stem}.txt"
            with open(text_file, 'w', encoding='utf-8') as f:
                f.write(text)
            
            return result
            
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path.name}: {e}")
            return None
    
    def process_all_pdfs(self, max_workers: int = 4) -> Dict[str, Any]:
        """å¹¶è¡Œå¤„ç†æ‰€æœ‰PDFæ–‡ä»¶"""
        pdf_files = list(self.data_dir.glob("*.pdf"))
        logger.info(f"æ‰¾åˆ° {len(pdf_files)} ä¸ªPDFæ–‡ä»¶")
        
        results = []
        failed_files = []
        
        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤ä»»åŠ¡
            future_to_file = {
                executor.submit(self.process_single_pdf, pdf_file): pdf_file 
                for pdf_file in pdf_files
            }
            
            # æ”¶é›†ç»“æœ
            for i, future in enumerate(as_completed(future_to_file)):
                pdf_file = future_to_file[future]
                try:
                    result = future.result()
                    if result:
                        results.append(result)
                        logger.info(f"å®Œæˆ {i+1}/{len(pdf_files)}: {pdf_file.name}")
                    else:
                        failed_files.append(pdf_file.name)
                except Exception as e:
                    logger.error(f"ä»»åŠ¡å¤±è´¥ {pdf_file.name}: {e}")
                    failed_files.append(pdf_file.name)
                
                # å®šæœŸä¿å­˜ç¼“å­˜
                if (i + 1) % 10 == 0:
                    self.save_cache()
        
        # æœ€ç»ˆä¿å­˜ç¼“å­˜
        self.save_cache()
        
        # æŒ‰ä¼˜å…ˆçº§å’Œç±»åˆ«ç»„ç»‡ç»“æœ
        categorized_results = defaultdict(list)
        priority_results = defaultdict(list)
        
        for result in results:
            category = result["pdf_info"]["category"]
            priority = result["pdf_info"]["priority"]
            
            categorized_results[category].append(result)
            priority_results[priority].append(result)
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            "total_files": len(pdf_files),
            "processed_successfully": len(results),
            "failed_files": len(failed_files),
            "categories": {cat: len(files) for cat, files in categorized_results.items()},
            "priorities": {f"ä¼˜å…ˆçº§{p}": len(files) for p, files in priority_results.items()},
            "failed_file_list": failed_files,
            "processing_time": datetime.now().isoformat()
        }
        
        return {
            "statistics": stats,
            "results": results,
            "by_category": dict(categorized_results),
            "by_priority": dict(priority_results)
        }
    
    def save_results(self, results: Dict[str, Any]):
        """ä¿å­˜å¤„ç†ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        full_results_file = self.output_dir / "structured_data" / f"complete_results_{timestamp}.json"
        with open(full_results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # æŒ‰ç±»åˆ«ä¿å­˜
        for category, files in results["by_category"].items():
            category_file = self.output_dir / "categories" / f"{category}_{timestamp}.json"
            with open(category_file, 'w', encoding='utf-8') as f:
                json.dump({
                    "category": category,
                    "file_count": len(files),
                    "files": files
                }, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats_file = self.output_dir / "structured_data" / f"statistics_{timestamp}.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(results["statistics"], f, ensure_ascii=False, indent=2)
        
        # ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
        self.generate_summary_report(results, timestamp)
        
        logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {self.output_dir}")
        logger.info(f"å®Œæ•´ç»“æœ: {full_results_file}")
        logger.info(f"ç»Ÿè®¡ä¿¡æ¯: {stats_file}")
    
    def generate_summary_report(self, results: Dict[str, Any], timestamp: str):
        """ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š"""
        stats = results["statistics"]
        
        report = f"""
# PDFå¤„ç†æ‘˜è¦æŠ¥å‘Š
æ—¶é—´: {timestamp}

## å¤„ç†ç»Ÿè®¡
- æ€»æ–‡ä»¶æ•°: {stats['total_files']}
- æˆåŠŸå¤„ç†: {stats['processed_successfully']}
- å¤±è´¥æ•°é‡: {stats['failed_files']}
- æˆåŠŸç‡: {stats['processed_successfully']/stats['total_files']*100:.1f}%

## åˆ†ç±»ç»Ÿè®¡
"""
        
        for category, count in stats["categories"].items():
            report += f"- {category}: {count} ä¸ªæ–‡ä»¶\n"
        
        report += "\n## ä¼˜å…ˆçº§åˆ†å¸ƒ\n"
        for priority, count in stats["priorities"].items():
            report += f"- {priority}: {count} ä¸ªæ–‡ä»¶\n"
        
        if stats["failed_files"]:
            report += f"\n## å¤±è´¥æ–‡ä»¶\n"
            for failed_file in stats["failed_file_list"]:
                report += f"- {failed_file}\n"
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.output_dir / f"processing_report_{timestamp}.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(report)

def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®è·¯å¾„
    data_dir = "/mnt/d/desktop/appp/data"
    output_dir = "/mnt/d/desktop/appp/structured_data"
    
    print("ğŸš€ å¼€å§‹æ‰¹é‡å¤„ç†æ˜“å­¦PDFæ–‡ä»¶...")
    print(f"ğŸ“‚ æ•°æ®ç›®å½•: {data_dir}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    
    # åˆ›å»ºå¤„ç†å™¨
    processor = PDFProcessor(data_dir, output_dir)
    
    # æ£€æŸ¥æ•°æ®ç›®å½•
    if not Path(data_dir).exists():
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        return
    
    # ç»Ÿè®¡PDFæ–‡ä»¶
    pdf_count = len(list(Path(data_dir).glob("*.pdf")))
    print(f"ğŸ“‹ æ‰¾åˆ° {pdf_count} ä¸ªPDFæ–‡ä»¶")
    
    if pdf_count == 0:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°PDFæ–‡ä»¶")
        return
    
    # å¼€å§‹å¤„ç†
    start_time = datetime.now()
    print(f"â° å¼€å§‹æ—¶é—´: {start_time}")
    
    try:
        # ä½¿ç”¨4ä¸ªè¿›ç¨‹å¹¶è¡Œå¤„ç†
        results = processor.process_all_pdfs(max_workers=4)
        
        # ä¿å­˜ç»“æœ
        processor.save_results(results)
        
        end_time = datetime.now()
        duration = end_time - start_time
        
        print(f"\nâœ… å¤„ç†å®Œæˆ!")
        print(f"â° è€—æ—¶: {duration}")
        print(f"ğŸ“Š æˆåŠŸå¤„ç†: {results['statistics']['processed_successfully']}/{results['statistics']['total_files']}")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {output_dir}")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­å¤„ç†")
        processor.save_cache()  # ä¿å­˜å·²å¤„ç†çš„ç¼“å­˜
    except Exception as e:
        print(f"\nâŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        logger.error(f"å¤„ç†å¼‚å¸¸: {e}", exc_info=True)

if __name__ == "__main__":
    main()