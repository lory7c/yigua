#!/usr/bin/env python3
"""
å¸¦è¿›åº¦æ¡çš„é«˜æ•ˆæ‰¹é‡æ˜“å­¦PDFæ–‡ä»¶å¤„ç†è„šæœ¬
æ”¯æŒ200+PDFæ–‡ä»¶çš„å¹¶è¡Œå¤„ç†ã€æ–‡æœ¬æå–ã€è‡ªåŠ¨åˆ†ç±»å’Œæ•°æ®ç»“æ„åŒ–
"""

import os
import json
import re
import logging
from concurrent.futures import ProcessPoolExecutor, as_completed
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from datetime import datetime
import hashlib
import pickle
from collections import defaultdict
import time

try:
    import pdfplumber
    from tqdm import tqdm
except ImportError:
    print("è¯·å®‰è£…ä¾èµ–: python install_dependencies.py")
    exit(1)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('pdf_processing.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class PDFInfo:
    """PDFæ–‡ä»¶ä¿¡æ¯"""
    file_path: str
    file_name: str
    file_size: int
    pages: int
    category: str
    confidence: float
    priority: int  # 1-5, 1æœ€é«˜
    processed_at: str
    text_length: int
    processing_time: float
    
@dataclass 
class ExtractedContent:
    """æå–çš„å†…å®¹"""
    hexagrams: List[Dict[str, Any]]  # 64å¦ä¿¡æ¯
    yao_ci: List[Dict[str, Any]]     # 384çˆ»è¾
    annotations: List[Dict[str, Any]] # æ³¨è§£
    cases: List[Dict[str, Any]]      # æ¡ˆä¾‹
    keywords: List[str]              # å…³é”®è¯
    author: Optional[str]            # ä½œè€…
    dynasty: Optional[str]           # æœä»£
    
class PDFProcessorWithProgress:
    """å¸¦è¿›åº¦æ˜¾ç¤ºçš„PDFå¤„ç†å™¨"""
    
    def __init__(self, data_dir: str, output_dir: str):
        self.data_dir = Path(data_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        (self.output_dir / "raw_texts").mkdir(exist_ok=True)
        (self.output_dir / "structured_data").mkdir(exist_ok=True)
        (self.output_dir / "categories").mkdir(exist_ok=True)
        (self.output_dir / "cache").mkdir(exist_ok=True)
        (self.output_dir / "reports").mkdir(exist_ok=True)
        
        # åˆ†ç±»æ¨¡å¼ - æ›´ç²¾ç¡®çš„åŒ¹é…
        self.category_patterns = {
            "å…­çˆ»": {
                "keywords": ["å…­çˆ»", "åœæ˜“", "å¢åˆ ", "ç«ç æ—", "é»„é‡‘ç­–", "ç­®", "å¦è±¡", "çˆ»è¾", "çˆ»å˜"],
                "priority": 1,
                "patterns": [r"å…­çˆ»", r"ç­®\w*", r"å¦è±¡", r"çˆ»\w+", r"åŠ¨çˆ»", r"å˜çˆ»", r"ä¸–åº”", r"å…­äº²", r"ç”¨ç¥"]
            },
            "æ¢…èŠ±æ˜“æ•°": {
                "keywords": ["æ¢…èŠ±", "æ˜“æ•°", "æ¢…èŠ±æ˜“", "è§‚æ¢…", "æ•°ç†", "å…ˆå¤©", "åå¤©"],
                "priority": 2,
                "patterns": [r"æ¢…èŠ±\w*æ˜“\w*", r"è§‚æ¢…", r"æ˜“æ•°", r"æ•°ç†", r"å…ˆå¤©\w*æ•°", r"åå¤©\w*æ•°"]
            },
            "å¤§å…­å£¬": {
                "keywords": ["å…­å£¬", "å£¬å­¦", "å£¬å ", "è¯¾ä¼ ", "ç¥å°†", "åäºŒå°†", "å››è¯¾", "ä¸‰ä¼ "],
                "priority": 1,
                "patterns": [r"å…­å£¬", r"å£¬å ", r"è¯¾ä¼ ", r"ç¥å°†", r"åäºŒå°†", r"å››è¯¾", r"ä¸‰ä¼ "]
            },
            "ç´«å¾®æ–—æ•°": {
                "keywords": ["ç´«å¾®", "æ–—æ•°", "å‘½ç›˜", "å®«ä½", "æ˜Ÿæ›œ", "ä¸»æ˜Ÿ", "è¾…æ˜Ÿ"],
                "priority": 2,
                "patterns": [r"ç´«å¾®\w*æ–—æ•°", r"å‘½ç›˜", r"å®«ä½", r"æ˜Ÿæ›œ", r"ä¸»æ˜Ÿ", r"è¾…æ˜Ÿ", r"åŒ–\w+"]
            },
            "å¥‡é—¨éç”²": {
                "keywords": ["å¥‡é—¨", "éç”²", "ä¹å®«", "å…«é—¨", "ç¥ç…", "ä¸‰å¥‡", "å…­ä»ª"],
                "priority": 2,
                "patterns": [r"å¥‡é—¨\w*éç”²", r"ä¹å®«", r"å…«é—¨", r"ç¥ç…", r"ä¸‰å¥‡", r"å…­ä»ª"]
            },
            "å…«å­—å‘½ç†": {
                "keywords": ["å…«å­—", "å››æŸ±", "å‘½ç†", "å¹²æ”¯", "çº³éŸ³", "åç¥", "å–œç”¨ç¥"],
                "priority": 2,
                "patterns": [r"å…«å­—", r"å››æŸ±", r"å‘½ç†", r"å¹²æ”¯", r"çº³éŸ³", r"åç¥", r"å–œç”¨ç¥"]
            },
            "é‡‘å£è¯€": {
                "keywords": ["é‡‘å£è¯€", "é‡‘å£", "è¯¾å¼", "ç«‹è¯¾", "å››ä½"],
                "priority": 3,
                "patterns": [r"é‡‘å£\w*è¯€", r"é‡‘å£", r"è¯¾å¼", r"ç«‹è¯¾", r"å››ä½"]
            },
            "å¤ªä¹™ç¥æ•°": {
                "keywords": ["å¤ªä¹™", "ç¥æ•°", "å¤ªä¹™ç¥æ•°", "å¤ªä¹™å¼"],
                "priority": 3,
                "patterns": [r"å¤ªä¹™\w*ç¥æ•°", r"å¤ªä¹™", r"å¤ªä¹™å¼"]
            },
            "æ²³æ´›ç†æ•°": {
                "keywords": ["æ²³æ´›", "ç†æ•°", "æ²³å›¾", "æ´›ä¹¦", "å…ˆå¤©å¦", "åå¤©å¦"],
                "priority": 3,
                "patterns": [r"æ²³æ´›\w*ç†æ•°", r"æ²³å›¾", r"æ´›ä¹¦", r"å…ˆå¤©å¦", r"åå¤©å¦"]
            },
            "å‘¨æ˜“åŸºç¡€": {
                "keywords": ["å‘¨æ˜“", "æ˜“ç»", "å…«å¦", "å…­åå››å¦", "å¦è±¡", "å¦è¾", "è±¡ä¼ "],
                "priority": 1,
                "patterns": [r"å‘¨æ˜“", r"æ˜“ç»", r"å…«å¦", r"å…­åå››å¦", r"å¦è¾", r"è±¡ä¼ ", r"å½–ä¼ "]
            }
        }
        
        # 64å¦åç§°
        self.hexagram_names = [
            "ä¹¾", "å¤", "å±¯", "è’™", "éœ€", "è®¼", "å¸ˆ", "æ¯”",
            "å°ç•œ", "å±¥", "æ³°", "å¦", "åŒäºº", "å¤§æœ‰", "è°¦", "è±«",
            "éš", "è›Š", "ä¸´", "è§‚", "å™¬å—‘", "è´²", "å‰¥", "å¤",
            "æ— å¦„", "å¤§ç•œ", "é¢", "å¤§è¿‡", "å", "ç¦»", "å’¸", "æ’",
            "é", "å¤§å£®", "æ™‹", "æ˜å¤·", "å®¶äºº", "ç½", "è¹‡", "è§£",
            "æŸ", "ç›Š", "å¤¬", "å§¤", "èƒ", "å‡", "å›°", "äº•",
            "é©", "é¼", "éœ‡", "è‰®", "æ¸", "å½’å¦¹", "ä¸°", "æ—…",
            "å·½", "å…‘", "æ¶£", "èŠ‚", "ä¸­å­š", "å°è¿‡", "æ—¢æµ", "æœªæµ"
        ]
        
        # ç¼“å­˜æ–‡ä»¶è·¯å¾„
        self.cache_file = self.output_dir / "cache" / "processing_cache.pkl"
        self.processed_files = self.load_cache()
        
        # è¿›åº¦è·Ÿè¸ª
        self.progress_bar = None
        self.start_time = None
    
    def load_cache(self) -> Dict[str, Any]:
        """åŠ è½½ç¼“å­˜"""
        if self.cache_file.exists():
            try:
                with open(self.cache_file, 'rb') as f:
                    cache_data = pickle.load(f)
                    logger.info(f"åŠ è½½ç¼“å­˜: {len(cache_data)} ä¸ªå·²å¤„ç†æ–‡ä»¶")
                    return cache_data
            except Exception as e:
                logger.warning(f"ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
        return {}
    
    def save_cache(self):
        """ä¿å­˜ç¼“å­˜"""
        try:
            with open(self.cache_file, 'wb') as f:
                pickle.dump(self.processed_files, f)
            logger.info(f"ç¼“å­˜å·²ä¿å­˜: {len(self.processed_files)} ä¸ªæ–‡ä»¶")
        except Exception as e:
            logger.error(f"ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
    
    def get_file_hash(self, file_path: Path) -> str:
        """è·å–æ–‡ä»¶å“ˆå¸Œ"""
        stat = file_path.stat()
        return hashlib.md5(f"{file_path}_{stat.st_size}_{stat.st_mtime}".encode()).hexdigest()
    
    def classify_pdf(self, text: str, file_name: str) -> Tuple[str, float, int]:
        """åˆ†ç±»PDFæ–‡ä»¶ - æ”¹è¿›çš„åˆ†ç±»ç®—æ³•"""
        text_lower = text.lower()
        file_lower = file_name.lower()
        
        category_scores = {}
        
        for category, config in self.category_patterns.items():
            score = 0.0
            
            # æ–‡ä»¶åæƒé‡æ›´é«˜
            filename_matches = sum(1 for kw in config["keywords"] if kw in file_lower)
            score += filename_matches * 3.0
            
            # å†…å®¹æ¨¡å¼åŒ¹é…
            for pattern in config["patterns"]:
                matches = len(re.findall(pattern, text, re.IGNORECASE))
                score += matches * 1.0
            
            # å…³é”®è¯å¯†åº¦
            total_keywords = sum(text_lower.count(kw) for kw in config["keywords"])
            if len(text) > 100:  # é¿å…é™¤é›¶
                density = (total_keywords / len(text)) * 10000
                score += density
            
            category_scores[category] = score
        
        # æ‰¾å‡ºæœ€é«˜åˆ†
        if not category_scores or max(category_scores.values()) == 0:
            return "å…¶ä»–", 0.0, 5
        
        best_category = max(category_scores, key=category_scores.get)
        best_score = category_scores[best_category]
        
        # å½’ä¸€åŒ–ç½®ä¿¡åº¦
        confidence = min(best_score / 15.0, 1.0)  # è°ƒæ•´å½’ä¸€åŒ–å› å­
        priority = self.category_patterns[best_category]["priority"]
        
        return best_category, confidence, priority
    
    def extract_hexagrams(self, text: str) -> List[Dict[str, Any]]:
        """æå–64å¦ä¿¡æ¯ - æ”¹è¿›ç‰ˆ"""
        hexagrams = []
        
        for i, name in enumerate(self.hexagram_names):
            # å¤šç§å¦ååŒ¹é…æ¨¡å¼
            patterns = [
                rf"{name}[å¦]?[ï¼š:]\s*([^ã€‚\n]+)",
                rf"ç¬¬\w*{name}å¦[ï¼š:]\s*([^ã€‚\n]+)",
                rf"{name}[å¦]?\s*([^ã€‚\n]{10,50})",
            ]
            
            for pattern in patterns:
                matches = re.finditer(pattern, text)
                for match in matches:
                    description = match.group(1).strip()
                    if len(description) > 5:  # è¿‡æ»¤å¤ªçŸ­çš„æè¿°
                        hexagram = {
                            "number": i + 1,
                            "name": name,
                            "description": description,
                            "position": match.start(),
                            "pattern_used": pattern
                        }
                        hexagrams.append(hexagram)
        
        # å»é‡
        seen = set()
        unique_hexagrams = []
        for h in hexagrams:
            key = (h["name"], h["description"])
            if key not in seen:
                seen.add(key)
                unique_hexagrams.append(h)
        
        return unique_hexagrams
    
    def extract_yao_ci(self, text: str) -> List[Dict[str, Any]]:
        """æå–çˆ»è¾ - æ”¹è¿›ç‰ˆ"""
        yao_positions = ["åˆ", "äºŒ", "ä¸‰", "å››", "äº”", "ä¸Š"]
        yao_types = ["å…­", "ä¹"]
        yao_ci = []
        
        for pos in yao_positions:
            for yao_type in yao_types:
                # å¤šç§çˆ»è¾æ ¼å¼
                patterns = [
                    rf"({pos}{yao_type})[ï¼š:]([^ã€‚\n]+[ã€‚]?)",
                    rf"({pos}{yao_type})\s*([^ã€‚\n]{10,100})",
                ]
                
                for pattern in patterns:
                    matches = re.finditer(pattern, text)
                    for match in matches:
                        yao_text = match.group(2).strip()
                        if len(yao_text) > 5:
                            yao = {
                                "position": pos,
                                "type": yao_type,
                                "full_name": match.group(1),
                                "text": yao_text,
                                "location": match.start()
                            }
                            yao_ci.append(yao)
        
        return yao_ci
    
    def extract_annotations(self, text: str) -> List[Dict[str, Any]]:
        """æå–æ³¨è§£ - æ”¹è¿›ç‰ˆ"""
        annotations = []
        
        # æ›´å…¨é¢çš„æ³¨è§£æ¨¡å¼
        annotation_patterns = [
            (r"æ³¨[ï¼š:]([^ã€‚\n]{10,200})", "æ³¨"),
            (r"è§£[ï¼š:]([^ã€‚\n]{10,200})", "è§£"),
            (r"é‡Š[ï¼š:]([^ã€‚\n]{10,200})", "é‡Š"),
            (r"æŒ‰[ï¼š:]([^ã€‚\n]{10,200})", "æŒ‰"),
            (r"æ›°[ï¼š:]([^ã€‚\n]{10,200})", "æ›°"),
            (r"è¿°[ï¼š:]([^ã€‚\n]{10,200})", "è¿°"),
            (r"è¯„[ï¼š:]([^ã€‚\n]{10,200})", "è¯„"),
        ]
        
        for pattern, ann_type in annotation_patterns:
            matches = re.finditer(pattern, text)
            for match in matches:
                content = match.group(1).strip()
                if len(content) > 10:  # è¿‡æ»¤å¤ªçŸ­çš„æ³¨è§£
                    annotation = {
                        "type": ann_type,
                        "content": content,
                        "position": match.start(),
                        "length": len(content)
                    }
                    annotations.append(annotation)
        
        return annotations
    
    def extract_cases(self, text: str) -> List[Dict[str, Any]]:
        """æå–æ¡ˆä¾‹ - æ”¹è¿›ç‰ˆ"""
        cases = []
        
        # æ›´å¤šæ¡ˆä¾‹æ¨¡å¼
        case_patterns = [
            r"ä¾‹[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d+][ï¼š:]([^ã€‚]{30,500})",
            r"æ¡ˆä¾‹[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d*][ï¼š:]([^ã€‚]{30,500})",
            r"å®ä¾‹[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d*][ï¼š:]([^ã€‚]{30,500})",
            r"å ä¾‹[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d*][ï¼š:]([^ã€‚]{30,500})",
            r"æµ‹ä¾‹[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d*][ï¼š:]([^ã€‚]{30,500})",
        ]
        
        for pattern in case_patterns:
            matches = re.finditer(pattern, text, re.DOTALL)
            for match in matches:
                content = match.group(1).strip()
                if len(content) >= 30:  # ç¡®ä¿æ¡ˆä¾‹æœ‰è¶³å¤Ÿå†…å®¹
                    case = {
                        "content": content,
                        "position": match.start(),
                        "length": len(content),
                        "preview": content[:100] + "..." if len(content) > 100 else content
                    }
                    cases.append(case)
        
        return cases
    
    def extract_keywords(self, text: str) -> List[str]:
        """æå–å…³é”®è¯ - æ”¹è¿›ç‰ˆ"""
        keywords = set()
        
        # åˆ†ç±»åˆ«çš„æœ¯è¯­
        term_categories = {
            "åŸºç¡€æ¦‚å¿µ": ["é˜´é˜³", "äº”è¡Œ", "å…«å¦", "å…­çˆ»", "å åœ", "é¢„æµ‹", "å‘½ç†", "é£æ°´", "å‘¨æ˜“", "å¤ªæ"],
            "å…­çˆ»æœ¯è¯­": ["ç¥ç…", "å…­äº²", "ç”¨ç¥", "ä¸–åº”", "åŠ¨çˆ»", "é™çˆ»", "å˜çˆ»", "é£ç¥", "ä¼ç¥"],
            "å…«å­—æœ¯è¯­": ["åç¥", "å¤©å¹²", "åœ°æ”¯", "çº³éŸ³", "å–œç”¨ç¥", "å¿Œç¥", "æ ¼å±€", "è¿åŠ¿"],
            "ç´«å¾®æœ¯è¯­": ["å‘½å®«", "èº«å®«", "ä¸»æ˜Ÿ", "è¾…æ˜Ÿ", "åŒ–ç¦„", "åŒ–æƒ", "åŒ–ç§‘", "åŒ–å¿Œ"],
            "å¥‡é—¨æœ¯è¯­": ["ä¹å®«", "å…«é—¨", "ä¸‰å¥‡", "å…­ä»ª", "å€¼ç¬¦", "å€¼ä½¿", "å¤©ç›˜", "åœ°ç›˜"]
        }
        
        for category, terms in term_categories.items():
            for term in terms:
                if term in text:
                    keywords.add(term)
        
        # æå–ä¹¦å
        book_names = re.findall(r'ã€Š([^ã€‹]{2,20})ã€‹', text)
        keywords.update(book_names[:5])  # é™åˆ¶ä¹¦åæ•°é‡
        
        # æå–äººåï¼ˆå¯èƒ½çš„ä½œè€…ï¼‰
        author_patterns = re.findall(r'([ç‹æå¼ åˆ˜é™ˆæ¨èµµé»„å‘¨å´å¾å­™èƒ¡æœ±é«˜æ—ä½•éƒ­é©¬ç½—æ¢å®‹éƒ‘è°¢éŸ©å”å†¯äºè‘£è§ç¨‹æ›¹è¢é‚“è®¸å‚…æ²ˆæ›¾å½­å•è‹å¢è’‹è”¡è´¾ä¸é­è–›å¶é˜ä½™æ½˜æœæˆ´å¤é’Ÿæ±ªç”°ä»»å§œèŒƒæ–¹çŸ³å§šè°­å»–é‚¹ç†Šé‡‘é™†éƒå­”ç™½å´”åº·æ¯›é‚±ç§¦æ±Ÿå²é¡¾ä¾¯é‚µå­Ÿé¾™ä¸‡æ®µé›·é’±æ±¤å°¹é»æ˜“å¸¸æ­¦ä¹”è´ºèµ–é¾šæ–‡][\u4e00-\u9fff]{1,3})', text)
        keywords.update(author_patterns[:3])  # é™åˆ¶äººåæ•°é‡
        
        return sorted(list(keywords))
    
    def extract_author_dynasty(self, text: str, filename: str) -> Tuple[Optional[str], Optional[str]]:
        """æå–ä½œè€…å’Œæœä»£ - æ”¹è¿›ç‰ˆ"""
        author = None
        dynasty = None
        
        # ä»æ–‡ä»¶åæå–ä½œè€…
        filename_patterns = [
            r'([ç‹æå¼ åˆ˜é™ˆæ¨èµµé»„å‘¨å´å¾å­™èƒ¡æœ±é«˜æ—ä½•éƒ­é©¬ç½—æ¢å®‹éƒ‘è°¢éŸ©å”å†¯äºè‘£è§ç¨‹æ›¹è¢é‚“è®¸å‚…æ²ˆæ›¾å½­å•è‹å¢è’‹è”¡è´¾ä¸é­è–›å¶é˜ä½™æ½˜æœæˆ´å¤é’Ÿæ±ªç”°ä»»å§œèŒƒæ–¹çŸ³å§šè°­å»–é‚¹ç†Šé‡‘é™†éƒå­”ç™½å´”åº·æ¯›é‚±ç§¦æ±Ÿå²é¡¾ä¾¯é‚µå­Ÿé¾™ä¸‡æ®µé›·é’±æ±¤å°¹é»æ˜“å¸¸æ­¦ä¹”è´ºèµ–é¾šæ–‡][\u4e00-\u9fff]{1,3})[_-]',
            r'^([^0-9\s\-_]+)',
        ]
        
        for pattern in filename_patterns:
            match = re.search(pattern, filename)
            if match:
                potential_author = match.group(1)
                if len(potential_author) >= 2 and not any(char.isdigit() for char in potential_author):
                    author = potential_author
                    break
        
        # æœä»£æ¨¡å¼ - æ›´ç²¾ç¡®
        dynasty_patterns = [
            r'[(ï¼ˆ]?(æ±‰|å”|å®‹|å…ƒ|æ˜|æ¸…)[æœä»£)ï¼‰]?',
            r'(æ±‰|å”|å®‹|å…ƒ|æ˜|æ¸…)[Â·â€¢]',
            r'(æ±‰|å”|å®‹|å…ƒ|æ˜|æ¸…)ä»£',
        ]
        
        for pattern in dynasty_patterns:
            match = re.search(pattern, text)
            if match:
                dynasty = match.group(1)
                break
        
        return author, dynasty
    
    def process_single_pdf(self, file_path: Path) -> Optional[Dict[str, Any]]:
        """å¤„ç†å•ä¸ªPDFæ–‡ä»¶"""
        start_time = time.time()
        
        try:
            # æ£€æŸ¥ç¼“å­˜
            file_hash = self.get_file_hash(file_path)
            if file_hash in self.processed_files:
                if self.progress_bar:
                    self.progress_bar.set_postfix_str(f"ç¼“å­˜: {file_path.name[:30]}...")
                return self.processed_files[file_hash]
            
            if self.progress_bar:
                self.progress_bar.set_postfix_str(f"å¤„ç†: {file_path.name[:30]}...")
            
            # æå–æ–‡æœ¬
            text = ""
            page_count = 0
            
            with pdfplumber.open(file_path) as pdf:
                page_count = len(pdf.pages)
                # é™åˆ¶æå–é¡µæ•°ï¼Œé¿å…è¶…å¤§æ–‡ä»¶
                max_pages = min(page_count, 200)  # æœ€å¤šå¤„ç†200é¡µ
                
                for i, page in enumerate(pdf.pages[:max_pages]):
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + "\n"
            
            if not text.strip():
                logger.warning(f"æ— æ³•æå–æ–‡æœ¬: {file_path.name}")
                return None
            
            # åˆ†ç±»
            category, confidence, priority = self.classify_pdf(text, file_path.name)
            
            # æå–ç»“æ„åŒ–å†…å®¹
            hexagrams = self.extract_hexagrams(text)
            yao_ci = self.extract_yao_ci(text)
            annotations = self.extract_annotations(text)
            cases = self.extract_cases(text)
            keywords = self.extract_keywords(text)
            author, dynasty = self.extract_author_dynasty(text, file_path.name)
            
            processing_time = time.time() - start_time
            
            # åˆ›å»ºç»“æœ
            result = {
                "pdf_info": {
                    "file_path": str(file_path),
                    "file_name": file_path.name,
                    "file_size": file_path.stat().st_size,
                    "pages": page_count,
                    "category": category,
                    "confidence": confidence,
                    "priority": priority,
                    "processed_at": datetime.now().isoformat(),
                    "text_length": len(text),
                    "processing_time": processing_time
                },
                "content": {
                    "hexagrams": hexagrams,
                    "yao_ci": yao_ci,
                    "annotations": annotations,
                    "cases": cases,
                    "keywords": keywords,
                    "author": author,
                    "dynasty": dynasty
                },
                "statistics": {
                    "hexagram_count": len(hexagrams),
                    "yao_ci_count": len(yao_ci),
                    "annotation_count": len(annotations),
                    "case_count": len(cases),
                    "keyword_count": len(keywords)
                }
            }
            
            # ä¿å­˜åˆ°ç¼“å­˜
            self.processed_files[file_hash] = result
            
            # ä¿å­˜åŸå§‹æ–‡æœ¬ï¼ˆå¯é€‰ï¼‰
            if len(text) < 100000:  # åªä¿å­˜è¾ƒå°çš„æ–‡æœ¬æ–‡ä»¶
                text_file = self.output_dir / "raw_texts" / f"{file_path.stem}.txt"
                with open(text_file, 'w', encoding='utf-8') as f:
                    f.write(text)
            
            return result
            
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path.name}: {e}")
            return None
    
    def process_all_pdfs(self, max_workers: int = 4) -> Dict[str, Any]:
        """å¹¶è¡Œå¤„ç†æ‰€æœ‰PDFæ–‡ä»¶ - å¸¦è¿›åº¦æ¡"""
        pdf_files = list(self.data_dir.glob("*.pdf"))
        logger.info(f"æ‰¾åˆ° {len(pdf_files)} ä¸ªPDFæ–‡ä»¶")
        
        if len(pdf_files) == 0:
            return {"error": "æ²¡æœ‰æ‰¾åˆ°PDFæ–‡ä»¶"}
        
        # è¿‡æ»¤å·²å¤„ç†çš„æ–‡ä»¶
        unprocessed_files = []
        for pdf_file in pdf_files:
            file_hash = self.get_file_hash(pdf_file)
            if file_hash not in self.processed_files:
                unprocessed_files.append(pdf_file)
        
        logger.info(f"éœ€è¦å¤„ç† {len(unprocessed_files)} ä¸ªæ–°æ–‡ä»¶ï¼Œ{len(pdf_files) - len(unprocessed_files)} ä¸ªå·²ç¼“å­˜")
        
        results = []
        failed_files = []
        
        # åˆ›å»ºè¿›åº¦æ¡
        self.progress_bar = tqdm(total=len(pdf_files), desc="å¤„ç†PDFæ–‡ä»¶", unit="æ–‡ä»¶")
        
        # æ·»åŠ å·²ç¼“å­˜çš„ç»“æœ
        for pdf_file in pdf_files:
            file_hash = self.get_file_hash(pdf_file)
            if file_hash in self.processed_files:
                results.append(self.processed_files[file_hash])
                self.progress_bar.update(1)
        
        if unprocessed_files:
            with ProcessPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤ä»»åŠ¡
                future_to_file = {
                    executor.submit(self.process_single_pdf, pdf_file): pdf_file 
                    for pdf_file in unprocessed_files
                }
                
                # æ”¶é›†ç»“æœ
                for future in as_completed(future_to_file):
                    pdf_file = future_to_file[future]
                    try:
                        result = future.result()
                        if result:
                            results.append(result)
                        else:
                            failed_files.append(pdf_file.name)
                    except Exception as e:
                        logger.error(f"ä»»åŠ¡å¤±è´¥ {pdf_file.name}: {e}")
                        failed_files.append(pdf_file.name)
                    finally:
                        self.progress_bar.update(1)
                        
                        # å®šæœŸä¿å­˜ç¼“å­˜
                        if len(results) % 10 == 0:
                            self.save_cache()
        
        self.progress_bar.close()
        
        # æœ€ç»ˆä¿å­˜ç¼“å­˜
        self.save_cache()
        
        # æŒ‰ä¼˜å…ˆçº§å’Œç±»åˆ«ç»„ç»‡ç»“æœ
        categorized_results = defaultdict(list)
        priority_results = defaultdict(list)
        
        for result in results:
            category = result["pdf_info"]["category"]
            priority = result["pdf_info"]["priority"]
            
            categorized_results[category].append(result)
            priority_results[priority].append(result)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        total_hexagrams = sum(r.get("statistics", {}).get("hexagram_count", 0) for r in results)
        total_yao_ci = sum(r.get("statistics", {}).get("yao_ci_count", 0) for r in results)
        total_annotations = sum(r.get("statistics", {}).get("annotation_count", 0) for r in results)
        total_cases = sum(r.get("statistics", {}).get("case_count", 0) for r in results)
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            "total_files": len(pdf_files),
            "processed_successfully": len(results),
            "failed_files": len(failed_files),
            "cached_files": len(pdf_files) - len(unprocessed_files),
            "categories": {cat: len(files) for cat, files in categorized_results.items()},
            "priorities": {f"ä¼˜å…ˆçº§{p}": len(files) for p, files in priority_results.items()},
            "content_statistics": {
                "total_hexagrams": total_hexagrams,
                "total_yao_ci": total_yao_ci,
                "total_annotations": total_annotations,
                "total_cases": total_cases,
                "avg_per_file": {
                    "hexagrams": total_hexagrams / max(len(results), 1),
                    "yao_ci": total_yao_ci / max(len(results), 1),
                    "annotations": total_annotations / max(len(results), 1),
                    "cases": total_cases / max(len(results), 1)
                }
            },
            "failed_file_list": failed_files,
            "processing_time": datetime.now().isoformat()
        }
        
        return {
            "statistics": stats,
            "results": results,
            "by_category": dict(categorized_results),
            "by_priority": dict(priority_results)
        }
    
    def save_results(self, results: Dict[str, Any]):
        """ä¿å­˜å¤„ç†ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        full_results_file = self.output_dir / "structured_data" / f"complete_results_{timestamp}.json"
        with open(full_results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # æŒ‰ç±»åˆ«ä¿å­˜
        for category, files in results["by_category"].items():
            category_dir = self.output_dir / "categories" / category
            category_dir.mkdir(exist_ok=True)
            
            category_file = category_dir / f"{category}_{timestamp}.json"
            with open(category_file, 'w', encoding='utf-8') as f:
                json.dump({
                    "category": category,
                    "file_count": len(files),
                    "files": files
                }, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats_file = self.output_dir / "structured_data" / f"statistics_{timestamp}.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(results["statistics"], f, ensure_ascii=False, indent=2)
        
        # ç”ŸæˆHTMLæŠ¥å‘Š
        self.generate_html_report(results, timestamp)
        
        # ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
        self.generate_summary_report(results, timestamp)
        
        logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {self.output_dir}")
        return full_results_file
    
    def generate_html_report(self, results: Dict[str, Any], timestamp: str):
        """ç”ŸæˆHTMLå¯è§†åŒ–æŠ¥å‘Š"""
        stats = results["statistics"]
        
        html_content = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>æ˜“å­¦PDFå¤„ç†æŠ¥å‘Š - {timestamp}</title>
    <style>
        body {{ font-family: 'Microsoft YaHei', sans-serif; margin: 20px; background: #f5f5f5; }}
        .container {{ max-width: 1200px; margin: 0 auto; background: white; padding: 30px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}
        .header {{ text-align: center; border-bottom: 3px solid #4CAF50; padding-bottom: 20px; margin-bottom: 30px; }}
        .stats-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 20px; margin-bottom: 30px; }}
        .stat-card {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 20px; border-radius: 10px; text-align: center; }}
        .stat-number {{ font-size: 2.5em; font-weight: bold; margin-bottom: 10px; }}
        .stat-label {{ font-size: 1.1em; opacity: 0.9; }}
        .category-section {{ margin: 30px 0; }}
        .category-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 15px; }}
        .category-item {{ background: #e8f5e8; padding: 15px; border-radius: 8px; border-left: 4px solid #4CAF50; }}
        .priority-section {{ background: #f9f9f9; padding: 20px; border-radius: 10px; margin: 20px 0; }}
        h1 {{ color: #333; }}
        h2 {{ color: #4CAF50; border-bottom: 2px solid #4CAF50; padding-bottom: 10px; }}
        .failed-files {{ background: #ffebee; padding: 15px; border-radius: 8px; border-left: 4px solid #f44336; }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ”® æ˜“å­¦PDFæ‰¹é‡å¤„ç†æŠ¥å‘Š</h1>
            <p>å¤„ç†æ—¶é—´: {timestamp}</p>
        </div>
        
        <div class="stats-grid">
            <div class="stat-card">
                <div class="stat-number">{stats['total_files']}</div>
                <div class="stat-label">æ€»æ–‡ä»¶æ•°</div>
            </div>
            <div class="stat-card">
                <div class="stat-number">{stats['processed_successfully']}</div>
                <div class="stat-label">æˆåŠŸå¤„ç†</div>
            </div>
            <div class="stat-card">
                <div class="stat-number">{stats.get('cached_files', 0)}</div>
                <div class="stat-label">ç¼“å­˜æ–‡ä»¶</div>
            </div>
            <div class="stat-card">
                <div class="stat-number">{stats['processed_successfully']/max(stats['total_files'], 1)*100:.1f}%</div>
                <div class="stat-label">æˆåŠŸç‡</div>
            </div>
        </div>
        
        <div class="category-section">
            <h2>ğŸ“š åˆ†ç±»ç»Ÿè®¡</h2>
            <div class="category-grid">
        """
        
        for category, count in stats["categories"].items():
            percentage = count / max(stats['processed_successfully'], 1) * 100
            html_content += f"""
                <div class="category-item">
                    <strong>{category}</strong><br>
                    {count} ä¸ªæ–‡ä»¶ ({percentage:.1f}%)
                </div>
            """
        
        html_content += """
            </div>
        </div>
        
        <div class="priority-section">
            <h2>â­ ä¼˜å…ˆçº§åˆ†å¸ƒ</h2>
            <div class="category-grid">
        """
        
        for priority, count in stats["priorities"].items():
            html_content += f"""
                <div class="category-item">
                    <strong>{priority}</strong><br>
                    {count} ä¸ªæ–‡ä»¶
                </div>
            """
        
        html_content += "</div></div>"
        
        # å†…å®¹ç»Ÿè®¡
        if "content_statistics" in stats:
            content_stats = stats["content_statistics"]
            html_content += f"""
            <div class="category-section">
                <h2>ğŸ“– å†…å®¹ç»Ÿè®¡</h2>
                <div class="stats-grid">
                    <div class="stat-card" style="background: linear-gradient(135deg, #ff7b7b 0%, #d63384 100%);">
                        <div class="stat-number">{content_stats['total_hexagrams']}</div>
                        <div class="stat-label">æ€»å¦è±¡</div>
                    </div>
                    <div class="stat-card" style="background: linear-gradient(135deg, #20c997 0%, #0d7377 100%);">
                        <div class="stat-number">{content_stats['total_yao_ci']}</div>
                        <div class="stat-label">æ€»çˆ»è¾</div>
                    </div>
                    <div class="stat-card" style="background: linear-gradient(135deg, #fd7e14 0%, #e55100 100%);">
                        <div class="stat-number">{content_stats['total_annotations']}</div>
                        <div class="stat-label">æ€»æ³¨è§£</div>
                    </div>
                    <div class="stat-card" style="background: linear-gradient(135deg, #6610f2 0%, #4c1a57 100%);">
                        <div class="stat-number">{content_stats['total_cases']}</div>
                        <div class="stat-label">æ€»æ¡ˆä¾‹</div>
                    </div>
                </div>
            </div>
            """
        
        # å¤±è´¥æ–‡ä»¶
        if stats["failed_files"] > 0:
            html_content += f"""
            <div class="failed-files">
                <h2>âŒ å¤±è´¥æ–‡ä»¶ ({stats['failed_files']} ä¸ª)</h2>
                <ul>
            """
            for failed_file in stats["failed_file_list"]:
                html_content += f"<li>{failed_file}</li>"
            html_content += "</ul></div>"
        
        html_content += """
        </div>
    </body>
    </html>
        """
        
        # ä¿å­˜HTMLæŠ¥å‘Š
        html_file = self.output_dir / "reports" / f"report_{timestamp}.html"
        with open(html_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        logger.info(f"HTMLæŠ¥å‘Šå·²ç”Ÿæˆ: {html_file}")
    
    def generate_summary_report(self, results: Dict[str, Any], timestamp: str):
        """ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š"""
        stats = results["statistics"]
        
        report = f"""
# ğŸ“‹ PDFå¤„ç†æ‘˜è¦æŠ¥å‘Š
**æ—¶é—´**: {timestamp}

## ğŸ“Š å¤„ç†ç»Ÿè®¡
- **æ€»æ–‡ä»¶æ•°**: {stats['total_files']}
- **æˆåŠŸå¤„ç†**: {stats['processed_successfully']}
- **å¤±è´¥æ•°é‡**: {stats['failed_files']}
- **ç¼“å­˜æ–‡ä»¶**: {stats.get('cached_files', 0)}
- **æˆåŠŸç‡**: {stats['processed_successfully']/max(stats['total_files'], 1)*100:.1f}%

## ğŸ“š åˆ†ç±»ç»Ÿè®¡
"""
        
        for category, count in sorted(stats["categories"].items(), key=lambda x: x[1], reverse=True):
            percentage = count / max(stats['processed_successfully'], 1) * 100
            report += f"- **{category}**: {count} ä¸ªæ–‡ä»¶ ({percentage:.1f}%)\n"
        
        report += "\n## â­ ä¼˜å…ˆçº§åˆ†å¸ƒ\n"
        for priority in sorted(stats["priorities"].keys()):
            count = stats["priorities"][priority]
            report += f"- **{priority}**: {count} ä¸ªæ–‡ä»¶\n"
        
        # å†…å®¹ç»Ÿè®¡
        if "content_statistics" in stats:
            content_stats = stats["content_statistics"]
            report += f"""
## ğŸ“– å†…å®¹æå–ç»Ÿè®¡
- **æ€»å¦è±¡**: {content_stats['total_hexagrams']} ä¸ª
- **æ€»çˆ»è¾**: {content_stats['total_yao_ci']} ä¸ª
- **æ€»æ³¨è§£**: {content_stats['total_annotations']} ä¸ª
- **æ€»æ¡ˆä¾‹**: {content_stats['total_cases']} ä¸ª

### å¹³å‡æ¯æ–‡ä»¶
- **å¦è±¡**: {content_stats['avg_per_file']['hexagrams']:.1f} ä¸ª
- **çˆ»è¾**: {content_stats['avg_per_file']['yao_ci']:.1f} ä¸ª
- **æ³¨è§£**: {content_stats['avg_per_file']['annotations']:.1f} ä¸ª
- **æ¡ˆä¾‹**: {content_stats['avg_per_file']['cases']:.1f} ä¸ª
"""
        
        if stats["failed_files"] > 0:
            report += f"\n## âŒ å¤±è´¥æ–‡ä»¶ ({stats['failed_files']} ä¸ª)\n"
            for failed_file in stats["failed_file_list"]:
                report += f"- {failed_file}\n"
        
        report += f"""
## ğŸ’¾ è¾“å‡ºæ–‡ä»¶
- å®Œæ•´ç»“æœ: `structured_data/complete_results_{timestamp}.json`
- ç»Ÿè®¡ä¿¡æ¯: `structured_data/statistics_{timestamp}.json`
- HTMLæŠ¥å‘Š: `reports/report_{timestamp}.html`
- åˆ†ç±»ç»“æœ: `categories/` ç›®å½•ä¸‹æŒ‰ç±»åˆ«ä¿å­˜
- åŸå§‹æ–‡æœ¬: `raw_texts/` ç›®å½•ä¸‹ï¼ˆå°æ–‡ä»¶ï¼‰

## ğŸ“ ä½¿ç”¨å»ºè®®
1. ä¼˜å…ˆçº§1çš„æ–‡ä»¶åŒ…å«æœ€é‡è¦çš„æ˜“å­¦å†…å®¹
2. æŸ¥çœ‹HTMLæŠ¥å‘Šè·å¾—å¯è§†åŒ–ç»Ÿè®¡
3. æŒ‰ç±»åˆ«æŸ¥çœ‹ä¸“é—¨çš„JSONæ–‡ä»¶
4. å¤±è´¥çš„æ–‡ä»¶å¯èƒ½éœ€è¦æ‰‹åŠ¨å¤„ç†
"""
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.output_dir / f"processing_report_{timestamp}.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(report)
        return report_file

def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®è·¯å¾„
    data_dir = "/mnt/d/desktop/appp/data"
    output_dir = "/mnt/d/desktop/appp/structured_data"
    
    print("ğŸš€ å¼€å§‹æ‰¹é‡å¤„ç†æ˜“å­¦PDFæ–‡ä»¶...")
    print(f"ğŸ“‚ æ•°æ®ç›®å½•: {data_dir}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    
    # åˆ›å»ºå¤„ç†å™¨
    processor = PDFProcessorWithProgress(data_dir, output_dir)
    
    # æ£€æŸ¥æ•°æ®ç›®å½•
    if not Path(data_dir).exists():
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        return
    
    # ç»Ÿè®¡PDFæ–‡ä»¶
    pdf_count = len(list(Path(data_dir).glob("*.pdf")))
    print(f"ğŸ“‹ æ‰¾åˆ° {pdf_count} ä¸ªPDFæ–‡ä»¶")
    
    if pdf_count == 0:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°PDFæ–‡ä»¶")
        return
    
    # å¼€å§‹å¤„ç†
    start_time = datetime.now()
    processor.start_time = start_time
    print(f"â° å¼€å§‹æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    try:
        # ä½¿ç”¨4ä¸ªè¿›ç¨‹å¹¶è¡Œå¤„ç†
        print("ğŸ”„ å¼€å§‹å¹¶è¡Œå¤„ç†...")
        results = processor.process_all_pdfs(max_workers=4)
        
        if "error" in results:
            print(f"âŒ {results['error']}")
            return
        
        # ä¿å­˜ç»“æœ
        print("\nğŸ’¾ ä¿å­˜å¤„ç†ç»“æœ...")
        result_file = processor.save_results(results)
        
        end_time = datetime.now()
        duration = end_time - start_time
        
        print(f"\nâœ… å¤„ç†å®Œæˆ!")
        print(f"â° æ€»è€—æ—¶: {duration}")
        print(f"ğŸ“Š æˆåŠŸå¤„ç†: {results['statistics']['processed_successfully']}/{results['statistics']['total_files']}")
        print(f"ğŸ—‚ï¸ ä¸»è¦ç»“æœæ–‡ä»¶: {result_file}")
        print(f"ğŸ“ æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {output_dir}")
        
        # æ˜¾ç¤ºåˆ†ç±»ç»Ÿè®¡
        print(f"\nğŸ“š åˆ†ç±»ç»Ÿè®¡:")
        for category, count in results['statistics']['categories'].items():
            print(f"  - {category}: {count} ä¸ªæ–‡ä»¶")
            
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­å¤„ç†")
        processor.save_cache()  # ä¿å­˜å·²å¤„ç†çš„ç¼“å­˜
        print("ğŸ’¾ å·²ä¿å­˜å¤„ç†è¿›åº¦ï¼Œä¸‹æ¬¡è¿è¡Œå°†ä»æ–­ç‚¹ç»§ç»­")
    except Exception as e:
        print(f"\nâŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        logger.error(f"å¤„ç†å¼‚å¸¸: {e}", exc_info=True)
        processor.save_cache()  # ä¿å­˜å·²å¤„ç†çš„ç¼“å­˜

if __name__ == "__main__":
    main()