#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½æ–‡æœ¬å¤„ç†å¼•æ“é›†æˆæµ‹è¯•è„šæœ¬
æµ‹è¯•å’Œæ¼”ç¤ºæ‰€æœ‰æ¨¡å—çš„åŠŸèƒ½ï¼ŒåŒ…æ‹¬æ€§èƒ½åŸºå‡†æµ‹è¯•
"""

import time
import json
import sys
from pathlib import Path
from typing import Dict, List

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
try:
    from text_cleaner import TextCleaner
    from content_classifier import ContentClassifier
    from info_extractor import InfoExtractor
    from quality_checker import QualityChecker
except ImportError as e:
    print(f"æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿æ‰€æœ‰æ¨¡å—æ–‡ä»¶éƒ½åœ¨å½“å‰ç›®å½•ä¸­")
    sys.exit(1)


class ProcessingEngine:
    """æ™ºèƒ½æ–‡æœ¬å¤„ç†å¼•æ“é›†æˆç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ‰€æœ‰å¤„ç†æ¨¡å—"""
        print("æ­£åœ¨åˆå§‹åŒ–æ™ºèƒ½æ–‡æœ¬å¤„ç†å¼•æ“...")
        
        self.text_cleaner = TextCleaner()
        self.content_classifier = ContentClassifier()
        self.info_extractor = InfoExtractor()
        self.quality_checker = QualityChecker()
        
        print("âœ… æ‰€æœ‰æ¨¡å—åˆå§‹åŒ–å®Œæˆ")
    
    def process_text_pipeline(self, text: str, text_type: str = 'yijing') -> Dict:
        """å®Œæ•´çš„æ–‡æœ¬å¤„ç†æµæ°´çº¿"""
        pipeline_start = time.time()
        results = {
            'original_text': text,
            'text_type': text_type,
            'pipeline_stages': {}
        }
        
        print(f"\nğŸš€ å¼€å§‹å¤„ç†æ–‡æœ¬ (ç±»å‹: {text_type})")
        print(f"åŸå§‹æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
        
        # é˜¶æ®µ1: æ–‡æœ¬æ¸…æ´—
        print("\nğŸ“ é˜¶æ®µ1: æ–‡æœ¬æ¸…æ´—...")
        clean_start = time.time()
        
        if text_type == 'yijing':
            cleaned_text = self.text_cleaner.clean_yijing_text(text)
        else:
            cleaned_text = self.text_cleaner.clean_text_basic(text)
        
        cleaned_text = self.text_cleaner.normalize_characters(cleaned_text)
        clean_time = time.time() - clean_start
        
        results['pipeline_stages']['cleaning'] = {
            'cleaned_text': cleaned_text,
            'original_length': len(text),
            'cleaned_length': len(cleaned_text),
            'compression_ratio': (len(text) - len(cleaned_text)) / len(text) if len(text) > 0 else 0,
            'processing_time': clean_time
        }
        
        print(f"âœ… æ¸…æ´—å®Œæˆ - å‹ç¼©ç‡: {results['pipeline_stages']['cleaning']['compression_ratio']:.1%}")
        print(f"   å¤„ç†æ—¶é—´: {clean_time:.3f}s")
        
        # é˜¶æ®µ2: å†…å®¹åˆ†ç±»
        print("\nğŸ·ï¸  é˜¶æ®µ2: å†…å®¹åˆ†ç±»...")
        classify_start = time.time()
        
        classification_results = self.content_classifier.classify_document(cleaned_text)
        merged_results = self.content_classifier.merge_adjacent_segments(classification_results)
        structure_analysis = self.content_classifier.analyze_document_structure(merged_results)
        classify_time = time.time() - classify_start
        
        results['pipeline_stages']['classification'] = {
            'segments': len(merged_results),
            'structure_analysis': structure_analysis,
            'classification_results': [
                {
                    'type': r.content_type,
                    'confidence': r.confidence,
                    'text_preview': r.text_segment[:50] + '...' if len(r.text_segment) > 50 else r.text_segment,
                    'position': r.start_position
                }
                for r in merged_results[:10]  # åªæ˜¾ç¤ºå‰10ä¸ªç»“æœ
            ],
            'processing_time': classify_time
        }
        
        print(f"âœ… åˆ†ç±»å®Œæˆ - å‘ç° {len(merged_results)} ä¸ªæ–‡æœ¬æ®µ")
        print(f"   ä¸»è¦å†…å®¹ç±»å‹: {list(structure_analysis.get('content_types', {}).keys())[:5]}")
        print(f"   å¤„ç†æ—¶é—´: {classify_time:.3f}s")
        
        # é˜¶æ®µ3: ä¿¡æ¯æŠ½å–
        print("\nğŸ” é˜¶æ®µ3: ä¿¡æ¯æŠ½å–...")
        extract_start = time.time()
        
        extracted_info = self.info_extractor.extract_structured_info(cleaned_text, text_type)
        extract_time = time.time() - extract_start
        
        results['pipeline_stages']['extraction'] = {
            'info_type': extracted_info['type'],
            'extracted_data': self._summarize_extracted_info(extracted_info['data']),
            'processing_time': extract_time
        }
        
        print(f"âœ… æŠ½å–å®Œæˆ - ç±»å‹: {extracted_info['type']}")
        print(f"   å¤„ç†æ—¶é—´: {extract_time:.3f}s")
        
        # é˜¶æ®µ4: è´¨é‡æ£€æŸ¥
        print("\nğŸ” é˜¶æ®µ4: è´¨é‡æ£€æŸ¥...")
        quality_start = time.time()
        
        quality_report = self.quality_checker.check_quality(cleaned_text, text_type)
        quality_time = time.time() - quality_start
        
        results['pipeline_stages']['quality_check'] = {
            'overall_score': quality_report.overall_score,
            'total_issues': quality_report.total_issues,
            'issue_summary': {
                'critical': quality_report.critical_issues,
                'major': quality_report.major_issues,
                'minor': quality_report.minor_issues,
                'warnings': quality_report.warnings
            },
            'top_issues': [
                {
                    'type': issue.issue_type,
                    'severity': issue.severity,
                    'description': issue.description,
                    'line': issue.line_number
                }
                for issue in quality_report.issues[:5]
            ],
            'recommendations': quality_report.recommendations[:5],
            'processing_time': quality_time
        }
        
        print(f"âœ… è´¨é‡æ£€æŸ¥å®Œæˆ - å¾—åˆ†: {quality_report.overall_score:.1f}/100")
        print(f"   é—®é¢˜ç»Ÿè®¡: {quality_report.total_issues} æ€»è®¡ "
              f"({quality_report.critical_issues} ä¸¥é‡, {quality_report.major_issues} ä¸»è¦, "
              f"{quality_report.minor_issues} æ¬¡è¦, {quality_report.warnings} è­¦å‘Š)")
        print(f"   å¤„ç†æ—¶é—´: {quality_time:.3f}s")
        
        # æ€»ç»“
        total_time = time.time() - pipeline_start
        results['pipeline_summary'] = {
            'total_processing_time': total_time,
            'stages_time': {
                'cleaning': clean_time,
                'classification': classify_time,
                'extraction': extract_time,
                'quality_check': quality_time
            },
            'performance_metrics': {
                'chars_per_second': len(text) / total_time if total_time > 0 else 0,
                'total_stages': 4,
                'success': True
            }
        }
        
        print(f"\nğŸ‰ å¤„ç†æµæ°´çº¿å®Œæˆ!")
        print(f"æ€»å¤„ç†æ—¶é—´: {total_time:.3f}s")
        print(f"å¤„ç†é€Ÿåº¦: {results['pipeline_summary']['performance_metrics']['chars_per_second']:.0f} å­—ç¬¦/ç§’")
        
        return results
    
    def _summarize_extracted_info(self, data) -> Dict:
        """æ±‡æ€»æŠ½å–çš„ä¿¡æ¯"""
        if hasattr(data, '__dict__'):
            # å¤„ç†dataclasså¯¹è±¡
            summary = {}
            for key, value in data.__dict__.items():
                if isinstance(value, str):
                    summary[key] = value[:100] + '...' if len(value) > 100 else value
                elif isinstance(value, list):
                    summary[key] = f"åˆ—è¡¨ï¼Œ{len(value)} é¡¹"
                elif isinstance(value, dict):
                    summary[key] = f"å­—å…¸ï¼Œ{len(value)} é”®"
                else:
                    summary[key] = str(value)
            return summary
        elif isinstance(data, dict):
            return {k: f"å­—å…¸é¡¹ ({type(v).__name__})" for k, v in list(data.items())[:10]}
        else:
            return {'type': type(data).__name__, 'content': str(data)[:100]}
    
    def benchmark_performance(self, text_samples: List[str]) -> Dict:
        """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print("\nğŸƒ å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•...")
        
        benchmark_results = {
            'test_samples': len(text_samples),
            'module_performance': {},
            'overall_performance': {}
        }
        
        total_chars = sum(len(text) for text in text_samples)
        print(f"æµ‹è¯•æ ·æœ¬: {len(text_samples)} ä¸ªï¼Œæ€»å­—ç¬¦æ•°: {total_chars}")
        
        # æµ‹è¯•æ–‡æœ¬æ¸…æ´—æ€§èƒ½
        print("\næµ‹è¯•æ–‡æœ¬æ¸…æ´—æ¨¡å—...")
        clean_start = time.time()
        for text in text_samples:
            self.text_cleaner.clean_text_basic(text)
        clean_time = time.time() - clean_start
        
        benchmark_results['module_performance']['text_cleaner'] = {
            'total_time': clean_time,
            'chars_per_second': total_chars / clean_time if clean_time > 0 else 0,
            'texts_per_second': len(text_samples) / clean_time if clean_time > 0 else 0
        }
        
        # æµ‹è¯•å†…å®¹åˆ†ç±»æ€§èƒ½
        print("æµ‹è¯•å†…å®¹åˆ†ç±»æ¨¡å—...")
        classify_start = time.time()
        for text in text_samples:
            self.content_classifier.classify_document(text)
        classify_time = time.time() - classify_start
        
        benchmark_results['module_performance']['content_classifier'] = {
            'total_time': classify_time,
            'chars_per_second': total_chars / classify_time if classify_time > 0 else 0,
            'texts_per_second': len(text_samples) / classify_time if classify_time > 0 else 0
        }
        
        # æµ‹è¯•ä¿¡æ¯æŠ½å–æ€§èƒ½
        print("æµ‹è¯•ä¿¡æ¯æŠ½å–æ¨¡å—...")
        extract_start = time.time()
        for text in text_samples:
            self.info_extractor.extract_all_info(text)
        extract_time = time.time() - extract_start
        
        benchmark_results['module_performance']['info_extractor'] = {
            'total_time': extract_time,
            'chars_per_second': total_chars / extract_time if extract_time > 0 else 0,
            'texts_per_second': len(text_samples) / extract_time if extract_time > 0 else 0
        }
        
        # æµ‹è¯•è´¨é‡æ£€æŸ¥æ€§èƒ½
        print("æµ‹è¯•è´¨é‡æ£€æŸ¥æ¨¡å—...")
        quality_start = time.time()
        for text in text_samples:
            self.quality_checker.check_quality(text)
        quality_time = time.time() - quality_start
        
        benchmark_results['module_performance']['quality_checker'] = {
            'total_time': quality_time,
            'chars_per_second': total_chars / quality_time if quality_time > 0 else 0,
            'texts_per_second': len(text_samples) / quality_time if quality_time > 0 else 0
        }
        
        # æ€»ä½“æ€§èƒ½
        total_benchmark_time = clean_time + classify_time + extract_time + quality_time
        benchmark_results['overall_performance'] = {
            'total_time': total_benchmark_time,
            'avg_chars_per_second': total_chars * 4 / total_benchmark_time if total_benchmark_time > 0 else 0,
            'avg_texts_per_second': len(text_samples) * 4 / total_benchmark_time if total_benchmark_time > 0 else 0
        }
        
        print("\nğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœ:")
        for module, perf in benchmark_results['module_performance'].items():
            print(f"  {module}: {perf['chars_per_second']:.0f} å­—ç¬¦/ç§’, {perf['texts_per_second']:.2f} æ–‡æ¡£/ç§’")
        
        print(f"\næ€»ä½“å¹³å‡æ€§èƒ½: {benchmark_results['overall_performance']['avg_chars_per_second']:.0f} å­—ç¬¦/ç§’")
        
        return benchmark_results
    
    def process_file(self, file_path: str, output_dir: str = None) -> Dict:
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        file_path = Path(file_path)
        
        if not file_path.exists():
            return {'error': f'æ–‡ä»¶ä¸å­˜åœ¨: {file_path}'}
        
        try:
            # è¯»å–æ–‡ä»¶
            with open(file_path, 'r', encoding='utf-8') as f:
                text = f.read()
            
            # è‡ªåŠ¨æ£€æµ‹æ–‡æœ¬ç±»å‹
            text_type = 'yijing' if any(keyword in text for keyword in ['å½–æ›°', 'è±¡æ›°', 'æ–‡è¨€']) else 'general'
            
            # å¤„ç†æ–‡æœ¬
            results = self.process_text_pipeline(text, text_type)
            results['file_info'] = {
                'file_path': str(file_path),
                'file_size': file_path.stat().st_size,
                'file_name': file_path.name
            }
            
            # ä¿å­˜ç»“æœ
            if output_dir:
                output_dir = Path(output_dir)
                output_dir.mkdir(parents=True, exist_ok=True)
                
                output_file = output_dir / f"{file_path.stem}_processed.json"
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(results, f, ensure_ascii=False, indent=2, default=str)
                
                print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
            
            return results
            
        except Exception as e:
            return {'error': f'å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {e}', 'file_path': str(file_path)}


def create_test_samples() -> List[str]:
    """åˆ›å»ºæµ‹è¯•æ ·æœ¬"""
    return [
        # æ˜“ç»æ–‡æœ¬ç¤ºä¾‹
        """
        ä¹¾å¦
        
        ä¹¾ï¼šå…ƒäº¨åˆ©è´ã€‚
        
        å½–æ›°ï¼šå¤§å“‰ä¹¾å…ƒï¼Œä¸‡ç‰©èµ„å§‹ï¼Œä¹ƒç»Ÿå¤©ã€‚äº‘è¡Œé›¨æ–½ï¼Œå“ç‰©æµå½¢ã€‚
        å¤§æ˜å§‹ç»ˆï¼Œå…­ä½æ—¶æˆï¼Œæ—¶ä¹˜å…­é¾™ä»¥å¾¡å¤©ã€‚ä¹¾é“å˜åŒ–ï¼Œå„æ­£æ€§å‘½ï¼Œ
        ä¿åˆå¤§å’Œï¼Œä¹ƒåˆ©è´ã€‚é¦–å‡ºåº¶ç‰©ï¼Œä¸‡å›½å’¸å®ã€‚
        
        è±¡æ›°ï¼šå¤©è¡Œå¥ï¼Œå›å­ä»¥è‡ªå¼ºä¸æ¯ã€‚
        
        åˆä¹ï¼šæ½œé¾™å‹¿ç”¨ã€‚
        è±¡æ›°ï¼šæ½œé¾™å‹¿ç”¨ï¼Œé˜³åœ¨ä¸‹ä¹Ÿã€‚
        
        ä¹äºŒï¼šè§é¾™åœ¨ç”°ï¼Œåˆ©è§å¤§äººã€‚
        è±¡æ›°ï¼šè§é¾™åœ¨ç”°ï¼Œå¾·æ–½æ™®ä¹Ÿã€‚
        
        ä¹ä¸‰ï¼šå›å­ç»ˆæ—¥ä¹¾ä¹¾ï¼Œå¤•æƒ•è‹¥å‰ï¼Œæ— å’ã€‚
        è±¡æ›°ï¼šç»ˆæ—¥ä¹¾ä¹¾ï¼Œåå¤é“ä¹Ÿã€‚
        """,
        
        # å…­çˆ»æ–‡æœ¬ç¤ºä¾‹
        """
        æµ‹äº‹ä¸šè¿åŠ¿
        
        èµ·å¦æ—¶é—´ï¼šç”²å­å¹´æ­£æœˆåˆä¸€å­æ—¶
        
        æœ¬å¦ï¼šä¹¾ä¸ºå¤©
        å˜å¦ï¼šå¤©é£å§¤
        
        å…­çˆ»æ’åˆ—ï¼š
        ä¸Šä¹ï¼šæˆŒåœŸå…„å¼Ÿ ä¸–
        ä¹äº”ï¼šç”³é‡‘çˆ¶æ¯
        ä¹å››ï¼šåˆç«å®˜é¬¼
        ä¹ä¸‰ï¼šè¾°åœŸå…„å¼Ÿ åº”
        ä¹äºŒï¼šå¯…æœ¨å¦»è´¢
        åˆä¹ï¼šå­æ°´å­å­™
        
        ç”¨ç¥ï¼šå¦»è´¢å¯…æœ¨
        åŸç¥ï¼šå­å­™å­æ°´
        å¿Œç¥ï¼šå…„å¼ŸæˆŒåœŸ
        
        æ–­è¯­ï¼šç”¨ç¥å¾—æœˆç”Ÿæ—¥æ‰¶ï¼Œåˆæœ‰åŸç¥ç›¸ç”Ÿï¼Œä¸»äº‹ä¸šæœ‰æˆã€‚
        """,
        
        # ç†è®ºæ–‡æœ¬ç¤ºä¾‹
        """
        å‘¨æ˜“çš„å“²å­¦æ€æƒ³
        
        å‘¨æ˜“ä½œä¸ºä¸­å›½å¤ä»£é‡è¦çš„å“²å­¦å…¸ç±ï¼Œè•´å«ç€æ·±åˆ»çš„è¾©è¯æ³•æ€æƒ³ã€‚
        å…¶æ ¸å¿ƒç†å¿µåŒ…æ‹¬ï¼š
        
        1. é˜´é˜³å¯¹ç«‹ç»Ÿä¸€ï¼šä¸‡ç‰©çš†æœ‰é˜´é˜³ï¼Œé˜´é˜³äº’æ ¹ã€äº’ç”¨ã€äº’è½¬ã€‚
        2. å˜åŒ–è§„å¾‹ï¼šæ˜“è€…ï¼Œå˜ä¹Ÿã€‚ä¸€åˆ‡äº‹ç‰©éƒ½åœ¨ä¸æ–­å˜åŒ–å‘å±•ä¸­ã€‚
        3. ä¸­å’Œä¹‹é“ï¼šè¿‡çŠ¹ä¸åŠï¼Œé€‚ä¸­ä¸ºç¾ã€‚
        4. å¤©äººåˆä¸€ï¼šäººä¸è‡ªç„¶å’Œè°ç»Ÿä¸€ï¼Œå¤©äººç›¸åº”ã€‚
        
        è¿™äº›æ€æƒ³å¯¹åä¸–çš„å“²å­¦ã€æ”¿æ²»ã€æ–‡åŒ–äº§ç”Ÿäº†æ·±è¿œå½±å“ã€‚
        """,
        
        # æ¡ˆä¾‹åˆ†æç¤ºä¾‹
        """
        å¦ä¾‹åˆ†æï¼šé—®è´¢è¿å¾—éœ‡å¦
        
        æŸäººé—®ä»Šå¹´è´¢è¿å¦‚ä½•ï¼Œæ‘‡å¾—éœ‡ä¸ºé›·å¦ï¼Œä¸‰çˆ»åŠ¨ã€‚
        
        å¦è±¡åˆ†æï¼š
        éœ‡å¦ä¸»åŠ¨ï¼Œåˆ©äºä¸»åŠ¨æ±‚è´¢ã€‚
        ä¸‰çˆ»åŠ¨å˜ï¼Œå˜å¦ä¸ºè‰®ï¼ŒåŠ¨ææ€é™ã€‚
        
        çˆ»è±¡åˆ†æï¼š
        åˆä¹ï¼šé›·åŠ¨äºä¸‹ï¼Œå°æœ‰æ³¢åŠ¨ï¼Œæ— å¤§å®³ã€‚
        å…­äºŒï¼šé˜´çˆ»å¾—æ­£ï¼Œå®ˆé™å¾…æ—¶ã€‚
        ä¹ä¸‰ï¼šåŠ¨çˆ»ï¼Œä¸»å˜åŒ–ï¼Œéœ€è°¨æ…è¡Œäº‹ã€‚
        
        ç»¼åˆæ–­è¯­ï¼š
        ä¸ŠåŠå¹´å®œä¸»åŠ¨æ±‚è´¢ï¼Œä½†éœ€é€‚å¯è€Œæ­¢ã€‚
        ä¸‹åŠå¹´å®œé™å®ˆï¼Œä¸å®œå¤§ä¸¾æŠ•èµ„ã€‚
        æ€»ä½“è´¢è¿å¹³ç¨³ï¼Œå°æœ‰æ‰€å¾—ã€‚
        """
    ]


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒŸ æ™ºèƒ½æ–‡æœ¬å¤„ç†å¼•æ“æµ‹è¯•")
    print("=" * 50)
    
    try:
        # åˆå§‹åŒ–å¤„ç†å¼•æ“
        engine = ProcessingEngine()
        
        # åˆ›å»ºæµ‹è¯•æ ·æœ¬
        test_samples = create_test_samples()
        
        # 1. æ¼”ç¤ºå®Œæ•´å¤„ç†æµæ°´çº¿
        print("\n" + "=" * 50)
        print("ğŸ“‹ æ¼”ç¤º1: å®Œæ•´æ–‡æœ¬å¤„ç†æµæ°´çº¿")
        print("=" * 50)
        
        sample_text = test_samples[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ ·æœ¬
        results = engine.process_text_pipeline(sample_text, 'yijing')
        
        # 2. æ€§èƒ½åŸºå‡†æµ‹è¯•
        print("\n" + "=" * 50)
        print("ğŸ“Š æ¼”ç¤º2: æ€§èƒ½åŸºå‡†æµ‹è¯•")
        print("=" * 50)
        
        benchmark_results = engine.benchmark_performance(test_samples[:3])  # ä½¿ç”¨å‰3ä¸ªæ ·æœ¬
        
        # 3. å¤„ç†å®é™…æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        print("\n" + "=" * 50)
        print("ğŸ“ æ¼”ç¤º3: å¤„ç†å®é™…æ–‡ä»¶")
        print("=" * 50)
        
        # æŸ¥æ‰¾å¯èƒ½çš„æµ‹è¯•æ–‡ä»¶
        test_files = []
        current_dir = Path('.')
        for pattern in ['*.txt', '*.doc', '*.md']:
            test_files.extend(list(current_dir.glob(pattern)))
        
        if test_files:
            print(f"å‘ç° {len(test_files)} ä¸ªå¯å¤„ç†çš„æ–‡ä»¶:")
            for i, file_path in enumerate(test_files[:3]):  # åªå¤„ç†å‰3ä¸ª
                print(f"  {i+1}. {file_path.name}")
            
            # å¤„ç†ç¬¬ä¸€ä¸ªæ–‡ä»¶ä½œä¸ºæ¼”ç¤º
            if test_files:
                file_results = engine.process_file(test_files[0], 'structured_data')
        else:
            print("æœªå‘ç°å¯å¤„ç†çš„æ–‡ä»¶ï¼Œè·³è¿‡æ–‡ä»¶å¤„ç†æ¼”ç¤º")
        
        # 4. ä¿å­˜å®Œæ•´çš„æµ‹è¯•ç»“æœ
        print("\n" + "=" * 50)
        print("ğŸ’¾ ä¿å­˜æµ‹è¯•ç»“æœ")
        print("=" * 50)
        
        output_dir = Path('structured_data')
        output_dir.mkdir(exist_ok=True)
        
        # ä¿å­˜æµæ°´çº¿ç»“æœ
        pipeline_output = output_dir / 'pipeline_test_results.json'
        with open(pipeline_output, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
        
        # ä¿å­˜æ€§èƒ½åŸºå‡†ç»“æœ
        benchmark_output = output_dir / 'benchmark_results.json'
        with open(benchmark_output, 'w', encoding='utf-8') as f:
            json.dump(benchmark_results, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"âœ… æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ° {output_dir}/")
        
        print("\nğŸ‰ æ™ºèƒ½æ–‡æœ¬å¤„ç†å¼•æ“æµ‹è¯•å®Œæˆ!")
        print("=" * 50)
        print("\nğŸ“ˆ æ€»ç»“:")
        print(f"  - æ–‡æœ¬æ¸…æ´—: âœ… åŠŸèƒ½æ­£å¸¸")
        print(f"  - å†…å®¹åˆ†ç±»: âœ… åŠŸèƒ½æ­£å¸¸") 
        print(f"  - ä¿¡æ¯æŠ½å–: âœ… åŠŸèƒ½æ­£å¸¸")
        print(f"  - è´¨é‡æ£€æŸ¥: âœ… åŠŸèƒ½æ­£å¸¸")
        print(f"  - æ€§èƒ½åŸºå‡†: âœ… æµ‹è¯•å®Œæˆ")
        print(f"  - æ‰¹é‡å¤„ç†: âœ… æ”¯æŒå®Œæ•´")
        
        print(f"\nâš¡ æ€§èƒ½æŒ‡æ ‡:")
        print(f"  - å¹³å‡å¤„ç†é€Ÿåº¦: {benchmark_results['overall_performance']['avg_chars_per_second']:.0f} å­—ç¬¦/ç§’")
        print(f"  - æ”¯æŒå¹¶å‘å¤„ç†: âœ…")
        print(f"  - å¢é‡å¤„ç†: âœ…")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()