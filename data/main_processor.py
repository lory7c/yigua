#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½æ–‡æœ¬å¤„ç†å¼•æ“ä¸»å¤„ç†å™¨
ç”¨äºå¤„ç†æ˜“ç»ã€å…­çˆ»ã€å¤§å…­å£¬ç­‰å¤ç±æ–‡æ¡£çš„ä¸»å…¥å£ç¨‹åº
æ”¯æŒå•æ–‡ä»¶å¤„ç†ã€æ‰¹é‡å¤„ç†ã€å¢é‡å¤„ç†ç­‰å¤šç§æ¨¡å¼
"""

import argparse
import json
import sys
from pathlib import Path
import time
from typing import List, Dict
import logging

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from text_cleaner import TextCleaner
from content_classifier import ContentClassifier
from info_extractor import InfoExtractor  
from quality_checker import QualityChecker

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class MainProcessor:
    """ä¸»å¤„ç†å™¨ç±»"""
    
    def __init__(self, max_workers: int = None):
        """
        åˆå§‹åŒ–ä¸»å¤„ç†å™¨
        
        Args:
            max_workers: æœ€å¤§å·¥ä½œè¿›ç¨‹æ•°
        """
        logger.info("åˆå§‹åŒ–æ™ºèƒ½æ–‡æœ¬å¤„ç†å¼•æ“...")
        
        self.text_cleaner = TextCleaner(max_workers)
        self.content_classifier = ContentClassifier()
        self.info_extractor = InfoExtractor()
        self.quality_checker = QualityChecker()
        
        logger.info("æ™ºèƒ½æ–‡æœ¬å¤„ç†å¼•æ“åˆå§‹åŒ–å®Œæˆ")
    
    def process_file(self, input_path: str, output_dir: str = None, 
                    text_type: str = 'auto', quality_check: bool = True) -> Dict:
        """
        å¤„ç†å•ä¸ªæ–‡ä»¶
        
        Args:
            input_path: è¾“å…¥æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            text_type: æ–‡æœ¬ç±»å‹ ('auto', 'yijing', 'liuyao', 'general')
            quality_check: æ˜¯å¦è¿›è¡Œè´¨é‡æ£€æŸ¥
            
        Returns:
            å¤„ç†ç»“æœ
        """
        input_path = Path(input_path)
        
        if not input_path.exists():
            logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
            return {'error': f'æ–‡ä»¶ä¸å­˜åœ¨: {input_path}'}
        
        logger.info(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {input_path.name}")
        start_time = time.time()
        
        try:
            # è¯»å–æ–‡ä»¶
            with open(input_path, 'r', encoding='utf-8', errors='ignore') as f:
                original_text = f.read()
            
            if not original_text.strip():
                logger.warning(f"æ–‡ä»¶ä¸ºç©º: {input_path}")
                return {'error': f'æ–‡ä»¶ä¸ºç©º: {input_path}'}
            
            # è‡ªåŠ¨æ£€æµ‹æ–‡æœ¬ç±»å‹
            if text_type == 'auto':
                if any(keyword in original_text for keyword in ['å½–æ›°', 'è±¡æ›°', 'æ–‡è¨€']):
                    text_type = 'yijing'
                elif any(keyword in original_text for keyword in ['ä¸–', 'åº”', 'å…­çˆ»', 'æ‘‡å¦']):
                    text_type = 'liuyao'
                else:
                    text_type = 'general'
            
            logger.info(f"æ£€æµ‹åˆ°æ–‡æœ¬ç±»å‹: {text_type}")
            
            # 1. æ–‡æœ¬æ¸…æ´—
            logger.info("æ‰§è¡Œæ–‡æœ¬æ¸…æ´—...")
            if text_type == 'yijing':
                cleaned_text = self.text_cleaner.clean_yijing_text(original_text)
            else:
                cleaned_text = self.text_cleaner.clean_text_basic(original_text)
            
            cleaned_text = self.text_cleaner.normalize_characters(cleaned_text)
            
            # 2. å†…å®¹åˆ†ç±»
            logger.info("æ‰§è¡Œå†…å®¹åˆ†ç±»...")
            classification_results = self.content_classifier.classify_document(cleaned_text)
            merged_results = self.content_classifier.merge_adjacent_segments(classification_results)
            structure_analysis = self.content_classifier.analyze_document_structure(merged_results)
            
            # 3. ä¿¡æ¯æŠ½å–
            logger.info("æ‰§è¡Œä¿¡æ¯æŠ½å–...")
            extracted_info = self.info_extractor.extract_structured_info(cleaned_text, text_type)
            
            # 4. è´¨é‡æ£€æŸ¥ï¼ˆå¯é€‰ï¼‰
            quality_report = None
            if quality_check:
                logger.info("æ‰§è¡Œè´¨é‡æ£€æŸ¥...")
                quality_report = self.quality_checker.check_quality(cleaned_text, text_type)
            
            # æ•´åˆç»“æœ
            processing_time = time.time() - start_time
            result = {
                'file_info': {
                    'input_path': str(input_path),
                    'file_name': input_path.name,
                    'file_size': input_path.stat().st_size,
                    'text_type': text_type
                },
                'processing_results': {
                    'original_length': len(original_text),
                    'cleaned_length': len(cleaned_text),
                    'compression_ratio': (len(original_text) - len(cleaned_text)) / len(original_text) if len(original_text) > 0 else 0,
                    'cleaned_text': cleaned_text,
                    'classification': {
                        'segments_count': len(merged_results),
                        'structure_analysis': structure_analysis,
                        'segments': [
                            {
                                'type': r.content_type,
                                'confidence': r.confidence,
                                'start_position': r.start_position,
                                'end_position': r.end_position,
                                'text_preview': r.text_segment[:100] + '...' if len(r.text_segment) > 100 else r.text_segment
                            }
                            for r in merged_results
                        ]
                    },
                    'extracted_info': {
                        'type': extracted_info['type'],
                        'data': self._serialize_extracted_data(extracted_info['data'])
                    },
                    'quality_report': {
                        'overall_score': quality_report.overall_score if quality_report else None,
                        'total_issues': quality_report.total_issues if quality_report else None,
                        'issue_summary': {
                            'critical': quality_report.critical_issues if quality_report else 0,
                            'major': quality_report.major_issues if quality_report else 0,
                            'minor': quality_report.minor_issues if quality_report else 0,
                            'warnings': quality_report.warnings if quality_report else 0
                        } if quality_report else None,
                        'recommendations': quality_report.recommendations[:5] if quality_report else []
                    }
                },
                'processing_time': processing_time,
                'timestamp': time.time()
            }
            
            # ä¿å­˜ç»“æœ
            if output_dir:
                output_dir = Path(output_dir)
                output_dir.mkdir(parents=True, exist_ok=True)
                
                # ä¿å­˜æ¸…æ´—åçš„æ–‡æœ¬
                cleaned_file = output_dir / f"{input_path.stem}_cleaned.txt"
                with open(cleaned_file, 'w', encoding='utf-8') as f:
                    f.write(cleaned_text)
                
                # ä¿å­˜å¤„ç†ç»“æœ
                result_file = output_dir / f"{input_path.stem}_results.json"
                with open(result_file, 'w', encoding='utf-8') as f:
                    json.dump(result, f, ensure_ascii=False, indent=2, default=str)
                
                logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
                result['output_files'] = {
                    'cleaned_text': str(cleaned_file),
                    'results_json': str(result_file)
                }
            
            logger.info(f"æ–‡ä»¶å¤„ç†å®Œæˆ: {input_path.name}, è€—æ—¶: {processing_time:.2f}s")
            return result
            
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return {
                'error': str(e),
                'file_path': str(input_path),
                'processing_time': time.time() - start_time
            }
    
    def _serialize_extracted_data(self, data):
        """åºåˆ—åŒ–æŠ½å–çš„æ•°æ®"""
        if hasattr(data, '__dict__'):
            result = {}
            for key, value in data.__dict__.items():
                if isinstance(value, str):
                    result[key] = value
                elif isinstance(value, list):
                    result[key] = [self._serialize_extracted_data(item) for item in value]
                elif isinstance(value, dict):
                    result[key] = {k: str(v) for k, v in value.items()}
                else:
                    result[key] = str(value)
            return result
        elif isinstance(data, dict):
            return {k: self._serialize_extracted_data(v) for k, v in data.items()}
        elif isinstance(data, list):
            return [self._serialize_extracted_data(item) for item in data]
        else:
            return str(data)
    
    def batch_process(self, input_dir: str, output_dir: str = None,
                     file_patterns: List[str] = None, text_type: str = 'auto',
                     quality_check: bool = True) -> Dict:
        """
        æ‰¹é‡å¤„ç†æ–‡ä»¶
        
        Args:
            input_dir: è¾“å…¥ç›®å½•
            output_dir: è¾“å‡ºç›®å½•
            file_patterns: æ–‡ä»¶æ¨¡å¼åˆ—è¡¨
            text_type: æ–‡æœ¬ç±»å‹
            quality_check: æ˜¯å¦è¿›è¡Œè´¨é‡æ£€æŸ¥
            
        Returns:
            æ‰¹é‡å¤„ç†ç»“æœ
        """
        input_dir = Path(input_dir)
        
        if not input_dir.exists():
            logger.error(f"è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}")
            return {'error': f'è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}'}
        
        # é»˜è®¤æ–‡ä»¶æ¨¡å¼
        if file_patterns is None:
            file_patterns = ['*.txt', '*.md', '*.doc']
        
        # æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„æ–‡ä»¶
        files_to_process = []
        for pattern in file_patterns:
            files_to_process.extend(list(input_dir.glob(f"**/{pattern}")))
        
        if not files_to_process:
            logger.warning(f"åœ¨ç›®å½• {input_dir} ä¸­æœªæ‰¾åˆ°åŒ¹é…çš„æ–‡ä»¶")
            return {'warning': f'åœ¨ç›®å½• {input_dir} ä¸­æœªæ‰¾åˆ°åŒ¹é…çš„æ–‡ä»¶'}
        
        logger.info(f"æ‰¾åˆ° {len(files_to_process)} ä¸ªæ–‡ä»¶å¾…å¤„ç†")
        
        # æ‰¹é‡å¤„ç†
        start_time = time.time()
        results = []
        success_count = 0
        failed_count = 0
        
        for file_path in files_to_process:
            logger.info(f"å¤„ç†æ–‡ä»¶ ({len(results)+1}/{len(files_to_process)}): {file_path.name}")
            
            file_output_dir = None
            if output_dir:
                file_output_dir = Path(output_dir) / file_path.stem
            
            result = self.process_file(file_path, file_output_dir, text_type, quality_check)
            
            if 'error' not in result:
                success_count += 1
            else:
                failed_count += 1
            
            results.append(result)
        
        total_time = time.time() - start_time
        
        batch_result = {
            'batch_info': {
                'input_dir': str(input_dir),
                'output_dir': output_dir,
                'total_files': len(files_to_process),
                'success_count': success_count,
                'failed_count': failed_count,
                'file_patterns': file_patterns,
                'text_type': text_type,
                'quality_check': quality_check
            },
            'processing_time': total_time,
            'average_time_per_file': total_time / len(files_to_process) if files_to_process else 0,
            'results': results,
            'timestamp': time.time()
        }
        
        # ä¿å­˜æ‰¹é‡å¤„ç†æ±‡æ€»
        if output_dir:
            summary_file = Path(output_dir) / 'batch_processing_summary.json'
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(batch_result, f, ensure_ascii=False, indent=2, default=str)
            
            logger.info(f"æ‰¹é‡å¤„ç†æ±‡æ€»å·²ä¿å­˜åˆ°: {summary_file}")
        
        logger.info(f"æ‰¹é‡å¤„ç†å®Œæˆ: {success_count} æˆåŠŸ, {failed_count} å¤±è´¥, æ€»è€—æ—¶: {total_time:.2f}s")
        
        return batch_result
    
    def incremental_process(self, input_dir: str, output_dir: str,
                           text_type: str = 'auto') -> Dict:
        """å¢é‡å¤„ç†"""
        return self.text_cleaner.incremental_clean(input_dir, output_dir, text_type)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æ™ºèƒ½æ–‡æœ¬å¤„ç†å¼•æ“')
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument('--input', '-i', required=True, help='è¾“å…¥æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„')
    parser.add_argument('--output', '-o', help='è¾“å‡ºç›®å½•è·¯å¾„')
    parser.add_argument('--type', '-t', choices=['auto', 'yijing', 'liuyao', 'general'], 
                       default='auto', help='æ–‡æœ¬ç±»å‹')
    parser.add_argument('--batch', '-b', action='store_true', help='æ‰¹é‡å¤„ç†æ¨¡å¼')
    parser.add_argument('--incremental', action='store_true', help='å¢é‡å¤„ç†æ¨¡å¼')
    parser.add_argument('--no-quality-check', action='store_true', help='è·³è¿‡è´¨é‡æ£€æŸ¥')
    parser.add_argument('--workers', '-w', type=int, help='å·¥ä½œè¿›ç¨‹æ•°')
    parser.add_argument('--patterns', nargs='+', default=['*.txt', '*.md'], 
                       help='æ–‡ä»¶æ¨¡å¼ï¼ˆæ‰¹é‡æ¨¡å¼ç”¨ï¼‰')
    
    args = parser.parse_args()
    
    try:
        # åˆå§‹åŒ–å¤„ç†å™¨
        processor = MainProcessor(max_workers=args.workers)
        
        # æ ¹æ®æ¨¡å¼æ‰§è¡Œå¤„ç†
        if args.incremental:
            logger.info("æ‰§è¡Œå¢é‡å¤„ç†æ¨¡å¼")
            result = processor.incremental_process(args.input, args.output, args.type)
            
        elif args.batch:
            logger.info("æ‰§è¡Œæ‰¹é‡å¤„ç†æ¨¡å¼")
            result = processor.batch_process(
                args.input, args.output, args.patterns, 
                args.type, not args.no_quality_check
            )
            
        else:
            logger.info("æ‰§è¡Œå•æ–‡ä»¶å¤„ç†æ¨¡å¼")
            result = processor.process_file(
                args.input, args.output, args.type, 
                not args.no_quality_check
            )
        
        # è¾“å‡ºç»“æœæ‘˜è¦
        if 'error' not in result:
            print("\nâœ… å¤„ç†å®Œæˆ!")
            if 'batch_info' in result:
                info = result['batch_info']
                print(f"ğŸ“Š æ‰¹é‡å¤„ç†ç»Ÿè®¡: {info['success_count']}/{info['total_files']} æˆåŠŸ")
            elif 'processed' in result:
                print(f"ğŸ“Š å¢é‡å¤„ç†ç»Ÿè®¡: {result['processed']} ä¸ªæ–‡ä»¶")
            else:
                print(f"ğŸ“Š æ–‡ä»¶å¤„ç†å®Œæˆ: {result.get('processing_time', 0):.2f}s")
        else:
            print(f"âŒ å¤„ç†å¤±è´¥: {result['error']}")
            sys.exit(1)
            
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­å¤„ç†")
        sys.exit(0)
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()