#!/usr/bin/env python3
"""
ç”Ÿäº§çº§æ˜“å­¦PDFæ–‡æ¡£æ‰¹é‡æå–ç®¡é“ - Production Extract Pipeline
æ•´åˆæœ€ä½³æå–æ–¹æ³•ã€å¹¶å‘å¤„ç†ã€æ–­ç‚¹ç»­ä¼ ã€æ ‡å‡†åŒ–è¾“å‡º

åŸºäºETL_Architecture_Design.mdæ–¹æ¡ˆå®ç°
- pdfplumber+PyMuPDFåŒé‡æå–
- multiprocessingå¹¶å‘æ‰¹å¤„ç†
- æ™ºèƒ½æ–­ç‚¹ç»­ä¼ æœºåˆ¶
- æ ‡å‡†åŒ–JSONè¾“å‡ºæ ¼å¼
- å®æ—¶è¿›åº¦ç›‘æ§å’Œé”™è¯¯å¤„ç†
- å†…å­˜ä¼˜åŒ–å’Œæ€§èƒ½è°ƒä¼˜

ç›®æ ‡: 191ä¸ªPDFæ–‡ä»¶ï¼Œ3å°æ—¶å†…å®Œæˆå¤„ç†
"""

import os
import sys
import json
import re
import time
import pickle
import hashlib
import logging
import multiprocessing as mp
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any, NamedTuple
from dataclasses import dataclass, asdict
from concurrent.futures import ProcessPoolExecutor, as_completed
import signal
import psutil
import gc
import resource
from collections import defaultdict, deque

# PDFå¤„ç†åº“
try:
    import pdfplumber
    import fitz  # PyMuPDF
    import PyPDF2
    from tqdm import tqdm
    import pandas as pd
    from functools import lru_cache
except ImportError as e:
    print(f"ç¼ºå°‘ä¾èµ–åº“: {e}")
    print("è¯·å®‰è£…: pip install pdfplumber pymupdf tqdm pandas")
    sys.exit(1)


# ============================================================================
# æ•°æ®ç»“æ„å®šä¹‰
# ============================================================================

@dataclass
class ProcessingConfig:
    """å¤„ç†é…ç½®"""
    source_dir: str
    output_dir: str
    max_workers: int = 6
    batch_size: int = 8
    memory_limit_gb: float = 3.0
    time_limit_hours: float = 3.0
    cache_enabled: bool = True
    resume_enabled: bool = True
    min_text_length: int = 50
    max_pages_per_file: int = 500
    
@dataclass
class FileMetadata:
    """æ–‡ä»¶å…ƒæ•°æ®"""
    file_path: str
    file_name: str
    file_size: int
    file_hash: str
    pages: int
    category: str
    priority: int
    confidence: float
    processing_time: float
    processed_at: str
    method_used: str

@dataclass 
class ExtractionResult:
    """æå–ç»“æœ"""
    metadata: FileMetadata
    raw_text: str
    structured_content: Dict[str, Any]
    statistics: Dict[str, int]
    quality_metrics: Dict[str, float]
    error_log: List[str]

class ProcessingProgress(NamedTuple):
    """å¤„ç†è¿›åº¦"""
    completed: int
    total: int
    success: int
    failed: int
    cached: int
    elapsed_time: float
    estimated_remaining: float


# ============================================================================
# æ ¸å¿ƒæå–å¼•æ“
# ============================================================================

class ProductionPDFExtractor:
    """ç”Ÿäº§çº§PDFæå–å™¨ - å¤šæ–¹æ³•èåˆ"""
    
    def __init__(self, config: ProcessingConfig):
        self.config = config
        self.logger = self._setup_logging()
        
        # 64å¦åç§°å’Œæ˜“å­¦æœ¯è¯­
        self.hexagram_names = [
            "ä¹¾", "å¤", "å±¯", "è’™", "éœ€", "è®¼", "å¸ˆ", "æ¯”",
            "å°ç•œ", "å±¥", "æ³°", "å¦", "åŒäºº", "å¤§æœ‰", "è°¦", "è±«",
            "éš", "è›Š", "ä¸´", "è§‚", "å™¬å—‘", "è´²", "å‰¥", "å¤",
            "æ— å¦„", "å¤§ç•œ", "é¢", "å¤§è¿‡", "å", "ç¦»", "å’¸", "æ’",
            "é", "å¤§å£®", "æ™‹", "æ˜å¤·", "å®¶äºº", "ç½", "è¹‡", "è§£",
            "æŸ", "ç›Š", "å¤¬", "å§¤", "èƒ", "å‡", "å›°", "äº•",
            "é©", "é¼", "éœ‡", "è‰®", "æ¸", "å½’å¦¹", "ä¸°", "æ—…",
            "å·½", "å…‘", "æ¶£", "èŠ‚", "ä¸­å­š", "å°è¿‡", "æ—¢æµ", "æœªæµ"
        ]
        
        # åˆ†ç±»æ¨¡å¼ - ç²¾ç¡®åŒ¹é…
        self.category_patterns = {
            "å…­çˆ»": {
                "keywords": ["å…­çˆ»", "åœæ˜“", "å¢åˆ ", "ç«ç æ—", "é»„é‡‘ç­–", "ç­®", "å¦è±¡", "çˆ»è¾", "ä¸–åº”", "å…­äº²"],
                "priority": 1,
                "patterns": [r"å…­çˆ»", r"ä¸–åº”", r"ç”¨ç¥", r"å¿Œç¥", r"åŠ¨çˆ»", r"å˜çˆ»", r"é£ç¥", r"ä¼ç¥"]
            },
            "å¤§å…­å£¬": {
                "keywords": ["å…­å£¬", "å£¬å­¦", "å£¬å ", "è¯¾ä¼ ", "ç¥å°†", "å››è¯¾", "ä¸‰ä¼ ", "åäºŒç¥"],
                "priority": 1,
                "patterns": [r"å…­å£¬", r"è¯¾ä¼ ", r"å››è¯¾", r"ä¸‰ä¼ ", r"å¤©ç½¡", r"å¤ªå†²"]
            },
            "å‘¨æ˜“åŸºç¡€": {
                "keywords": ["å‘¨æ˜“", "æ˜“ç»", "å…«å¦", "å…­åå››å¦", "å¦è¾", "è±¡ä¼ ", "å½–ä¼ "],
                "priority": 1,
                "patterns": [r"å‘¨æ˜“", r"æ˜“ç»", r"å…«å¦", r"å¦è¾", r"è±¡ä¼ ", r"å½–ä¼ "]
            },
            "æ¢…èŠ±æ˜“æ•°": {
                "keywords": ["æ¢…èŠ±", "æ˜“æ•°", "è§‚æ¢…", "æ•°ç†", "å…ˆå¤©", "åå¤©", "ä½“ç”¨"],
                "priority": 2,
                "patterns": [r"æ¢…èŠ±.{0,3}æ˜“", r"è§‚æ¢…", r"ä½“å¦", r"ç”¨å¦", r"äº’å¦", r"å˜å¦"]
            },
            "ç´«å¾®æ–—æ•°": {
                "keywords": ["ç´«å¾®", "æ–—æ•°", "å‘½ç›˜", "å®«ä½", "æ˜Ÿæ›œ", "ä¸»æ˜Ÿ", "åŒ–ç¦„", "åŒ–æƒ", "åŒ–ç§‘", "åŒ–å¿Œ"],
                "priority": 2,
                "patterns": [r"ç´«å¾®.{0,3}æ–—æ•°", r"å‘½å®«", r"èº«å®«", r"åäºŒå®«", r"åå››ä¸»æ˜Ÿ"]
            },
            "å¥‡é—¨éç”²": {
                "keywords": ["å¥‡é—¨", "éç”²", "ä¹å®«", "å…«é—¨", "ä¸‰å¥‡", "å…­ä»ª", "å€¼ç¬¦", "å€¼ä½¿"],
                "priority": 2,
                "patterns": [r"å¥‡é—¨.{0,3}éç”²", r"ä¹å®«", r"å…«é—¨", r"ä¸‰å¥‡å…­ä»ª", r"å€¼ç¬¦", r"å€¼ä½¿"]
            },
            "å…«å­—å‘½ç†": {
                "keywords": ["å…«å­—", "å››æŸ±", "å‘½ç†", "å¹²æ”¯", "çº³éŸ³", "åç¥", "ç”¨ç¥", "å–œç”¨ç¥"],
                "priority": 2,
                "patterns": [r"å…«å­—", r"å››æŸ±", r"åç¥", r"é£Ÿç¥", r"ä¼¤å®˜", r"æ­£è´¢", r"åè´¢"]
            },
            "é‡‘å£è¯€": {
                "keywords": ["é‡‘å£è¯€", "é‡‘å£", "è¯¾å¼", "ç«‹è¯¾", "å››ä½", "è´µç¥"],
                "priority": 3,
                "patterns": [r"é‡‘å£.{0,3}è¯€", r"ç«‹è¯¾", r"å››ä½", r"è´µç¥"]
            },
            "å…¶ä»–æœ¯æ•°": {
                "keywords": ["å¤ªä¹™", "æ²³æ´›", "é£æ°´", "ç›¸æœ¯", "å åœ", "é¢„æµ‹"],
                "priority": 4,
                "patterns": [r"å¤ªä¹™", r"æ²³æ´›", r"ç†æ•°", r"æ²³å›¾", r"æ´›ä¹¦"]
            }
        }
        
        # æ€§èƒ½ç›‘æ§
        self._method_performance = defaultdict(lambda: {"success": 0, "failure": 0, "total_time": 0.0})
        self._memory_usage_history = deque(maxlen=100)
        
    def _setup_logging(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        log_dir = Path(self.config.output_dir) / "logs"
        log_dir.mkdir(parents=True, exist_ok=True)
        
        logger = logging.getLogger("ProductionExtractor")
        logger.setLevel(logging.INFO)
        
        # æ¸…é™¤ç°æœ‰çš„handler
        for handler in logger.handlers:
            logger.removeHandler(handler)
        
        # æ–‡ä»¶handler
        file_handler = logging.FileHandler(
            log_dir / f"production_extract_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log",
            encoding='utf-8'
        )
        file_handler.setLevel(logging.DEBUG)
        
        # æ§åˆ¶å°handler
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # æ ¼å¼åŒ–
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        logger.addHandler(file_handler)
        logger.addHandler(console_handler)
        
        return logger
    
    def get_file_hash(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œ"""
        stat = file_path.stat()
        content = f"{file_path}_{stat.st_size}_{stat.st_mtime}_{stat.st_ctime}"
        return hashlib.sha256(content.encode()).hexdigest()[:16]
    
    def extract_with_pdfplumber(self, pdf_path: Path) -> Tuple[str, Dict[str, Any]]:
        """pdfplumberæå– - ä¸»è¦æ–¹æ³•"""
        start_time = time.time()
        text_parts = []
        metadata = {
            'method': 'pdfplumber',
            'pages_processed': [],
            'tables_extracted': 0,
            'errors': []
        }
        
        try:
            with pdfplumber.open(pdf_path) as pdf:
                total_pages = len(pdf.pages)
                metadata['total_pages'] = total_pages
                max_pages = min(total_pages, self.config.max_pages_per_file)
                
                for i in range(max_pages):
                    try:
                        page = pdf.pages[i]
                        
                        # ä¼˜åŒ–å‚æ•°çš„æ–‡æœ¬æå–
                        page_text = page.extract_text(
                            x_tolerance=3,
                            y_tolerance=3,
                            layout=True,
                            use_text_flow=True
                        )
                        
                        if page_text and len(page_text.strip()) > 20:
                            text_parts.append(page_text)
                            metadata['pages_processed'].append(i + 1)
                        
                        # è¡¨æ ¼æå– - ä»…å‰50é¡µï¼Œé¿å…æ€§èƒ½é—®é¢˜
                        if i < 50 and len(page_text.strip()) < 2000:
                            try:
                                tables = page.extract_tables()
                                for table in tables[:2]:  # æœ€å¤š2ä¸ªè¡¨æ ¼
                                    if len(table) <= 30:  # é¿å…å·¨å¤§è¡¨æ ¼
                                        table_text = self._format_table(table)
                                        if table_text:
                                            text_parts.append(f"\n[è¡¨æ ¼{metadata['tables_extracted']+1}]\n{table_text}\n[/è¡¨æ ¼]\n")
                                            metadata['tables_extracted'] += 1
                            except Exception as e:
                                metadata['errors'].append(f"è¡¨æ ¼æå–å¤±è´¥ é¡µ{i+1}: {str(e)}")
                        
                        # å†…å­˜ç®¡ç†
                        if i > 0 and i % 50 == 0:
                            gc.collect()
                            
                    except Exception as e:
                        metadata['errors'].append(f"é¡µé¢{i+1}å¤„ç†å¤±è´¥: {str(e)}")
                        continue
                
                if max_pages < total_pages:
                    metadata['truncated_pages'] = total_pages - max_pages
                    
        except Exception as e:
            raise Exception(f"pdfplumberå¤„ç†å¤±è´¥: {e}")
        
        full_text = '\n'.join(text_parts)
        processing_time = time.time() - start_time
        
        # è®°å½•æ€§èƒ½
        self._method_performance['pdfplumber']['total_time'] += processing_time
        self._method_performance['pdfplumber']['success'] += 1
        
        metadata.update({
            'processing_time': processing_time,
            'text_length': len(full_text),
            'pages_ratio': len(metadata['pages_processed']) / total_pages if total_pages > 0 else 0
        })
        
        return full_text, metadata
    
    def extract_with_pymupdf(self, pdf_path: Path) -> Tuple[str, Dict[str, Any]]:
        """PyMuPDFæå– - å¤‡é€‰æ–¹æ³•"""
        start_time = time.time()
        text_parts = []
        metadata = {
            'method': 'pymupdf',
            'pages_processed': [],
            'images_processed': 0,
            'errors': []
        }
        
        doc = None
        try:
            doc = fitz.open(pdf_path)
            total_pages = len(doc)
            metadata['total_pages'] = total_pages
            max_pages = min(total_pages, self.config.max_pages_per_file)
            
            for page_num in range(max_pages):
                try:
                    page = doc.load_page(page_num)
                    
                    # æ–‡æœ¬æå–
                    page_text = page.get_text("text", 
                                            flags=fitz.TEXT_PRESERVE_WHITESPACE | fitz.TEXT_PRESERVE_IMAGES)
                    
                    if page_text and len(page_text.strip()) > 15:
                        text_parts.append(page_text)
                        metadata['pages_processed'].append(page_num + 1)
                    
                    # å›¾ç‰‡æ–‡å­—è¯†åˆ« - ä»…å‰20é¡µï¼Œæ–‡å­—å°‘çš„é¡µé¢
                    elif len(page_text.strip()) < 50 and page_num < 20:
                        try:
                            # ç®€å•å›¾ç‰‡è½¬æ–‡å­—ï¼ˆä¸ä¾èµ–OCRï¼‰
                            image_list = page.get_images()
                            if image_list:
                                metadata['images_processed'] += len(image_list)
                                # è¿™é‡Œå¯ä»¥æ·»åŠ OCRé€»è¾‘ï¼Œä½†ä¸ºä¿æŒæ€§èƒ½æš‚æ—¶è·³è¿‡
                        except Exception as e:
                            metadata['errors'].append(f"å›¾ç‰‡å¤„ç†å¤±è´¥ é¡µ{page_num+1}: {str(e)}")
                    
                    # å†…å­˜ç®¡ç†
                    if page_num > 0 and page_num % 100 == 0:
                        gc.collect()
                        
                except Exception as e:
                    metadata['errors'].append(f"é¡µé¢{page_num+1}å¤„ç†å¤±è´¥: {str(e)}")
                    continue
            
            if max_pages < total_pages:
                metadata['truncated_pages'] = total_pages - max_pages
                
        finally:
            if doc:
                doc.close()
        
        full_text = '\n'.join(text_parts)
        processing_time = time.time() - start_time
        
        # è®°å½•æ€§èƒ½
        self._method_performance['pymupdf']['total_time'] += processing_time
        self._method_performance['pymupdf']['success'] += 1
        
        metadata.update({
            'processing_time': processing_time,
            'text_length': len(full_text),
            'pages_ratio': len(metadata['pages_processed']) / total_pages if total_pages > 0 else 0
        })
        
        return full_text, metadata
    
    def extract_with_pypdf2(self, pdf_path: Path) -> Tuple[str, Dict[str, Any]]:
        """PyPDF2æå– - æœ€åå¤‡é€‰"""
        start_time = time.time()
        text_parts = []
        metadata = {
            'method': 'pypdf2',
            'pages_processed': [],
            'errors': []
        }
        
        try:
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                total_pages = len(pdf_reader.pages)
                metadata['total_pages'] = total_pages
                max_pages = min(total_pages, 100)  # PyPDF2æ€§èƒ½è¾ƒæ…¢ï¼Œé™åˆ¶æ›´å°‘é¡µæ•°
                
                for i in range(max_pages):
                    try:
                        page = pdf_reader.pages[i]
                        page_text = page.extract_text()
                        
                        if page_text and len(page_text.strip()) > 10:
                            text_parts.append(page_text)
                            metadata['pages_processed'].append(i + 1)
                            
                    except Exception as e:
                        metadata['errors'].append(f"é¡µé¢{i+1}å¤„ç†å¤±è´¥: {str(e)}")
                        continue
                
                if max_pages < total_pages:
                    metadata['truncated_pages'] = total_pages - max_pages
                    
        except Exception as e:
            raise Exception(f"PyPDF2å¤„ç†å¤±è´¥: {e}")
        
        full_text = '\n'.join(text_parts)
        processing_time = time.time() - start_time
        
        # è®°å½•æ€§èƒ½
        self._method_performance['pypdf2']['total_time'] += processing_time
        self._method_performance['pypdf2']['success'] += 1
        
        metadata.update({
            'processing_time': processing_time,
            'text_length': len(full_text),
            'pages_ratio': len(metadata['pages_processed']) / total_pages if total_pages > 0 else 0
        })
        
        return full_text, metadata
    
    def _format_table(self, table) -> str:
        """æ ¼å¼åŒ–è¡¨æ ¼æ•°æ®"""
        try:
            formatted_rows = []
            for row in table:
                if row and any(cell for cell in row):
                    formatted_row = '\t'.join([
                        str(cell).strip() if cell is not None else '' 
                        for cell in row
                    ])
                    if formatted_row.strip():
                        formatted_rows.append(formatted_row)
            return '\n'.join(formatted_rows)
        except Exception:
            return ''
    
    def classify_content(self, text: str, filename: str) -> Tuple[str, float, int]:
        """æ™ºèƒ½åˆ†ç±»ç®—æ³•"""
        if not text:
            return "å…¶ä»–æœ¯æ•°", 0.0, 5
        
        text_lower = text.lower()
        filename_lower = filename.lower()
        
        category_scores = {}
        
        for category, config in self.category_patterns.items():
            score = 0.0
            
            # æ–‡ä»¶ååŒ¹é… (æƒé‡é«˜)
            filename_matches = sum(1 for kw in config["keywords"] if kw in filename_lower)
            score += filename_matches * 5.0
            
            # æ­£åˆ™æ¨¡å¼åŒ¹é…
            for pattern in config["patterns"]:
                matches = len(re.findall(pattern, text, re.IGNORECASE))
                score += matches * 2.0
            
            # å…³é”®è¯å¯†åº¦
            total_keywords = sum(text_lower.count(kw) for kw in config["keywords"])
            if len(text) > 100:
                density = (total_keywords / len(text)) * 10000
                score += density
            
            # ç‰¹æ®ŠåŠ åˆ†é¡¹
            if category == "å…­çˆ»" and any(name in text for name in ["ç‹è™åº”", "å¢åˆ åœæ˜“", "ç«ç æ—"]):
                score += 10.0
            elif category == "å‘¨æ˜“åŸºç¡€" and any(name in text for name in self.hexagram_names[:10]):
                score += 8.0
            elif category == "å¤§å…­å£¬" and "å£¬å " in text:
                score += 8.0
            
            category_scores[category] = score
        
        if not category_scores or max(category_scores.values()) == 0:
            return "å…¶ä»–æœ¯æ•°", 0.0, 5
        
        best_category = max(category_scores, key=category_scores.get)
        best_score = category_scores[best_category]
        
        # å½’ä¸€åŒ–ç½®ä¿¡åº¦
        confidence = min(best_score / 20.0, 1.0)
        priority = self.category_patterns[best_category]["priority"]
        
        return best_category, confidence, priority
    
    def extract_structured_content(self, text: str) -> Dict[str, Any]:
        """æå–ç»“æ„åŒ–å†…å®¹"""
        content = {
            "hexagrams": [],
            "yao_ci": [],
            "annotations": [],
            "cases": [],
            "keywords": [],
            "author": None,
            "dynasty": None
        }
        
        # æå–64å¦ä¿¡æ¯
        for i, name in enumerate(self.hexagram_names):
            patterns = [
                rf"(?:ç¬¬?\s*)?{name}[å¦]?[ï¼š:]\s*([^ã€‚\n]{10,200})",
                rf"{name}[å¦]?\s*([^ã€‚\n]{20,100})"
            ]
            
            for pattern in patterns:
                matches = re.finditer(pattern, text)
                for match in matches:
                    description = match.group(1).strip()
                    if len(description) >= 10:
                        content["hexagrams"].append({
                            "number": i + 1,
                            "name": name,
                            "description": description,
                            "position": match.start()
                        })
                        break
        
        # æå–çˆ»è¾
        yao_positions = ["åˆ", "äºŒ", "ä¸‰", "å››", "äº”", "ä¸Š"]
        yao_types = ["å…­", "ä¹"]
        
        for pos in yao_positions:
            for yao_type in yao_types:
                pattern = rf"({pos}{yao_type})[ï¼š:]\s*([^ã€‚\n]{10,200})"
                matches = re.finditer(pattern, text)
                
                for match in matches:
                    content["yao_ci"].append({
                        "position": pos,
                        "type": yao_type,
                        "full_name": match.group(1),
                        "text": match.group(2).strip(),
                        "location": match.start()
                    })
        
        # æå–æ³¨è§£
        annotation_patterns = [
            (r"æ³¨[ï¼š:]\s*([^ã€‚\n]{15,300})", "æ³¨"),
            (r"è§£[ï¼š:]\s*([^ã€‚\n]{15,300})", "è§£"), 
            (r"é‡Š[ï¼š:]\s*([^ã€‚\n]{15,300})", "é‡Š"),
            (r"æŒ‰[ï¼š:]\s*([^ã€‚\n]{15,300})", "æŒ‰"),
            (r"æ›°[ï¼š:]\s*([^ã€‚\n]{15,300})", "æ›°")
        ]
        
        for pattern, ann_type in annotation_patterns:
            matches = re.finditer(pattern, text)
            for match in matches:
                content["annotations"].append({
                    "type": ann_type,
                    "content": match.group(1).strip(),
                    "position": match.start()
                })
        
        # æå–æ¡ˆä¾‹
        case_patterns = [
            r"ä¾‹[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]*[ï¼š:]\s*([^ã€‚]{50,800})",
            r"æ¡ˆä¾‹[ï¼š:]\s*([^ã€‚]{50,800})",
            r"å®ä¾‹[ï¼š:]\s*([^ã€‚]{50,800})",
            r"å ä¾‹[ï¼š:]\s*([^ã€‚]{50,800})"
        ]
        
        for pattern in case_patterns:
            matches = re.finditer(pattern, text, re.DOTALL)
            for match in matches:
                case_text = match.group(1).strip()
                if len(case_text) >= 50:
                    content["cases"].append({
                        "content": case_text,
                        "position": match.start(),
                        "length": len(case_text)
                    })
        
        # æå–å…³é”®è¯
        keyword_terms = [
            "é˜´é˜³", "äº”è¡Œ", "å…«å¦", "å¤ªæ", "æ— æ", "å…ˆå¤©", "åå¤©",
            "å åœ", "é¢„æµ‹", "å‘½ç†", "ç›¸æœ¯", "é£æ°´", "æ‹©æ—¥",
            "ä¸–åº”", "å…­äº²", "ç”¨ç¥", "å¿Œç¥", "å…ƒç¥", "ä»‡ç¥",
            "åŠ¨çˆ»", "å˜çˆ»", "é£ç¥", "ä¼ç¥", "æœˆå»º", "æ—¥å»º",
            "å­å­™", "å¦»è´¢", "å…„å¼Ÿ", "å®˜é¬¼", "çˆ¶æ¯",
            "é’é¾™", "æœ±é›€", "å‹¾é™ˆ", "è£è›‡", "ç™½è™", "ç„æ­¦"
        ]
        
        found_keywords = []
        for term in keyword_terms:
            if term in text:
                found_keywords.append(term)
        
        # æå–ä¹¦å
        book_names = re.findall(r'ã€Š([^ã€‹]{2,30})ã€‹', text)
        found_keywords.extend(book_names[:5])
        
        content["keywords"] = list(set(found_keywords))
        
        # æå–ä½œè€…ä¿¡æ¯ï¼ˆç®€åŒ–ç‰ˆï¼‰
        author_patterns = [
            r'(?:è‘—|æ’°|ç¼–)[è€…]?[ï¼š:]?\s*([ç‹æå¼ åˆ˜é™ˆæ¨èµµé»„å‘¨å´å¾å­™èƒ¡æœ±é«˜æ—ä½•éƒ­é©¬][\u4e00-\u9fff]{1,3})',
            r'([ç‹æå¼ åˆ˜é™ˆæ¨èµµé»„å‘¨å´å¾å­™èƒ¡æœ±é«˜æ—ä½•éƒ­é©¬][\u4e00-\u9fff]{1,3})\s*(?:è‘—|æ’°|ç¼–)'
        ]
        
        for pattern in author_patterns:
            match = re.search(pattern, text)
            if match:
                content["author"] = match.group(1)
                break
        
        # æå–æœä»£
        dynasty_patterns = [
            r'[(ï¼ˆ]?(æ±‰|å”|å®‹|å…ƒ|æ˜|æ¸…)[æœä»£)ï¼‰]?',
            r'(æ±‰|å”|å®‹|å…ƒ|æ˜|æ¸…)[Â·â€¢]',
            r'(æ±‰|å”|å®‹|å…ƒ|æ˜|æ¸…)ä»£'
        ]
        
        for pattern in dynasty_patterns:
            match = re.search(pattern, text)
            if match:
                content["dynasty"] = match.group(1)
                break
        
        return content
    
    def calculate_quality_metrics(self, text: str, metadata: Dict[str, Any]) -> Dict[str, float]:
        """è®¡ç®—è´¨é‡æŒ‡æ ‡"""
        if not text:
            return {"overall": 0.0, "completeness": 0.0, "accuracy": 0.0, "relevance": 0.0}
        
        metrics = {}
        
        # å®Œæ•´æ€§è¯„åˆ†
        text_length = len(text)
        page_ratio = metadata.get('pages_ratio', 0)
        completeness = min(1.0, (text_length / 5000) * 0.7 + page_ratio * 0.3)
        metrics["completeness"] = completeness
        
        # å‡†ç¡®æ€§è¯„åˆ†ï¼ˆåŸºäºä¸­æ–‡å­—ç¬¦æ¯”ä¾‹å’Œç¼–ç è´¨é‡ï¼‰
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        chinese_ratio = chinese_chars / text_length if text_length > 0 else 0
        encoding_quality = 1.0 - (text.count('ï¿½') / max(text_length, 1))
        accuracy = chinese_ratio * 0.6 + encoding_quality * 0.4
        metrics["accuracy"] = accuracy
        
        # ç›¸å…³æ€§è¯„åˆ†ï¼ˆåŸºäºæ˜“å­¦æœ¯è¯­å¯†åº¦ï¼‰
        yixue_terms = ["æ˜“", "å¦", "çˆ»", "é˜´", "é˜³", "äº”è¡Œ", "å…«å¦", "å ", "åœ"]
        term_count = sum(text.count(term) for term in yixue_terms)
        term_density = term_count / max(text_length / 1000, 1)  # æ¯åƒå­—æœ¯è¯­æ•°
        relevance = min(1.0, term_density / 10)
        metrics["relevance"] = relevance
        
        # ç»¼åˆè¯„åˆ†
        overall = (completeness * 0.3 + accuracy * 0.4 + relevance * 0.3)
        metrics["overall"] = overall
        
        return metrics
    
    def process_single_file(self, file_path: Path) -> Optional[ExtractionResult]:
        """å¤„ç†å•ä¸ªPDFæ–‡ä»¶ - æ ¸å¿ƒæ–¹æ³•"""
        start_time = time.time()
        error_log = []
        
        try:
            self.logger.debug(f"å¼€å§‹å¤„ç†: {file_path.name}")
            
            # æ–‡ä»¶åŸºæœ¬ä¿¡æ¯
            stat = file_path.stat()
            file_hash = self.get_file_hash(file_path)
            
            # å¤šæ–¹æ³•æå–ï¼Œä¼˜å…ˆä½¿ç”¨pdfplumber
            extraction_methods = [
                ("pdfplumber", self.extract_with_pdfplumber),
                ("pymupdf", self.extract_with_pymupdf), 
                ("pypdf2", self.extract_with_pypdf2)
            ]
            
            best_text = ""
            best_metadata = {}
            best_method = "failed"
            
            for method_name, method_func in extraction_methods:
                try:
                    text, metadata = method_func(file_path)
                    
                    if text and len(text.strip()) >= self.config.min_text_length:
                        # é€‰æ‹©æœ€ä½³ç»“æœï¼ˆæ–‡æœ¬é•¿åº¦ + é¡µé¢æ¯”ä¾‹ï¼‰
                        score = len(text) * metadata.get('pages_ratio', 0)
                        best_score = len(best_text) * best_metadata.get('pages_ratio', 0)
                        
                        if score > best_score:
                            best_text = text
                            best_metadata = metadata
                            best_method = method_name
                        
                        self.logger.debug(f"{method_name} æˆåŠŸæå– {len(text)} å­—ç¬¦")
                        break  # æ‰¾åˆ°æœ‰æ•ˆæå–å°±ä½¿ç”¨ï¼Œä¸éœ€è¦å°è¯•æ‰€æœ‰æ–¹æ³•
                        
                except Exception as e:
                    error_msg = f"{method_name} å¤±è´¥: {str(e)}"
                    error_log.append(error_msg)
                    self.logger.debug(error_msg)
                    self._method_performance[method_name]['failure'] += 1
                    continue
            
            if not best_text:
                error_log.append("æ‰€æœ‰æå–æ–¹æ³•å‡å¤±è´¥")
                return None
            
            # å†…å®¹åˆ†ç±»
            category, confidence, priority = self.classify_content(best_text, file_path.name)
            
            # æå–ç»“æ„åŒ–å†…å®¹
            structured_content = self.extract_structured_content(best_text)
            
            # è®¡ç®—è´¨é‡æŒ‡æ ‡
            quality_metrics = self.calculate_quality_metrics(best_text, best_metadata)
            
            # ç»Ÿè®¡ä¿¡æ¯
            statistics = {
                "text_length": len(best_text),
                "word_count": len(best_text.split()),
                "line_count": best_text.count('\n'),
                "hexagram_count": len(structured_content["hexagrams"]),
                "yao_ci_count": len(structured_content["yao_ci"]),
                "annotation_count": len(structured_content["annotations"]),
                "case_count": len(structured_content["cases"]),
                "keyword_count": len(structured_content["keywords"])
            }
            
            # å¤„ç†æ—¶é—´
            processing_time = time.time() - start_time
            
            # åˆ›å»ºæ–‡ä»¶å…ƒæ•°æ®
            file_metadata = FileMetadata(
                file_path=str(file_path),
                file_name=file_path.name,
                file_size=stat.st_size,
                file_hash=file_hash,
                pages=best_metadata.get('total_pages', 0),
                category=category,
                priority=priority,
                confidence=confidence,
                processing_time=processing_time,
                processed_at=datetime.now().isoformat(),
                method_used=best_method
            )
            
            # åˆ›å»ºæå–ç»“æœ
            result = ExtractionResult(
                metadata=file_metadata,
                raw_text=best_text,
                structured_content=structured_content,
                statistics=statistics,
                quality_metrics=quality_metrics,
                error_log=error_log
            )
            
            self.logger.debug(f"å®Œæˆå¤„ç†: {file_path.name}, è€—æ—¶: {processing_time:.2f}s")
            return result
            
        except Exception as e:
            error_msg = f"å¤„ç†å¤±è´¥ {file_path.name}: {str(e)}"
            error_log.append(error_msg)
            self.logger.error(error_msg)
            return None
        finally:
            # å†…å­˜ç®¡ç†
            gc.collect()


# ============================================================================
# ç”Ÿäº§çº§æ‰¹å¤„ç†ç®¡é“
# ============================================================================

class ProductionPipeline:
    """ç”Ÿäº§çº§å¤„ç†ç®¡é“"""
    
    def __init__(self, config: ProcessingConfig):
        self.config = config
        self.extractor = ProductionPDFExtractor(config)
        self.logger = self.extractor.logger
        
        # åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„
        self.output_dir = Path(config.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.raw_texts_dir = self.output_dir / "raw_texts"
        self.structured_data_dir = self.output_dir / "structured_data"
        self.categories_dir = self.output_dir / "categories"
        self.cache_dir = self.output_dir / "cache"
        self.reports_dir = self.output_dir / "reports"
        
        for dir_path in [self.raw_texts_dir, self.structured_data_dir, 
                        self.categories_dir, self.cache_dir, self.reports_dir]:
            dir_path.mkdir(exist_ok=True)
        
        # ç¼“å­˜å’Œæ–­ç‚¹ç»­ä¼ 
        self.cache_file = self.cache_dir / "processing_cache.pkl"
        self.progress_file = self.cache_dir / "progress.json"
        self.processed_cache = self._load_cache()
        
        # æ€§èƒ½ç›‘æ§
        self.start_time = None
        self.processed_count = 0
        self.failed_count = 0
        self.cached_count = 0
        
        # ä¿¡å·å¤„ç†ï¼ˆä¼˜é›…é€€å‡ºï¼‰
        self._setup_signal_handlers()
    
    def _setup_signal_handlers(self):
        """è®¾ç½®ä¿¡å·å¤„ç†å™¨"""
        def signal_handler(signum, frame):
            self.logger.info(f"æ¥æ”¶åˆ°ä¿¡å· {signum}ï¼Œæ­£åœ¨å®‰å…¨é€€å‡º...")
            self._save_cache()
            self._save_progress()
            sys.exit(0)
        
        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)
    
    def _load_cache(self) -> Dict[str, Any]:
        """åŠ è½½ç¼“å­˜"""
        if not self.config.cache_enabled or not self.cache_file.exists():
            return {}
        
        try:
            with open(self.cache_file, 'rb') as f:
                cache_data = pickle.load(f)
                self.logger.info(f"åŠ è½½ç¼“å­˜: {len(cache_data)} ä¸ªå·²å¤„ç†æ–‡ä»¶")
                return cache_data
        except Exception as e:
            self.logger.warning(f"ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
            return {}
    
    def _save_cache(self):
        """ä¿å­˜ç¼“å­˜"""
        if not self.config.cache_enabled:
            return
        
        try:
            with open(self.cache_file, 'wb') as f:
                pickle.dump(self.processed_cache, f)
            self.logger.debug(f"ç¼“å­˜å·²ä¿å­˜: {len(self.processed_cache)} ä¸ªæ–‡ä»¶")
        except Exception as e:
            self.logger.error(f"ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
    
    def _save_progress(self):
        """ä¿å­˜è¿›åº¦"""
        progress_data = {
            "processed_count": self.processed_count,
            "failed_count": self.failed_count,
            "cached_count": self.cached_count,
            "start_time": self.start_time.isoformat() if self.start_time else None,
            "last_update": datetime.now().isoformat()
        }
        
        try:
            with open(self.progress_file, 'w', encoding='utf-8') as f:
                json.dump(progress_data, f, indent=2)
        except Exception as e:
            self.logger.error(f"è¿›åº¦ä¿å­˜å¤±è´¥: {e}")
    
    def _check_memory_usage(self) -> Dict[str, float]:
        """æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        memory = psutil.virtual_memory()
        process = psutil.Process()
        
        return {
            "system_used_gb": memory.used / (1024**3),
            "system_available_gb": memory.available / (1024**3),
            "system_percent": memory.percent,
            "process_memory_gb": process.memory_info().rss / (1024**3),
            "memory_limit_gb": self.config.memory_limit_gb
        }
    
    def _should_reduce_workers(self, memory_info: Dict[str, float]) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦å‡å°‘å·¥ä½œè¿›ç¨‹"""
        return (memory_info["process_memory_gb"] > self.config.memory_limit_gb * 0.8 or
                memory_info["system_percent"] > 85)
    
    def process_file_batch(self, pdf_files: List[Path]) -> List[Optional[ExtractionResult]]:
        """å¤„ç†æ–‡ä»¶æ‰¹æ¬¡"""
        results = []
        
        # æ£€æŸ¥ç¼“å­˜ï¼Œè¿‡æ»¤å·²å¤„ç†æ–‡ä»¶
        unprocessed_files = []
        for pdf_file in pdf_files:
            file_hash = self.extractor.get_file_hash(pdf_file)
            if file_hash in self.processed_cache:
                results.append(self.processed_cache[file_hash])
                self.cached_count += 1
            else:
                unprocessed_files.append(pdf_file)
        
        if not unprocessed_files:
            return results
        
        # å†…å­˜æ£€æŸ¥å’ŒåŠ¨æ€è°ƒæ•´
        memory_info = self._check_memory_usage()
        if self._should_reduce_workers(memory_info):
            actual_workers = max(1, self.config.max_workers // 2)
            self.logger.warning(f"å†…å­˜ä½¿ç”¨è¿‡é«˜ï¼Œå‡å°‘å·¥ä½œè¿›ç¨‹è‡³ {actual_workers}")
        else:
            actual_workers = self.config.max_workers
        
        # å¹¶å‘å¤„ç†
        batch_results = []
        with ProcessPoolExecutor(max_workers=actual_workers) as executor:
            # æäº¤ä»»åŠ¡
            future_to_file = {
                executor.submit(process_single_file_worker, file_path, self.config): file_path
                for file_path in unprocessed_files
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_file):
                file_path = future_to_file[future]
                try:
                    result = future.result(timeout=300)  # 5åˆ†é’Ÿè¶…æ—¶
                    if result:
                        batch_results.append(result)
                        self.processed_count += 1
                        
                        # åŠ å…¥ç¼“å­˜
                        file_hash = result.metadata.file_hash
                        self.processed_cache[file_hash] = result
                    else:
                        self.failed_count += 1
                        
                except Exception as e:
                    self.logger.error(f"å¤„ç†ä»»åŠ¡å¤±è´¥ {file_path}: {e}")
                    self.failed_count += 1
        
        results.extend(batch_results)
        
        # å®šæœŸä¿å­˜ç¼“å­˜
        if (self.processed_count + self.failed_count) % 10 == 0:
            self._save_cache()
            self._save_progress()
        
        return results
    
    def run_full_pipeline(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´ç®¡é“"""
        self.start_time = datetime.now()
        self.logger.info("=" * 80)
        self.logger.info("ğŸš€ å¯åŠ¨ç”Ÿäº§çº§PDFæå–ç®¡é“")
        self.logger.info(f"ğŸ“‚ æºç›®å½•: {self.config.source_dir}")
        self.logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {self.config.output_dir}")
        self.logger.info(f"âš¡ æœ€å¤§å·¥ä½œè¿›ç¨‹: {self.config.max_workers}")
        self.logger.info(f"ğŸ“¦ æ‰¹å¤„ç†å¤§å°: {self.config.batch_size}")
        self.logger.info(f"ğŸ’¾ å†…å­˜é™åˆ¶: {self.config.memory_limit_gb:.1f} GB")
        self.logger.info(f"â° æ—¶é—´é™åˆ¶: {self.config.time_limit_hours:.1f} å°æ—¶")
        self.logger.info("=" * 80)
        
        # æ‰«æPDFæ–‡ä»¶
        source_path = Path(self.config.source_dir)
        pdf_files = list(source_path.glob("*.pdf"))
        total_files = len(pdf_files)
        
        if total_files == 0:
            self.logger.error("âŒ æ²¡æœ‰æ‰¾åˆ°PDFæ–‡ä»¶")
            return {"status": "error", "message": "æ²¡æœ‰æ‰¾åˆ°PDFæ–‡ä»¶"}
        
        self.logger.info(f"ğŸ“‹ å‘ç° {total_files} ä¸ªPDFæ–‡ä»¶")
        
        # æŒ‰æ–‡ä»¶å¤§å°æ’åºï¼ˆå°æ–‡ä»¶ä¼˜å…ˆå¤„ç†ï¼‰
        pdf_files.sort(key=lambda f: f.stat().st_size)
        
        # è®¡ç®—æ€»æ•°æ®é‡
        total_size_mb = sum(f.stat().st_size for f in pdf_files) / (1024 * 1024)
        self.logger.info(f"ğŸ’¿ æ€»æ•°æ®é‡: {total_size_mb:.2f} MB")
        
        # æ‰¹æ¬¡å¤„ç†
        all_results = []
        time_limit_seconds = self.config.time_limit_hours * 3600
        
        # è¿›åº¦æ¡
        with tqdm(total=total_files, desc="å¤„ç†PDFæ–‡ä»¶", unit="æ–‡ä»¶") as pbar:
            
            for i in range(0, total_files, self.config.batch_size):
                batch = pdf_files[i:i + self.config.batch_size]
                batch_num = i // self.config.batch_size + 1
                
                # æ—¶é—´æ£€æŸ¥
                elapsed_time = (datetime.now() - self.start_time).total_seconds()
                if elapsed_time > time_limit_seconds:
                    self.logger.warning("â° è¾¾åˆ°æ—¶é—´é™åˆ¶ï¼Œåœæ­¢å¤„ç†")
                    break
                
                self.logger.info(f"ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_num}: {len(batch)} ä¸ªæ–‡ä»¶")
                
                # å¤„ç†æ‰¹æ¬¡
                batch_start_time = time.time()
                batch_results = self.process_file_batch(batch)
                batch_time = time.time() - batch_start_time
                
                # ç»Ÿè®¡æ‰¹æ¬¡ç»“æœ
                successful_batch = [r for r in batch_results if r is not None]
                all_results.extend(successful_batch)
                
                # æ›´æ–°è¿›åº¦æ¡
                pbar.update(len(batch))
                pbar.set_postfix({
                    "æˆåŠŸ": len(successful_batch),
                    "å¤±è´¥": len(batch) - len(successful_batch),
                    "ç”¨æ—¶": f"{batch_time:.1f}s"
                })
                
                # æ‰¹æ¬¡æ€§èƒ½æŠ¥å‘Š
                files_per_sec = len(batch) / batch_time if batch_time > 0 else 0
                self.logger.info(f"æ‰¹æ¬¡ {batch_num} å®Œæˆ: {len(successful_batch)}/{len(batch)} æˆåŠŸ, "
                               f"è€—æ—¶ {batch_time:.2f}s, é€Ÿåº¦ {files_per_sec:.2f} æ–‡ä»¶/ç§’")
                
                # é¢„ä¼°å‰©ä½™æ—¶é—´
                if self.processed_count > 0:
                    avg_time_per_file = elapsed_time / (self.processed_count + self.failed_count)
                    remaining_files = total_files - (self.processed_count + self.failed_count + self.cached_count)
                    estimated_remaining_hours = (avg_time_per_file * remaining_files) / 3600
                    
                    self.logger.info(f"ğŸ“Š è¿›åº¦: {self.processed_count + self.failed_count + self.cached_count}/{total_files}, "
                                   f"é¢„è®¡å‰©ä½™: {estimated_remaining_hours:.2f} å°æ—¶")
                
                # å†…å­˜æ¸…ç†
                gc.collect()
        
        # æœ€ç»ˆä¿å­˜
        self._save_cache()
        self._save_progress()
        
        # ç”Ÿæˆå¤„ç†æŠ¥å‘Š
        total_time = (datetime.now() - self.start_time).total_seconds()
        report = self._generate_final_report(all_results, total_time, total_files)
        
        # ä¿å­˜ç»“æœ
        self._save_results(all_results, report)
        
        return report
    
    def _generate_final_report(self, results: List[ExtractionResult], 
                             total_time: float, total_files: int) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        successful_results = [r for r in results if r is not None]
        
        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        category_stats = defaultdict(int)
        priority_stats = defaultdict(int)
        method_stats = defaultdict(int)
        
        total_text_length = 0
        total_hexagrams = 0
        total_yao_ci = 0
        total_annotations = 0
        total_cases = 0
        
        for result in successful_results:
            category_stats[result.metadata.category] += 1
            priority_stats[f"ä¼˜å…ˆçº§{result.metadata.priority}"] += 1
            method_stats[result.metadata.method_used] += 1
            
            total_text_length += result.statistics["text_length"]
            total_hexagrams += result.statistics["hexagram_count"]
            total_yao_ci += result.statistics["yao_ci_count"]
            total_annotations += result.statistics["annotation_count"]
            total_cases += result.statistics["case_count"]
        
        # è´¨é‡ç»Ÿè®¡
        quality_scores = [r.quality_metrics["overall"] for r in successful_results]
        avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0
        
        # æ€§èƒ½ç»Ÿè®¡
        processing_times = [r.metadata.processing_time for r in successful_results]
        avg_processing_time = sum(processing_times) / len(processing_times) if processing_times else 0
        
        report = {
            "status": "completed",
            "summary": {
                "total_files": total_files,
                "processed_files": len(successful_results),
                "failed_files": self.failed_count,
                "cached_files": self.cached_count,
                "success_rate": len(successful_results) / total_files * 100 if total_files > 0 else 0,
                "total_processing_time_hours": total_time / 3600,
                "average_file_processing_time": avg_processing_time,
                "files_per_second": len(successful_results) / total_time if total_time > 0 else 0
            },
            "content_statistics": {
                "total_text_length": total_text_length,
                "total_hexagrams": total_hexagrams,
                "total_yao_ci": total_yao_ci,
                "total_annotations": total_annotations,
                "total_cases": total_cases,
                "average_quality_score": avg_quality
            },
            "category_distribution": dict(category_stats),
            "priority_distribution": dict(priority_stats),
            "method_distribution": dict(method_stats),
            "performance_metrics": {
                "memory_peak_gb": max([info.get("process_memory_gb", 0) 
                                     for info in self.extractor._memory_usage_history] or [0]),
                "method_performance": dict(self.extractor._method_performance)
            },
            "processing_timestamp": datetime.now().isoformat(),
            "config_used": asdict(self.config)
        }
        
        return report
    
    def _save_results(self, results: List[ExtractionResult], report: Dict[str, Any]):
        """ä¿å­˜å¤„ç†ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        complete_results = []
        for result in results:
            if result:
                result_dict = {
                    "metadata": asdict(result.metadata),
                    "statistics": result.statistics,
                    "quality_metrics": result.quality_metrics,
                    "structured_content": result.structured_content,
                    "error_log": result.error_log
                }
                complete_results.append(result_dict)
        
        complete_file = self.structured_data_dir / f"complete_results_{timestamp}.json"
        with open(complete_file, 'w', encoding='utf-8') as f:
            json.dump(complete_results, f, ensure_ascii=False, indent=2)
        
        # æŒ‰ç±»åˆ«ä¿å­˜
        category_results = defaultdict(list)
        for result in results:
            if result:
                category_results[result.metadata.category].append(asdict(result.metadata))
        
        for category, files in category_results.items():
            category_file = self.categories_dir / f"{category}_{timestamp}.json"
            with open(category_file, 'w', encoding='utf-8') as f:
                json.dump({
                    "category": category,
                    "file_count": len(files),
                    "files": files
                }, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜å¤„ç†æŠ¥å‘Š
        report_file = self.structured_data_dir / f"processing_report_{timestamp}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # ç”ŸæˆHTMLæŠ¥å‘Š
        self._generate_html_report(report, timestamp)
        
        # ä¿å­˜åŸå§‹æ–‡æœ¬ï¼ˆå°æ–‡ä»¶ï¼‰
        for result in results:
            if result and len(result.raw_text) < 200000:  # å°äº200KBçš„æ–‡æœ¬
                text_file = self.raw_texts_dir / f"{Path(result.metadata.file_name).stem}.txt"
                with open(text_file, 'w', encoding='utf-8') as f:
                    f.write(result.raw_text)
        
        self.logger.info(f"âœ… ç»“æœå·²ä¿å­˜")
        self.logger.info(f"ğŸ“„ å®Œæ•´ç»“æœ: {complete_file}")
        self.logger.info(f"ğŸ“Š å¤„ç†æŠ¥å‘Š: {report_file}")
        self.logger.info(f"ğŸ“ åˆ†ç±»ç»“æœ: {self.categories_dir}")
    
    def _generate_html_report(self, report: Dict[str, Any], timestamp: str):
        """ç”ŸæˆHTMLå¯è§†åŒ–æŠ¥å‘Š"""
        summary = report["summary"]
        content_stats = report["content_statistics"]
        category_dist = report["category_distribution"]
        
        html_content = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç”Ÿäº§çº§PDFæå–æŠ¥å‘Š - {timestamp}</title>
    <style>
        body {{ 
            font-family: 'Microsoft YaHei', -apple-system, BlinkMacSystemFont, sans-serif; 
            margin: 20px; 
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
        }}
        .container {{ 
            max-width: 1200px; 
            margin: 0 auto; 
            background: white; 
            padding: 30px; 
            border-radius: 20px; 
            box-shadow: 0 20px 40px rgba(0,0,0,0.1);
        }}
        .header {{ 
            text-align: center; 
            border-bottom: 4px solid #4CAF50; 
            padding-bottom: 20px; 
            margin-bottom: 30px; 
        }}
        .header h1 {{
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 2.5em;
        }}
        .stats-grid {{ 
            display: grid; 
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr)); 
            gap: 25px; 
            margin-bottom: 40px; 
        }}
        .stat-card {{ 
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); 
            color: white; 
            padding: 25px; 
            border-radius: 15px; 
            text-align: center;
            transition: transform 0.3s ease;
        }}
        .stat-card:hover {{
            transform: translateY(-5px);
        }}
        .stat-number {{ 
            font-size: 3em; 
            font-weight: bold; 
            margin-bottom: 10px; 
            text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
        }}
        .stat-label {{ 
            font-size: 1.2em; 
            opacity: 0.9; 
        }}
        .section {{
            margin: 40px 0;
            padding: 25px;
            border-radius: 15px;
            box-shadow: 0 5px 15px rgba(0,0,0,0.08);
        }}
        .category-section {{ 
            background: linear-gradient(135deg, #e8f5e8 0%, #f0f9ff 100%);
        }}
        .performance-section {{
            background: linear-gradient(135deg, #fff3e0 0%, #fce4ec 100%);
        }}
        .content-section {{
            background: linear-gradient(135deg, #f3e5f5 0%, #e8f5e8 100%);
        }}
        .category-grid {{ 
            display: grid; 
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); 
            gap: 20px; 
            margin-top: 20px;
        }}
        .category-item {{ 
            background: white; 
            padding: 20px; 
            border-radius: 12px; 
            border-left: 6px solid #4CAF50;
            box-shadow: 0 3px 10px rgba(0,0,0,0.1);
            transition: transform 0.2s ease;
        }}
        .category-item:hover {{
            transform: scale(1.02);
        }}
        .progress-bar {{
            background: #e0e0e0;
            border-radius: 10px;
            height: 8px;
            margin: 10px 0;
            overflow: hidden;
        }}
        .progress-fill {{
            background: linear-gradient(135deg, #4CAF50, #45a049);
            height: 100%;
            border-radius: 10px;
            transition: width 0.3s ease;
        }}
        h2 {{ 
            color: #2c3e50; 
            border-bottom: 3px solid #4CAF50; 
            padding-bottom: 10px;
            margin-bottom: 20px;
            font-size: 1.8em;
        }}
        .highlight {{
            background: linear-gradient(135deg, #ff6b6b, #ee5a24);
            color: white;
            padding: 3px 8px;
            border-radius: 5px;
            font-weight: bold;
        }}
        .success {{
            color: #27ae60;
            font-weight: bold;
        }}
        .footer {{
            text-align: center;
            margin-top: 40px;
            padding: 20px;
            color: #7f8c8d;
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸš€ ç”Ÿäº§çº§PDFæå–æŠ¥å‘Š</h1>
            <p style="font-size: 1.2em; color: #7f8c8d;">å¤„ç†æ—¶é—´: {timestamp}</p>
            <p style="font-size: 1.1em;">åŸºäºETL_Architecture_Design.mdæ–¹æ¡ˆå®ç°</p>
        </div>
        
        <div class="stats-grid">
            <div class="stat-card">
                <div class="stat-number">{summary['total_files']}</div>
                <div class="stat-label">ğŸ“„ æ€»æ–‡ä»¶æ•°</div>
            </div>
            <div class="stat-card" style="background: linear-gradient(135deg, #00b894 0%, #00cec9 100%);">
                <div class="stat-number">{summary['processed_files']}</div>
                <div class="stat-label">âœ… æˆåŠŸå¤„ç†</div>
            </div>
            <div class="stat-card" style="background: linear-gradient(135deg, #fdcb6e 0%, #e17055 100%);">
                <div class="stat-number">{summary.get('cached_files', 0)}</div>
                <div class="stat-label">ğŸ’¾ ç¼“å­˜å‘½ä¸­</div>
            </div>
            <div class="stat-card" style="background: linear-gradient(135deg, #6c5ce7 0%, #a29bfe 100%);">
                <div class="stat-number">{summary['success_rate']:.1f}%</div>
                <div class="stat-label">ğŸ“ˆ æˆåŠŸç‡</div>
            </div>
            <div class="stat-card" style="background: linear-gradient(135deg, #fd79a8 0%, #fdcb6e 100%);">
                <div class="stat-number">{summary['total_processing_time_hours']:.2f}h</div>
                <div class="stat-label">â±ï¸ æ€»è€—æ—¶</div>
            </div>
            <div class="stat-card" style="background: linear-gradient(135deg, #00b894 0%, #55a3ff 100%);">
                <div class="stat-number">{summary['files_per_second']:.2f}</div>
                <div class="stat-label">ğŸ”¥ æ–‡ä»¶/ç§’</div>
            </div>
        </div>
        
        <div class="section category-section">
            <h2>ğŸ“š å†…å®¹åˆ†ç±»ç»Ÿè®¡</h2>
            <div class="category-grid">
        """
        
        # æ·»åŠ åˆ†ç±»ç»Ÿè®¡
        total_processed = summary['processed_files']
        for category, count in sorted(category_dist.items(), key=lambda x: x[1], reverse=True):
            percentage = (count / max(total_processed, 1)) * 100
            html_content += f"""
                <div class="category-item">
                    <h3 style="margin: 0 0 10px 0; color: #2c3e50;">{category}</h3>
                    <div style="font-size: 1.4em; font-weight: bold; color: #27ae60;">{count} ä¸ªæ–‡ä»¶</div>
                    <div class="progress-bar">
                        <div class="progress-fill" style="width: {percentage}%"></div>
                    </div>
                    <div style="text-align: right; color: #7f8c8d; margin-top: 5px;">{percentage:.1f}%</div>
                </div>
            """
        
        html_content += """
            </div>
        </div>
        
        <div class="section content-section">
            <h2>ğŸ“– å†…å®¹æå–ç»Ÿè®¡</h2>
            <div class="stats-grid">
        """
        
        # å†…å®¹ç»Ÿè®¡å¡ç‰‡
        content_cards = [
            ("total_hexagrams", "ğŸ”® å¦è±¡æ€»æ•°", "#e74c3c"),
            ("total_yao_ci", "ğŸ“¿ çˆ»è¾æ€»æ•°", "#3498db"),
            ("total_annotations", "ğŸ“ æ³¨è§£æ€»æ•°", "#f39c12"),
            ("total_cases", "ğŸ“‹ æ¡ˆä¾‹æ€»æ•°", "#9b59b6")
        ]
        
        for key, label, color in content_cards:
            value = content_stats.get(key, 0)
            html_content += f"""
                <div class="stat-card" style="background: linear-gradient(135deg, {color}, {color}aa);">
                    <div class="stat-number">{value}</div>
                    <div class="stat-label">{label}</div>
                </div>
            """
        
        html_content += f"""
            </div>
            <div style="text-align: center; margin-top: 30px; font-size: 1.3em;">
                ğŸ“Š å¹³å‡è´¨é‡è¯„åˆ†: <span class="highlight">{content_stats.get('average_quality_score', 0):.3f}</span>
                ğŸ“ æ€»æ–‡æœ¬é•¿åº¦: <span class="highlight">{content_stats.get('total_text_length', 0):,}</span> å­—ç¬¦
            </div>
        </div>
        
        <div class="section performance-section">
            <h2>âš¡ æ€§èƒ½æŒ‡æ ‡</h2>
            <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 25px;">
                <div style="background: white; padding: 20px; border-radius: 12px;">
                    <h3>ğŸ¯ ç›®æ ‡è¾¾æˆæƒ…å†µ</h3>
                    <p>â° <strong>ç›®æ ‡æ—¶é—´</strong>: 3.0 å°æ—¶</p>
                    <p>â±ï¸ <strong>å®é™…ç”¨æ—¶</strong>: <span class="{'success' if summary['total_processing_time_hours'] <= 3.0 else 'highlight'}">{summary['total_processing_time_hours']:.2f} å°æ—¶</span></p>
                    <p>ğŸ¯ <strong>ç›®æ ‡è¾¾æˆ</strong>: <span class="{'success' if summary['total_processing_time_hours'] <= 3.0 else 'highlight'}">{'âœ… æ˜¯' if summary['total_processing_time_hours'] <= 3.0 else 'âŒ å¦'}</span></p>
                </div>
                
                <div style="background: white; padding: 20px; border-radius: 12px;">
                    <h3>ğŸ”§ å¤„ç†æ–¹æ³•ç»Ÿè®¡</h3>
        """
        
        method_dist = report.get("method_distribution", {})
        for method, count in method_dist.items():
            html_content += f"<p><strong>{method}</strong>: {count} æ¬¡ä½¿ç”¨</p>"
        
        html_content += f"""
                </div>
            </div>
        </div>
        
        <div class="footer">
            <p>ğŸ¤– ç”±Production Extract Pipelineè‡ªåŠ¨ç”Ÿæˆ</p>
            <p>åŸºäºETL_Architecture_Design.mdæ–¹æ¡ˆ | {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        </div>
    </div>
</body>
</html>
        """
        
        # ä¿å­˜HTMLæŠ¥å‘Š
        html_file = self.reports_dir / f"production_report_{timestamp}.html"
        with open(html_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        self.logger.info(f"ğŸ“„ HTMLæŠ¥å‘Šå·²ç”Ÿæˆ: {html_file}")


# ============================================================================
# å·¥ä½œè¿›ç¨‹å‡½æ•°ï¼ˆç”¨äºmultiprocessingï¼‰
# ============================================================================

def process_single_file_worker(file_path: Path, config: ProcessingConfig) -> Optional[ExtractionResult]:
    """å·¥ä½œè¿›ç¨‹å¤„ç†å•ä¸ªæ–‡ä»¶ï¼ˆç”¨äºmultiprocessingï¼‰"""
    try:
        extractor = ProductionPDFExtractor(config)
        return extractor.process_single_file(file_path)
    except Exception as e:
        logging.getLogger("Worker").error(f"å·¥ä½œè¿›ç¨‹å¤„ç†å¤±è´¥ {file_path}: {e}")
        return None


# ============================================================================
# ä¸»ç¨‹åºå…¥å£
# ============================================================================

def main():
    """ä¸»ç¨‹åº"""
    print("ğŸš€ ç”Ÿäº§çº§æ˜“å­¦PDFæ–‡æ¡£æ‰¹é‡æå–ç®¡é“")
    print("=" * 80)
    print("åŸºäºETL_Architecture_Design.mdæ–¹æ¡ˆå®ç°")
    print("æ•´åˆpdfplumber+PyMuPDF+multiprocessing+æ–­ç‚¹ç»­ä¼ ")
    print("ç›®æ ‡: 191ä¸ªPDFæ–‡ä»¶ï¼Œ3å°æ—¶å†…å®Œæˆå¤„ç†")
    print("=" * 80)
    
    # æ£€æŸ¥æ•°æ®ç›®å½•
    data_dir = "/mnt/d/desktop/appp/data"
    output_dir = "/mnt/d/desktop/appp/structured_data"
    
    if not Path(data_dir).exists():
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        return
    
    # ç»Ÿè®¡PDFæ–‡ä»¶
    pdf_files = list(Path(data_dir).glob("*.pdf"))
    print(f"ğŸ“‹ å‘ç° {len(pdf_files)} ä¸ªPDFæ–‡ä»¶")
    
    if len(pdf_files) == 0:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°PDFæ–‡ä»¶")
        return
    
    # æ£€æŸ¥ç³»ç»Ÿèµ„æº
    memory = psutil.virtual_memory()
    cpu_count = psutil.cpu_count()
    
    print(f"ğŸ’» ç³»ç»Ÿèµ„æº:")
    print(f"   CPUæ ¸å¿ƒ: {cpu_count}")
    print(f"   æ€»å†…å­˜: {memory.total / (1024**3):.1f} GB")
    print(f"   å¯ç”¨å†…å­˜: {memory.available / (1024**3):.1f} GB")
    
    # åŠ¨æ€è®¡ç®—æœ€ä¼˜é…ç½®
    optimal_workers = min(cpu_count - 1, 8)  # ä¿ç•™1ä¸ªæ ¸å¿ƒç»™ç³»ç»Ÿ
    optimal_batch_size = max(4, min(optimal_workers * 2, 12))
    memory_limit_gb = min(memory.available / (1024**3) * 0.7, 3.0)  # ä½¿ç”¨70%å¯ç”¨å†…å­˜
    
    print(f"âš™ï¸ ä¼˜åŒ–é…ç½®:")
    print(f"   å·¥ä½œè¿›ç¨‹: {optimal_workers}")
    print(f"   æ‰¹å¤„ç†å¤§å°: {optimal_batch_size}")
    print(f"   å†…å­˜é™åˆ¶: {memory_limit_gb:.1f} GB")
    
    # ç”¨æˆ·ç¡®è®¤
    print(f"\nğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"â° é¢„è®¡å¤„ç†æ—¶é—´: < 3.0 å°æ—¶")
    print("\nğŸš€ è‡ªåŠ¨å¼€å§‹å¤„ç†...")  # å»é™¤äº¤äº’å¼ç¡®è®¤
    
    # åˆ›å»ºé…ç½®
    config = ProcessingConfig(
        source_dir=data_dir,
        output_dir=output_dir,
        max_workers=optimal_workers,
        batch_size=optimal_batch_size,
        memory_limit_gb=memory_limit_gb,
        time_limit_hours=3.0,
        cache_enabled=True,
        resume_enabled=True,
        min_text_length=50,
        max_pages_per_file=500
    )
    
    # è¿è¡Œç®¡é“
    pipeline = ProductionPipeline(config)
    
    try:
        print(f"\nâš¡ å¯åŠ¨å¤„ç†ç®¡é“...")
        start_time = time.time()
        
        report = pipeline.run_full_pipeline()
        
        end_time = time.time()
        total_time_hours = (end_time - start_time) / 3600
        
        print(f"\nâœ… å¤„ç†å®Œæˆ!")
        print(f"â° å®é™…è€—æ—¶: {total_time_hours:.2f} å°æ—¶")
        print(f"ğŸ“Š æˆåŠŸç‡: {report['summary']['success_rate']:.1f}%")
        print(f"ğŸ“„ å¤„ç†æ–‡ä»¶: {report['summary']['processed_files']}/{report['summary']['total_files']}")
        print(f"ğŸ¯ ç›®æ ‡è¾¾æˆ: {'âœ… æ˜¯' if total_time_hours <= 3.0 else 'âŒ å¦'}")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {output_dir}")
        
        # æ˜¾ç¤ºæ ¸å¿ƒç»Ÿè®¡
        content_stats = report["content_statistics"]
        print(f"\nğŸ“– æå–å†…å®¹ç»Ÿè®¡:")
        print(f"   ğŸ”® å¦è±¡: {content_stats['total_hexagrams']} ä¸ª")
        print(f"   ğŸ“¿ çˆ»è¾: {content_stats['total_yao_ci']} ä¸ª")
        print(f"   ğŸ“ æ³¨è§£: {content_stats['total_annotations']} ä¸ª")
        print(f"   ğŸ“‹ æ¡ˆä¾‹: {content_stats['total_cases']} ä¸ª")
        print(f"   ğŸ“ æ–‡æœ¬: {content_stats['total_text_length']:,} å­—ç¬¦")
        
        print(f"\nğŸ‰ ç”Ÿäº§çº§PDFæå–ç®¡é“æ‰§è¡Œå®Œæ¯•!")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­å¤„ç†")
        pipeline._save_cache()
        pipeline._save_progress()
        print("ğŸ’¾ å·²ä¿å­˜å¤„ç†è¿›åº¦ï¼Œä¸‹æ¬¡è¿è¡Œå°†ä»æ–­ç‚¹ç»§ç»­")
        
    except Exception as e:
        print(f"\nâŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        pipeline._save_cache()
        pipeline._save_progress()


if __name__ == "__main__":
    main()