#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–å™¨
è‡ªåŠ¨åˆ†æå’Œä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½ï¼Œåˆ›å»ºç´¢å¼•ï¼Œä¼˜åŒ–FTSæœç´¢
ç›®æ ‡ï¼šæ‰€æœ‰æŸ¥è¯¢ < 10msï¼ŒFTSæœç´¢ < 15ms
"""

import sqlite3
import time
import logging
import json
import re
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
import statistics
from performance_optimizer import PerformanceOptimizer


@dataclass
class QueryAnalysis:
    """æŸ¥è¯¢åˆ†æç»“æœ"""
    query: str
    query_type: str
    execution_time_ms: float
    rows_examined: int
    rows_returned: int
    uses_index: bool
    index_names: List[str]
    bottlenecks: List[str]
    recommendations: List[str]
    complexity_score: float


@dataclass  
class IndexRecommendation:
    """ç´¢å¼•æ¨è"""
    table_name: str
    column_names: List[str]
    index_type: str  # 'btree', 'fts', 'covering'
    estimated_improvement: float
    priority: int  # 1-10, 10 is highest
    reason: str
    size_estimate_mb: float


class QueryOptimizer:
    """æŸ¥è¯¢ä¼˜åŒ–å™¨"""
    
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.logger = logging.getLogger(__name__)
        
        # åˆå§‹åŒ–æ€§èƒ½ä¼˜åŒ–å™¨
        self.perf_optimizer = PerformanceOptimizer(db_path, enable_monitoring=True)
        
        # æŸ¥è¯¢åˆ†æç»“æœ
        self.analyzed_queries = []
        self.slow_queries = []
        self.index_recommendations = []
        
    def analyze_database_schema(self) -> Dict[str, Any]:
        """åˆ†ææ•°æ®åº“æ¨¡å¼"""
        self.logger.info("åˆ†ææ•°æ®åº“æ¨¡å¼...")
        
        schema_info = {
            'tables': {},
            'indexes': {},
            'fts_tables': {},
            'statistics': {}
        }
        
        with self.perf_optimizer.connection_pool.get_connection() as conn:
            # è·å–è¡¨ä¿¡æ¯
            tables = conn.execute("SELECT name FROM sqlite_master WHERE type='table'").fetchall()
            
            for table_row in tables:
                table_name = table_row[0]
                
                # è·³è¿‡ç³»ç»Ÿè¡¨
                if table_name.startswith('sqlite_'):
                    continue
                
                # è·å–è¡¨ç»“æ„
                columns = conn.execute(f"PRAGMA table_info({table_name})").fetchall()
                
                # è·å–è¡¨ç»Ÿè®¡
                count_result = conn.execute(f"SELECT COUNT(*) FROM {table_name}").fetchone()
                row_count = count_result[0] if count_result else 0
                
                schema_info['tables'][table_name] = {
                    'columns': [{'name': col[1], 'type': col[2], 'notnull': col[3], 'pk': col[5]} for col in columns],
                    'row_count': row_count
                }
            
            # è·å–ç´¢å¼•ä¿¡æ¯
            indexes = conn.execute("SELECT name, tbl_name, sql FROM sqlite_master WHERE type='index'").fetchall()
            
            for index_row in indexes:
                index_name, table_name, sql = index_row
                
                if not index_name.startswith('sqlite_autoindex'):
                    schema_info['indexes'][index_name] = {
                        'table': table_name,
                        'sql': sql
                    }
            
            # è¯†åˆ«FTSè¡¨
            fts_tables = conn.execute("""
                SELECT name FROM sqlite_master 
                WHERE type='table' AND sql LIKE '%FTS%'
            """).fetchall()
            
            for fts_row in fts_tables:
                fts_name = fts_row[0]
                schema_info['fts_tables'][fts_name] = {
                    'type': 'fts5' if 'fts5' in fts_name else 'fts4'
                }
        
        return schema_info
    
    def analyze_query_performance(self, query: str, params: tuple = None, 
                                 iterations: int = 10) -> QueryAnalysis:
        """åˆ†æå•ä¸ªæŸ¥è¯¢çš„æ€§èƒ½"""
        
        if params is None:
            params = ()
        
        # æ‰§è¡Œå¤šæ¬¡è·å–å¹³å‡æ—¶é—´
        execution_times = []
        
        with self.perf_optimizer.connection_pool.get_connection() as conn:
            
            # è·å–æŸ¥è¯¢è®¡åˆ’
            explain_query = f"EXPLAIN QUERY PLAN {query}"
            query_plan = conn.execute(explain_query, params).fetchall()
            
            # åˆ†ææŸ¥è¯¢è®¡åˆ’
            uses_index = False
            index_names = []
            bottlenecks = []
            
            for step in query_plan:
                step_detail = str(step).upper()
                
                if 'USING INDEX' in step_detail:
                    uses_index = True
                    # æå–ç´¢å¼•å
                    index_match = re.search(r'USING INDEX (\w+)', step_detail)
                    if index_match:
                        index_names.append(index_match.group(1))
                
                if 'SCAN TABLE' in step_detail:
                    bottlenecks.append("å…¨è¡¨æ‰«æï¼Œå»ºè®®åˆ›å»ºç´¢å¼•")
                
                if 'TEMP B-TREE' in step_detail:
                    bottlenecks.append("ä½¿ç”¨ä¸´æ—¶è¡¨ï¼Œå¯èƒ½ç”±äºå¤æ‚çš„GROUP BYæˆ–ORDER BYé€ æˆ")
                
                if 'SORT' in step_detail:
                    bottlenecks.append("æ’åºæ“ä½œå¯èƒ½æ¶ˆè€—å¤§é‡èµ„æº")
            
            # æ‰§è¡Œæ€§èƒ½æµ‹è¯•
            for _ in range(iterations):
                start_time = time.time()
                cursor = conn.execute(query, params)
                results = cursor.fetchall()
                execution_time = (time.time() - start_time) * 1000
                execution_times.append(execution_time)
            
            rows_returned = len(results)
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        avg_time = statistics.mean(execution_times)
        
        # ä¼°ç®—æ£€æŸ¥çš„è¡Œæ•°ï¼ˆåŸºäºæŸ¥è¯¢è®¡åˆ’ï¼‰
        rows_examined = self._estimate_rows_examined(query_plan)
        
        # è®¡ç®—å¤æ‚åº¦è¯„åˆ†
        complexity_score = self._calculate_complexity_score(query, query_plan)
        
        # ç”Ÿæˆä¼˜åŒ–å»ºè®®
        recommendations = self._generate_query_recommendations(query, query_plan, avg_time)
        
        # ç¡®å®šæŸ¥è¯¢ç±»å‹
        query_type = self._classify_query(query)
        
        analysis = QueryAnalysis(
            query=query,
            query_type=query_type,
            execution_time_ms=avg_time,
            rows_examined=rows_examined,
            rows_returned=rows_returned,
            uses_index=uses_index,
            index_names=index_names,
            bottlenecks=bottlenecks,
            recommendations=recommendations,
            complexity_score=complexity_score
        )
        
        self.analyzed_queries.append(analysis)
        
        if avg_time > 10:  # æ…¢æŸ¥è¯¢é˜ˆå€¼
            self.slow_queries.append(analysis)
        
        return analysis
    
    def _estimate_rows_examined(self, query_plan: List) -> int:
        """ä¼°ç®—æ£€æŸ¥çš„è¡Œæ•°"""
        rows_examined = 0
        
        for step in query_plan:
            step_detail = str(step).upper()
            
            if 'SCAN TABLE' in step_detail:
                # å…¨è¡¨æ‰«æï¼Œä¼°ç®—ä¸ºè¡¨çš„è¡Œæ•°
                rows_examined += 10000  # é»˜è®¤ä¼°ç®—å€¼
            elif 'SEARCH TABLE' in step_detail and 'USING INDEX' in step_detail:
                # ç´¢å¼•æœç´¢ï¼Œä¼°ç®—æ£€æŸ¥è¾ƒå°‘è¡Œæ•°
                rows_examined += 100
            elif 'SCAN SUBQUERY' in step_detail:
                rows_examined += 1000
        
        return max(rows_examined, 1)
    
    def _calculate_complexity_score(self, query: str, query_plan: List) -> float:
        """è®¡ç®—æŸ¥è¯¢å¤æ‚åº¦è¯„åˆ† (0-10)"""
        score = 0
        
        query_upper = query.upper()
        
        # åŸºç¡€å¤æ‚åº¦
        if 'SELECT' in query_upper:
            score += 1
        
        # JOIN å¤æ‚åº¦
        join_count = query_upper.count('JOIN')
        score += join_count * 1.5
        
        # å­æŸ¥è¯¢å¤æ‚åº¦
        subquery_count = query_upper.count('SELECT') - 1  # å‡å»ä¸»æŸ¥è¯¢
        score += subquery_count * 2
        
        # WHERE æ¡ä»¶å¤æ‚åº¦
        if 'WHERE' in query_upper:
            score += 0.5
            # å¤æ‚æ¡ä»¶
            if 'LIKE' in query_upper:
                score += 1
            if 'IN (' in query_upper:
                score += 0.5
        
        # GROUP BY / ORDER BY
        if 'GROUP BY' in query_upper:
            score += 1
        if 'ORDER BY' in query_upper:
            score += 0.5
        
        # æŸ¥è¯¢è®¡åˆ’å¤æ‚åº¦
        for step in query_plan:
            step_detail = str(step).upper()
            if 'TEMP B-TREE' in step_detail:
                score += 1.5
            if 'SCAN TABLE' in step_detail:
                score += 2
        
        return min(10, score)
    
    def _classify_query(self, query: str) -> str:
        """åˆ†ç±»æŸ¥è¯¢ç±»å‹"""
        query_upper = query.upper().strip()
        
        if query_upper.startswith('SELECT'):
            if 'JOIN' in query_upper:
                return 'join_query'
            elif 'GROUP BY' in query_upper or 'COUNT(' in query_upper:
                return 'aggregation'
            elif 'LIKE' in query_upper or 'MATCH' in query_upper:
                return 'search_query'
            elif 'ORDER BY' in query_upper:
                return 'sorted_query'
            else:
                return 'simple_select'
        elif query_upper.startswith('INSERT'):
            return 'insert'
        elif query_upper.startswith('UPDATE'):
            return 'update'
        elif query_upper.startswith('DELETE'):
            return 'delete'
        else:
            return 'other'
    
    def _generate_query_recommendations(self, query: str, query_plan: List, 
                                      execution_time: float) -> List[str]:
        """ç”ŸæˆæŸ¥è¯¢ä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        query_upper = query.upper()
        
        # åŸºäºæ‰§è¡Œæ—¶é—´çš„å»ºè®®
        if execution_time > 50:
            recommendations.append("æŸ¥è¯¢æ‰§è¡Œæ—¶é—´è¿‡é•¿ï¼Œéœ€è¦é‡ç‚¹ä¼˜åŒ–")
        elif execution_time > 10:
            recommendations.append("æŸ¥è¯¢æ‰§è¡Œæ—¶é—´è¾ƒé•¿ï¼Œå»ºè®®ä¼˜åŒ–")
        
        # åŸºäºæŸ¥è¯¢è®¡åˆ’çš„å»ºè®®
        has_table_scan = False
        has_temp_btree = False
        
        for step in query_plan:
            step_detail = str(step).upper()
            
            if 'SCAN TABLE' in step_detail:
                has_table_scan = True
                table_match = re.search(r'SCAN TABLE (\w+)', step_detail)
                if table_match:
                    table_name = table_match.group(1)
                    recommendations.append(f"åœ¨è¡¨ {table_name} ä¸Šåˆ›å»ºç´¢å¼•ä»¥é¿å…å…¨è¡¨æ‰«æ")
            
            if 'TEMP B-TREE' in step_detail:
                has_temp_btree = True
        
        # ç‰¹å®šæ¨¡å¼çš„å»ºè®®
        if 'LIKE' in query_upper and '%' in query:
            if not query.startswith('%'):
                recommendations.append("å¯¹äºå‰ç¼€æœç´¢ï¼Œè€ƒè™‘åˆ›å»ºå‰ç¼€ç´¢å¼•")
            else:
                recommendations.append("å¯¹äºå…¨æ–‡æœç´¢ï¼Œè€ƒè™‘ä½¿ç”¨FTSç´¢å¼•")
        
        if 'ORDER BY' in query_upper and has_temp_btree:
            recommendations.append("ä¸ºORDER BYå­—æ®µåˆ›å»ºç´¢å¼•ä»¥é¿å…æ’åºå¼€é”€")
        
        if 'GROUP BY' in query_upper and has_temp_btree:
            recommendations.append("ä¸ºGROUP BYå­—æ®µåˆ›å»ºç´¢å¼•ä»¥æé«˜åˆ†ç»„æ€§èƒ½")
        
        if 'JOIN' in query_upper:
            recommendations.append("ç¡®ä¿JOINæ¡ä»¶å­—æ®µéƒ½æœ‰é€‚å½“çš„ç´¢å¼•")
        
        return recommendations
    
    def generate_index_recommendations(self, schema_info: Dict[str, Any]) -> List[IndexRecommendation]:
        """ç”Ÿæˆç´¢å¼•æ¨è"""
        self.logger.info("ç”Ÿæˆç´¢å¼•æ¨è...")
        
        recommendations = []
        
        # åŸºäºæ…¢æŸ¥è¯¢åˆ†æç”Ÿæˆç´¢å¼•å»ºè®®
        for query_analysis in self.slow_queries:
            query = query_analysis.query.upper()
            
            # æå–è¡¨åå’Œæ¡ä»¶å­—æ®µ
            table_columns = self._extract_query_columns(query)
            
            for table_name, columns in table_columns.items():
                if table_name in schema_info['tables']:
                    
                    # WHEREæ¡ä»¶ç´¢å¼•
                    where_columns = columns.get('where', [])
                    if where_columns:
                        rec = IndexRecommendation(
                            table_name=table_name,
                            column_names=where_columns,
                            index_type='btree',
                            estimated_improvement=0.6,  # 60%æ”¹è¿›ä¼°ç®—
                            priority=8,
                            reason=f"ä¼˜åŒ–WHEREæ¡ä»¶æŸ¥è¯¢: {', '.join(where_columns)}",
                            size_estimate_mb=self._estimate_index_size(schema_info['tables'][table_name], where_columns)
                        )
                        recommendations.append(rec)
                    
                    # ORDER BYç´¢å¼•
                    order_columns = columns.get('order_by', [])
                    if order_columns:
                        rec = IndexRecommendation(
                            table_name=table_name,
                            column_names=order_columns,
                            index_type='btree',
                            estimated_improvement=0.4,
                            priority=6,
                            reason=f"ä¼˜åŒ–ORDER BYæ’åº: {', '.join(order_columns)}",
                            size_estimate_mb=self._estimate_index_size(schema_info['tables'][table_name], order_columns)
                        )
                        recommendations.append(rec)
                    
                    # å¤åˆç´¢å¼•å»ºè®®
                    if len(where_columns) > 1:
                        rec = IndexRecommendation(
                            table_name=table_name,
                            column_names=where_columns,
                            index_type='btree',
                            estimated_improvement=0.7,
                            priority=9,
                            reason=f"å¤åˆç´¢å¼•ä¼˜åŒ–å¤šæ¡ä»¶æŸ¥è¯¢",
                            size_estimate_mb=self._estimate_index_size(schema_info['tables'][table_name], where_columns)
                        )
                        recommendations.append(rec)
        
        # åŸºäºè¡¨å¤§å°å’Œè®¿é—®æ¨¡å¼çš„ç´¢å¼•å»ºè®®
        for table_name, table_info in schema_info['tables'].items():
            if table_info['row_count'] > 10000:  # å¤§è¡¨
                
                # ä¸ºä¸»è¦å­—æ®µæ¨èç´¢å¼•
                important_columns = []
                for col in table_info['columns']:
                    col_name = col['name'].lower()
                    if any(keyword in col_name for keyword in ['id', 'name', 'category', 'type', 'status']):
                        important_columns.append(col['name'])
                
                if important_columns:
                    for col_name in important_columns:
                        rec = IndexRecommendation(
                            table_name=table_name,
                            column_names=[col_name],
                            index_type='btree',
                            estimated_improvement=0.5,
                            priority=7,
                            reason=f"å¤§è¡¨ {table_name} çš„é‡è¦å­—æ®µç´¢å¼•",
                            size_estimate_mb=self._estimate_index_size(table_info, [col_name])
                        )
                        recommendations.append(rec)
        
        # å»é‡å’Œæ’åº
        self.index_recommendations = self._deduplicate_recommendations(recommendations)
        
        return self.index_recommendations
    
    def _extract_query_columns(self, query: str) -> Dict[str, Dict[str, List[str]]]:
        """ä»æŸ¥è¯¢ä¸­æå–è¡¨å’Œå­—æ®µä¿¡æ¯"""
        result = {}
        
        # ç®€åŒ–çš„SQLè§£æï¼ˆå®é™…åº”ç”¨ä¸­å»ºè®®ä½¿ç”¨ä¸“ä¸šçš„SQLè§£æå™¨ï¼‰
        
        # æå–è¡¨å
        from_match = re.search(r'FROM\s+(\w+)', query)
        if not from_match:
            return result
        
        table_name = from_match.group(1).lower()
        result[table_name] = {'where': [], 'order_by': [], 'group_by': []}
        
        # æå–WHEREæ¡ä»¶å­—æ®µ
        where_pattern = r'WHERE.*?(?=ORDER BY|GROUP BY|$)'
        where_match = re.search(where_pattern, query, re.DOTALL)
        if where_match:
            where_clause = where_match.group(0)
            
            # æå–å­—æ®µåï¼ˆç®€åŒ–ç‰ˆï¼‰
            column_pattern = r'(\w+)\s*[=<>!]'
            columns = re.findall(column_pattern, where_clause)
            result[table_name]['where'] = [col.lower() for col in columns if col.upper() not in ['AND', 'OR', 'NOT']]
        
        # æå–ORDER BYå­—æ®µ
        order_match = re.search(r'ORDER BY\s+([\w\s,]+)', query)
        if order_match:
            order_fields = [field.strip().lower() for field in order_match.group(1).split(',')]
            result[table_name]['order_by'] = [field.split()[0] for field in order_fields]  # å»é™¤ASC/DESC
        
        # æå–GROUP BYå­—æ®µ
        group_match = re.search(r'GROUP BY\s+([\w\s,]+)', query)
        if group_match:
            group_fields = [field.strip().lower() for field in group_match.group(1).split(',')]
            result[table_name]['group_by'] = group_fields
        
        return result
    
    def _estimate_index_size(self, table_info: Dict[str, Any], columns: List[str]) -> float:
        """ä¼°ç®—ç´¢å¼•å¤§å°"""
        row_count = table_info['row_count']
        
        # ç®€åŒ–çš„ç´¢å¼•å¤§å°ä¼°ç®—
        # å‡è®¾æ¯ä¸ªç´¢å¼•é¡¹å¹³å‡20å­—èŠ‚ï¼ŒåŠ ä¸ŠBæ ‘å¼€é”€
        estimated_size_bytes = row_count * len(columns) * 25
        
        return estimated_size_bytes / 1024 / 1024  # è½¬æ¢ä¸ºMB
    
    def _deduplicate_recommendations(self, recommendations: List[IndexRecommendation]) -> List[IndexRecommendation]:
        """å»é‡ç´¢å¼•æ¨è"""
        unique_recommendations = {}
        
        for rec in recommendations:
            # ä½¿ç”¨è¡¨åå’Œå­—æ®µç»„åˆä½œä¸ºé”®
            key = f"{rec.table_name}:{':'.join(sorted(rec.column_names))}"
            
            if key not in unique_recommendations or rec.priority > unique_recommendations[key].priority:
                unique_recommendations[key] = rec
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        return sorted(unique_recommendations.values(), key=lambda x: x.priority, reverse=True)
    
    def create_recommended_indexes(self, recommendations: List[IndexRecommendation] = None, 
                                 apply: bool = False) -> Dict[str, Any]:
        """åˆ›å»ºæ¨èçš„ç´¢å¼•"""
        
        if recommendations is None:
            recommendations = self.index_recommendations
        
        results = {
            'created_indexes': [],
            'failed_indexes': [],
            'total_time_seconds': 0
        }
        
        start_time = time.time()
        
        with self.perf_optimizer.connection_pool.get_connection() as conn:
            
            for rec in recommendations:
                
                # ç”Ÿæˆç´¢å¼•å
                index_name = f"idx_{rec.table_name}_{'_'.join(rec.column_names)}"
                
                # ç”ŸæˆCREATE INDEXè¯­å¥
                if rec.index_type == 'btree':
                    columns_sql = ', '.join(rec.column_names)
                    create_sql = f"CREATE INDEX IF NOT EXISTS {index_name} ON {rec.table_name} ({columns_sql})"
                elif rec.index_type == 'fts':
                    # FTSç´¢å¼•éœ€è¦ç‰¹æ®Šå¤„ç†
                    continue  # æš‚æ—¶è·³è¿‡FTSç´¢å¼•åˆ›å»º
                else:
                    continue
                
                try:
                    if apply:
                        self.logger.info(f"åˆ›å»ºç´¢å¼•: {index_name}")
                        conn.execute(create_sql)
                        conn.commit()
                        results['created_indexes'].append({
                            'name': index_name,
                            'table': rec.table_name,
                            'columns': rec.column_names,
                            'sql': create_sql
                        })
                    else:
                        self.logger.info(f"å»ºè®®åˆ›å»ºç´¢å¼•: {create_sql}")
                        results['created_indexes'].append({
                            'name': index_name,
                            'table': rec.table_name,
                            'columns': rec.column_names,
                            'sql': create_sql,
                            'dry_run': True
                        })
                
                except Exception as e:
                    self.logger.error(f"åˆ›å»ºç´¢å¼•å¤±è´¥ {index_name}: {e}")
                    results['failed_indexes'].append({
                        'name': index_name,
                        'error': str(e),
                        'sql': create_sql
                    })
        
        results['total_time_seconds'] = time.time() - start_time
        
        return results
    
    def optimize_fts_search(self) -> Dict[str, Any]:
        """ä¼˜åŒ–å…¨æ–‡æœç´¢"""
        self.logger.info("ä¼˜åŒ–å…¨æ–‡æœç´¢...")
        
        results = {
            'fts_tables_optimized': [],
            'new_fts_indexes': [],
            'optimization_time_seconds': 0
        }
        
        start_time = time.time()
        
        with self.perf_optimizer.connection_pool.get_connection() as conn:
            
            # æŸ¥æ‰¾ç°æœ‰çš„FTSè¡¨
            fts_tables = conn.execute("""
                SELECT name FROM sqlite_master 
                WHERE type='table' AND (name LIKE '%fts%' OR sql LIKE '%FTS%')
            """).fetchall()
            
            for table_row in fts_tables:
                table_name = table_row[0]
                
                try:
                    # ä¼˜åŒ–FTSè¡¨
                    self.logger.info(f"ä¼˜åŒ–FTSè¡¨: {table_name}")
                    
                    # é‡å»ºFTSç´¢å¼•
                    conn.execute(f"INSERT INTO {table_name}({table_name}) VALUES('rebuild')")
                    
                    # ä¼˜åŒ–FTSè¡¨
                    conn.execute(f"INSERT INTO {table_name}({table_name}) VALUES('optimize')")
                    
                    results['fts_tables_optimized'].append(table_name)
                    
                except Exception as e:
                    self.logger.error(f"ä¼˜åŒ–FTSè¡¨å¤±è´¥ {table_name}: {e}")
            
            # åˆ†æå¯èƒ½éœ€è¦FTSçš„æ–‡æœ¬å­—æ®µ
            tables = conn.execute("SELECT name FROM sqlite_master WHERE type='table'").fetchall()
            
            for table_row in tables:
                table_name = table_row[0]
                
                if table_name.startswith('sqlite_') or 'fts' in table_name.lower():
                    continue
                
                # è·å–è¡¨ç»“æ„
                columns = conn.execute(f"PRAGMA table_info({table_name})").fetchall()
                
                text_columns = []
                for col in columns:
                    col_name, col_type = col[1], col[2]
                    if col_type.upper() in ['TEXT', 'VARCHAR', 'CHAR'] and 'content' in col_name.lower():
                        text_columns.append(col_name)
                
                # å¦‚æœæœ‰æ–‡æœ¬å†…å®¹å­—æ®µï¼Œå»ºè®®åˆ›å»ºFTSç´¢å¼•
                if text_columns:
                    fts_table_name = f"fts_{table_name}"
                    
                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                    existing = conn.execute("""
                        SELECT name FROM sqlite_master 
                        WHERE type='table' AND name=?
                    """, (fts_table_name,)).fetchone()
                    
                    if not existing:
                        results['new_fts_indexes'].append({
                            'original_table': table_name,
                            'fts_table': fts_table_name,
                            'columns': text_columns,
                            'recommended': True
                        })
        
        results['optimization_time_seconds'] = time.time() - start_time
        
        return results
    
    def generate_optimization_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'summary': {
                'total_queries_analyzed': len(self.analyzed_queries),
                'slow_queries_found': len(self.slow_queries),
                'index_recommendations': len(self.index_recommendations)
            },
            'query_analysis': [asdict(qa) for qa in self.analyzed_queries],
            'slow_queries': [asdict(sq) for sq in self.slow_queries],
            'index_recommendations': [asdict(ir) for ir in self.index_recommendations],
            'performance_insights': self._generate_performance_insights()
        }
        
        return report
    
    def _generate_performance_insights(self) -> List[str]:
        """ç”Ÿæˆæ€§èƒ½æ´å¯Ÿ"""
        insights = []
        
        if not self.analyzed_queries:
            return ["æ²¡æœ‰åˆ†æçš„æŸ¥è¯¢æ•°æ®"]
        
        # å¹³å‡æŸ¥è¯¢æ—¶é—´
        avg_time = statistics.mean([qa.execution_time_ms for qa in self.analyzed_queries])
        if avg_time > 10:
            insights.append(f"å¹³å‡æŸ¥è¯¢æ—¶é—´ {avg_time:.1f}msï¼Œè¶…è¿‡10msç›®æ ‡ï¼Œéœ€è¦ä¼˜åŒ–")
        else:
            insights.append(f"å¹³å‡æŸ¥è¯¢æ—¶é—´ {avg_time:.1f}msï¼Œæ€§èƒ½è‰¯å¥½")
        
        # æ…¢æŸ¥è¯¢æ¯”ä¾‹
        slow_query_rate = len(self.slow_queries) / len(self.analyzed_queries)
        if slow_query_rate > 0.2:
            insights.append(f"æ…¢æŸ¥è¯¢æ¯”ä¾‹ {slow_query_rate:.1%}ï¼Œå»ºè®®é‡ç‚¹ä¼˜åŒ–")
        
        # ç´¢å¼•ä½¿ç”¨æƒ…å†µ
        queries_with_index = [qa for qa in self.analyzed_queries if qa.uses_index]
        index_usage_rate = len(queries_with_index) / len(self.analyzed_queries)
        
        if index_usage_rate < 0.5:
            insights.append(f"ç´¢å¼•ä½¿ç”¨ç‡ {index_usage_rate:.1%}ï¼Œå»ºè®®åˆ›å»ºæ›´å¤šç´¢å¼•")
        
        # å¸¸è§ç“¶é¢ˆ
        all_bottlenecks = []
        for qa in self.analyzed_queries:
            all_bottlenecks.extend(qa.bottlenecks)
        
        if all_bottlenecks:
            from collections import Counter
            common_bottlenecks = Counter(all_bottlenecks).most_common(3)
            for bottleneck, count in common_bottlenecks:
                insights.append(f"å¸¸è§ç“¶é¢ˆ: {bottleneck} (å‡ºç° {count} æ¬¡)")
        
        return insights
    
    def close(self):
        """å…³é—­ä¼˜åŒ–å™¨"""
        if self.perf_optimizer:
            self.perf_optimizer.close()


# ä½¿ç”¨ç¤ºä¾‹
def run_query_optimization(db_path: str, test_queries: List[str] = None) -> Dict[str, Any]:
    """è¿è¡ŒæŸ¥è¯¢ä¼˜åŒ–æµç¨‹"""
    
    print("ğŸ” å¼€å§‹æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–åˆ†æ")
    
    optimizer = QueryOptimizer(db_path)
    
    try:
        # 1. åˆ†ææ•°æ®åº“æ¨¡å¼
        print("1. åˆ†ææ•°æ®åº“æ¨¡å¼...")
        schema_info = optimizer.analyze_database_schema()
        
        print(f"   å‘ç° {len(schema_info['tables'])} ä¸ªè¡¨")
        print(f"   å‘ç° {len(schema_info['indexes'])} ä¸ªç´¢å¼•")
        print(f"   å‘ç° {len(schema_info['fts_tables'])} ä¸ªFTSè¡¨")
        
        # 2. åˆ†ææµ‹è¯•æŸ¥è¯¢
        if test_queries is None:
            # é»˜è®¤æµ‹è¯•æŸ¥è¯¢
            test_queries = [
                "SELECT * FROM yixue_data WHERE category = 'hexagram'",
                "SELECT * FROM yixue_data WHERE content LIKE '%æ˜“ç»%'",
                "SELECT COUNT(*) FROM yixue_data GROUP BY category",
                "SELECT * FROM yixue_data ORDER BY created_at DESC LIMIT 10",
                "SELECT * FROM yixue_data WHERE category = 'hexagram' AND confidence > 0.8"
            ]
        
        print(f"\n2. åˆ†æ {len(test_queries)} ä¸ªæµ‹è¯•æŸ¥è¯¢...")
        for i, query in enumerate(test_queries, 1):
            try:
                analysis = optimizer.analyze_query_performance(query)
                print(f"   æŸ¥è¯¢ {i}: {analysis.execution_time_ms:.2f}ms ({'âœ“' if analysis.execution_time_ms < 10 else 'âš ï¸'})")
                
                if analysis.bottlenecks:
                    for bottleneck in analysis.bottlenecks[:2]:  # æ˜¾ç¤ºå‰2ä¸ªç“¶é¢ˆ
                        print(f"     ç“¶é¢ˆ: {bottleneck}")
                        
            except Exception as e:
                print(f"   æŸ¥è¯¢ {i}: åˆ†æå¤±è´¥ - {e}")
        
        # 3. ç”Ÿæˆç´¢å¼•æ¨è
        print(f"\n3. ç”Ÿæˆç´¢å¼•ä¼˜åŒ–å»ºè®®...")
        recommendations = optimizer.generate_index_recommendations(schema_info)
        
        print(f"   ç”Ÿæˆ {len(recommendations)} ä¸ªç´¢å¼•å»ºè®®")
        for i, rec in enumerate(recommendations[:5], 1):  # æ˜¾ç¤ºå‰5ä¸ª
            print(f"   å»ºè®® {i}: {rec.table_name}.{','.join(rec.column_names)} "
                  f"(ä¼˜å…ˆçº§: {rec.priority}, é¢„æœŸæ”¹è¿›: {rec.estimated_improvement:.1%})")
        
        # 4. ä¼˜åŒ–FTSæœç´¢
        print(f"\n4. ä¼˜åŒ–å…¨æ–‡æœç´¢...")
        fts_results = optimizer.optimize_fts_search()
        
        if fts_results['fts_tables_optimized']:
            print(f"   ä¼˜åŒ–äº† {len(fts_results['fts_tables_optimized'])} ä¸ªFTSè¡¨")
        
        if fts_results['new_fts_indexes']:
            print(f"   å»ºè®®åˆ›å»º {len(fts_results['new_fts_indexes'])} ä¸ªæ–°FTSç´¢å¼•")
        
        # 5. åˆ›å»ºæ¨èç´¢å¼•ï¼ˆè¯•è¿è¡Œï¼‰
        print(f"\n5. ç”Ÿæˆç´¢å¼•åˆ›å»ºè„šæœ¬...")
        index_results = optimizer.create_recommended_indexes(apply=False)
        
        print(f"   å‡†å¤‡åˆ›å»º {len(index_results['created_indexes'])} ä¸ªç´¢å¼•")
        
        # 6. ç”ŸæˆæŠ¥å‘Š
        print(f"\n6. ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š...")
        report = optimizer.generate_optimization_report()
        
        # ä¿å­˜æŠ¥å‘Š
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"query_optimization_report_{timestamp}.json"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"   æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        # æ˜¾ç¤ºæ€»ç»“
        print(f"\nğŸ“Š ä¼˜åŒ–æ€»ç»“:")
        print(f"   åˆ†ææŸ¥è¯¢æ•°: {report['summary']['total_queries_analyzed']}")
        print(f"   æ…¢æŸ¥è¯¢æ•°: {report['summary']['slow_queries_found']}")
        print(f"   ç´¢å¼•å»ºè®®æ•°: {report['summary']['index_recommendations']}")
        
        for insight in report['performance_insights'][:3]:
            print(f"   ğŸ’¡ {insight}")
        
        return report
        
    finally:
        optimizer.close()


if __name__ == "__main__":
    # æµ‹è¯•æŸ¥è¯¢ä¼˜åŒ–
    import sys
    
    if len(sys.argv) > 1:
        db_path = sys.argv[1]
    else:
        db_path = "database/yijing_knowledge.db"
    
    if Path(db_path).exists():
        run_query_optimization(db_path)
    else:
        print(f"æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {db_path}")
        print("ä½¿ç”¨æµ‹è¯•æ•°æ®åº“è¿›è¡Œæ¼”ç¤º...")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®åº“
        test_db = "test_optimization.db"
        conn = sqlite3.connect(test_db)
        
        # åˆ›å»ºæµ‹è¯•è¡¨å’Œæ•°æ®
        conn.execute("""
            CREATE TABLE yixue_data (
                id INTEGER PRIMARY KEY,
                category TEXT,
                content TEXT,
                confidence REAL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # æ’å…¥æµ‹è¯•æ•°æ®
        for i in range(1000):
            conn.execute("""
                INSERT INTO yixue_data (category, content, confidence)
                VALUES (?, ?, ?)
            """, (
                f"category_{i % 10}",
                f"æµ‹è¯•å†…å®¹ {i} åŒ…å«æ˜“ç»çŸ¥è¯†",
                0.5 + (i % 50) / 100
            ))
        
        conn.commit()
        conn.close()
        
        print(f"åˆ›å»ºæµ‹è¯•æ•°æ®åº“: {test_db}")
        run_query_optimization(test_db)