#!/usr/bin/env python3
"""
ä¿®å¤ç‰ˆPDFå¤„ç†è„šæœ¬ - è§£å†³å¤šè¿›ç¨‹pickleé—®é¢˜
"""

import os
import json
import re
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from datetime import datetime
import hashlib
import pickle
from collections import defaultdict
import time

try:
    import pdfplumber
    from tqdm import tqdm
except ImportError:
    print("è¯·å®‰è£…ä¾èµ–: pip install pdfplumber tqdm")
    exit(1)

# é…ç½®ç®€å•çš„æ—¥å¿—è®°å½•ï¼Œé¿å…pickleé—®é¢˜
def setup_logging():
    """è®¾ç½®æ—¥å¿—è®°å½•"""
    logging.basicConfig(
        level=logging.WARNING,  # å‡å°‘æ—¥å¿—è¾“å‡º
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    return logging.getLogger(__name__)

logger = setup_logging()

@dataclass
class PDFInfo:
    """PDFæ–‡ä»¶ä¿¡æ¯"""
    file_path: str
    file_name: str
    file_size: int
    pages: int
    category: str
    confidence: float
    priority: int
    processed_at: str
    text_length: int
    processing_time: float

@dataclass 
class ExtractedContent:
    """æå–çš„å†…å®¹"""
    hexagrams: List[Dict[str, Any]]
    yao_ci: List[Dict[str, Any]]
    annotations: List[Dict[str, Any]]
    cases: List[Dict[str, Any]]
    keywords: List[str]
    author: Optional[str]
    dynasty: Optional[str]

def classify_pdf(text: str, file_name: str) -> Tuple[str, float, int]:
    """åˆ†ç±»PDFæ–‡ä»¶ - ç‹¬ç«‹å‡½æ•°é¿å…pickleé—®é¢˜"""
    category_patterns = {
        "å…­çˆ»": {
            "keywords": ["å…­çˆ»", "åœæ˜“", "å¢åˆ ", "ç«ç æ—", "é»„é‡‘ç­–", "ç­®", "å¦è±¡", "çˆ»è¾", "çˆ»å˜"],
            "priority": 1,
            "patterns": [r"å…­çˆ»", r"ç­®\w*", r"å¦è±¡", r"çˆ»\w+", r"åŠ¨çˆ»", r"å˜çˆ»", r"ä¸–åº”", r"å…­äº²", r"ç”¨ç¥"]
        },
        "æ¢…èŠ±æ˜“æ•°": {
            "keywords": ["æ¢…èŠ±", "æ˜“æ•°", "æ¢…èŠ±æ˜“", "è§‚æ¢…", "æ•°ç†"],
            "priority": 2,
            "patterns": [r"æ¢…èŠ±\w*æ˜“\w*", r"è§‚æ¢…", r"æ˜“æ•°", r"æ•°ç†"]
        },
        "å¤§å…­å£¬": {
            "keywords": ["å…­å£¬", "å£¬å­¦", "å£¬å ", "è¯¾ä¼ ", "ç¥å°†", "åäºŒå°†"],
            "priority": 1,
            "patterns": [r"å…­å£¬", r"å£¬å ", r"è¯¾ä¼ ", r"ç¥å°†", r"åäºŒå°†"]
        },
        "ç´«å¾®æ–—æ•°": {
            "keywords": ["ç´«å¾®", "æ–—æ•°", "å‘½ç›˜", "å®«ä½", "æ˜Ÿæ›œ"],
            "priority": 2,
            "patterns": [r"ç´«å¾®\w*æ–—æ•°", r"å‘½ç›˜", r"å®«ä½", r"æ˜Ÿæ›œ"]
        },
        "å¥‡é—¨éç”²": {
            "keywords": ["å¥‡é—¨", "éç”²", "ä¹å®«", "å…«é—¨", "ç¥ç…"],
            "priority": 2,
            "patterns": [r"å¥‡é—¨\w*éç”²", r"ä¹å®«", r"å…«é—¨", r"ç¥ç…"]
        },
        "å…«å­—å‘½ç†": {
            "keywords": ["å…«å­—", "å››æŸ±", "å‘½ç†", "å¹²æ”¯", "çº³éŸ³"],
            "priority": 2,
            "patterns": [r"å…«å­—", r"å››æŸ±", r"å‘½ç†", r"å¹²æ”¯", r"çº³éŸ³"]
        },
        "å‘¨æ˜“åŸºç¡€": {
            "keywords": ["å‘¨æ˜“", "æ˜“ç»", "å…«å¦", "å…­åå››å¦", "å¦è±¡"],
            "priority": 1,
            "patterns": [r"å‘¨æ˜“", r"æ˜“ç»", r"å…«å¦", r"å…­åå››å¦"]
        }
    }
    
    text_lower = text.lower()
    file_lower = file_name.lower()
    
    category_scores = {}
    
    for category, config in category_patterns.items():
        score = 0.0
        
        # æ–‡ä»¶åæƒé‡æ›´é«˜
        filename_matches = sum(1 for kw in config["keywords"] if kw in file_lower)
        score += filename_matches * 3.0
        
        # å†…å®¹æ¨¡å¼åŒ¹é…
        for pattern in config["patterns"]:
            matches = len(re.findall(pattern, text, re.IGNORECASE))
            score += matches * 1.0
        
        # å…³é”®è¯å¯†åº¦
        total_keywords = sum(text_lower.count(kw) for kw in config["keywords"])
        if len(text) > 100:
            density = (total_keywords / len(text)) * 10000
            score += density
        
        category_scores[category] = score
    
    # æ‰¾å‡ºæœ€é«˜åˆ†
    if not category_scores or max(category_scores.values()) == 0:
        return "å…¶ä»–", 0.0, 5
    
    best_category = max(category_scores, key=category_scores.get)
    best_score = category_scores[best_category]
    
    confidence = min(best_score / 15.0, 1.0)
    priority = category_patterns[best_category]["priority"]
    
    return best_category, confidence, priority

def extract_hexagrams(text: str) -> List[Dict[str, Any]]:
    """æå–64å¦ä¿¡æ¯"""
    hexagram_names = [
        "ä¹¾", "å¤", "å±¯", "è’™", "éœ€", "è®¼", "å¸ˆ", "æ¯”",
        "å°ç•œ", "å±¥", "æ³°", "å¦", "åŒäºº", "å¤§æœ‰", "è°¦", "è±«",
        "éš", "è›Š", "ä¸´", "è§‚", "å™¬å—‘", "è´²", "å‰¥", "å¤",
        "æ— å¦„", "å¤§ç•œ", "é¢", "å¤§è¿‡", "å", "ç¦»", "å’¸", "æ’",
        "é", "å¤§å£®", "æ™‹", "æ˜å¤·", "å®¶äºº", "ç½", "è¹‡", "è§£",
        "æŸ", "ç›Š", "å¤¬", "å§¤", "èƒ", "å‡", "å›°", "äº•",
        "é©", "é¼", "éœ‡", "è‰®", "æ¸", "å½’å¦¹", "ä¸°", "æ—…",
        "å·½", "å…‘", "æ¶£", "èŠ‚", "ä¸­å­š", "å°è¿‡", "æ—¢æµ", "æœªæµ"
    ]
    
    hexagrams = []
    for i, name in enumerate(hexagram_names):
        patterns = [
            rf"{name}[å¦]?[ï¼š:]\s*([^ã€‚\n]+)",
            rf"ç¬¬\w*{name}å¦[ï¼š:]\s*([^ã€‚\n]+)",
        ]
        
        for pattern in patterns:
            matches = re.finditer(pattern, text)
            for match in matches:
                description = match.group(1).strip()
                if len(description) > 5:
                    hexagram = {
                        "number": i + 1,
                        "name": name,
                        "description": description,
                        "position": match.start(),
                    }
                    hexagrams.append(hexagram)
                    break  # é¿å…é‡å¤
    
    return hexagrams

def extract_yao_ci(text: str) -> List[Dict[str, Any]]:
    """æå–çˆ»è¾"""
    yao_positions = ["åˆ", "äºŒ", "ä¸‰", "å››", "äº”", "ä¸Š"]
    yao_types = ["å…­", "ä¹"]
    yao_ci = []
    
    for pos in yao_positions:
        for yao_type in yao_types:
            pattern = rf"({pos}{yao_type})[ï¼š:]([^ã€‚\n]{10,100})"
            matches = re.finditer(pattern, text)
            for match in matches:
                yao_text = match.group(2).strip()
                if len(yao_text) > 5:
                    yao = {
                        "position": pos,
                        "type": yao_type,
                        "full_name": match.group(1),
                        "text": yao_text,
                        "location": match.start()
                    }
                    yao_ci.append(yao)
    
    return yao_ci

def extract_annotations(text: str) -> List[Dict[str, Any]]:
    """æå–æ³¨è§£"""
    annotations = []
    annotation_patterns = [
        (r"æ³¨[ï¼š:]([^ã€‚\n]{10,200})", "æ³¨"),
        (r"è§£[ï¼š:]([^ã€‚\n]{10,200})", "è§£"),
        (r"é‡Š[ï¼š:]([^ã€‚\n]{10,200})", "é‡Š"),
        (r"æŒ‰[ï¼š:]([^ã€‚\n]{10,200})", "æŒ‰"),
        (r"æ›°[ï¼š:]([^ã€‚\n]{10,200})", "æ›°"),
    ]
    
    for pattern, ann_type in annotation_patterns:
        matches = re.finditer(pattern, text)
        for match in matches:
            content = match.group(1).strip()
            if len(content) > 10:
                annotation = {
                    "type": ann_type,
                    "content": content,
                    "position": match.start(),
                    "length": len(content)
                }
                annotations.append(annotation)
    
    return annotations

def extract_cases(text: str) -> List[Dict[str, Any]]:
    """æå–æ¡ˆä¾‹"""
    cases = []
    case_patterns = [
        r"ä¾‹[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d+][ï¼š:]([^ã€‚]{30,500})",
        r"æ¡ˆä¾‹[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d*][ï¼š:]([^ã€‚]{30,500})",
        r"å®ä¾‹[ï¼š:]([^ã€‚]{30,500})",
        r"å ä¾‹[ï¼š:]([^ã€‚]{30,500})",
    ]
    
    for pattern in case_patterns:
        matches = re.finditer(pattern, text, re.DOTALL)
        for match in matches:
            content = match.group(1).strip()
            if len(content) >= 30:
                case = {
                    "content": content,
                    "position": match.start(),
                    "length": len(content),
                    "preview": content[:100] + "..." if len(content) > 100 else content
                }
                cases.append(case)
    
    return cases

def extract_keywords(text: str) -> List[str]:
    """æå–å…³é”®è¯"""
    keywords = set()
    
    common_terms = [
        "é˜´é˜³", "äº”è¡Œ", "å…«å¦", "å…­çˆ»", "å åœ", "é¢„æµ‹", "å‘½ç†", 
        "é£æ°´", "å‘¨æ˜“", "å¤ªæ", "ç¥ç…", "åç¥", "å¤©å¹²", "åœ°æ”¯",
        "å¦è±¡", "çˆ»å˜", "åŠ¨çˆ»", "é™çˆ»", "ä¸–åº”", "å…­äº²", "ç”¨ç¥"
    ]
    
    for term in common_terms:
        if term in text:
            keywords.add(term)
    
    # æå–ä¹¦å
    book_names = re.findall(r'ã€Š([^ã€‹]{2,20})ã€‹', text)
    keywords.update(book_names[:5])
    
    return sorted(list(keywords))

def extract_author_dynasty(text: str, filename: str) -> Tuple[Optional[str], Optional[str]]:
    """æå–ä½œè€…å’Œæœä»£"""
    author = None
    dynasty = None
    
    # ä»æ–‡ä»¶åæå–ä½œè€…
    author_patterns = [
        r'([ç‹æå¼ åˆ˜é™ˆæ¨èµµé»„å‘¨å´][\u4e00-\u9fff]{1,3})[_-]',
        r'^([^0-9\s\-_]+)',
    ]
    
    for pattern in author_patterns:
        match = re.search(pattern, filename)
        if match:
            potential_author = match.group(1)
            if len(potential_author) >= 2 and not any(char.isdigit() for char in potential_author):
                author = potential_author
                break
    
    # æœä»£æå–
    dynasty_patterns = [
        r'[(ï¼ˆ]?(æ±‰|å”|å®‹|å…ƒ|æ˜|æ¸…)[æœä»£)ï¼‰]?',
        r'(æ±‰|å”|å®‹|å…ƒ|æ˜|æ¸…)ä»£',
    ]
    
    for pattern in dynasty_patterns:
        match = re.search(pattern, text)
        if match:
            dynasty = match.group(1)
            break
    
    return author, dynasty

def process_single_pdf_simple(file_path: str) -> Optional[Dict[str, Any]]:
    """ç®€åŒ–çš„PDFå¤„ç†å‡½æ•° - é€‚ç”¨äºå¤šçº¿ç¨‹"""
    try:
        start_time = time.time()
        file_path = Path(file_path)
        
        # æå–æ–‡æœ¬
        text = ""
        page_count = 0
        
        with pdfplumber.open(file_path) as pdf:
            page_count = len(pdf.pages)
            max_pages = min(page_count, 100)  # é™åˆ¶é¡µæ•°
            
            for i, page in enumerate(pdf.pages[:max_pages]):
                try:
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + "\n"
                except:
                    continue  # è·³è¿‡æœ‰é—®é¢˜çš„é¡µé¢
        
        if not text.strip():
            return None
        
        # åˆ†ç±»
        category, confidence, priority = classify_pdf(text, file_path.name)
        
        # æå–å†…å®¹
        hexagrams = extract_hexagrams(text)
        yao_ci = extract_yao_ci(text)
        annotations = extract_annotations(text)
        cases = extract_cases(text)
        keywords = extract_keywords(text)
        author, dynasty = extract_author_dynasty(text, file_path.name)
        
        processing_time = time.time() - start_time
        
        # åˆ›å»ºç»“æœ
        result = {
            "pdf_info": {
                "file_path": str(file_path),
                "file_name": file_path.name,
                "file_size": file_path.stat().st_size,
                "pages": page_count,
                "category": category,
                "confidence": confidence,
                "priority": priority,
                "processed_at": datetime.now().isoformat(),
                "text_length": len(text),
                "processing_time": processing_time
            },
            "content": {
                "hexagrams": hexagrams,
                "yao_ci": yao_ci,
                "annotations": annotations,
                "cases": cases,
                "keywords": keywords,
                "author": author,
                "dynasty": dynasty
            },
            "statistics": {
                "hexagram_count": len(hexagrams),
                "yao_ci_count": len(yao_ci),
                "annotation_count": len(annotations),
                "case_count": len(cases),
                "keyword_count": len(keywords)
            }
        }
        
        return result
        
    except Exception as e:
        print(f"å¤„ç†å¤±è´¥ {file_path}: {e}")
        return None

class PDFProcessorFixed:
    """ä¿®å¤ç‰ˆPDFå¤„ç†å™¨"""
    
    def __init__(self, data_dir: str, output_dir: str):
        self.data_dir = Path(data_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        (self.output_dir / "raw_texts").mkdir(exist_ok=True)
        (self.output_dir / "structured_data").mkdir(exist_ok=True)
        (self.output_dir / "categories").mkdir(exist_ok=True)
        (self.output_dir / "reports").mkdir(exist_ok=True)
    
    def process_all_pdfs(self, max_workers: int = 6) -> Dict[str, Any]:
        """ä½¿ç”¨çº¿ç¨‹æ± å¤„ç†æ‰€æœ‰PDFæ–‡ä»¶"""
        pdf_files = list(self.data_dir.glob("*.pdf"))
        print(f"æ‰¾åˆ° {len(pdf_files)} ä¸ªPDFæ–‡ä»¶")
        
        if len(pdf_files) == 0:
            return {"error": "æ²¡æœ‰æ‰¾åˆ°PDFæ–‡ä»¶"}
        
        results = []
        failed_files = []
        
        # ä½¿ç”¨çº¿ç¨‹æ± è€Œä¸æ˜¯è¿›ç¨‹æ± ï¼Œé¿å…pickleé—®é¢˜
        print("å¼€å§‹å¤„ç†æ–‡ä»¶...")
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            with tqdm(total=len(pdf_files), desc="å¤„ç†PDF") as pbar:
                # æäº¤ä»»åŠ¡
                future_to_file = {
                    executor.submit(process_single_pdf_simple, str(pdf_file)): pdf_file 
                    for pdf_file in pdf_files
                }
                
                # æ”¶é›†ç»“æœ
                for future in as_completed(future_to_file):
                    pdf_file = future_to_file[future]
                    try:
                        result = future.result()
                        if result:
                            results.append(result)
                            pbar.set_postfix_str(f"æˆåŠŸ: {pdf_file.name[:20]}...")
                        else:
                            failed_files.append(pdf_file.name)
                            pbar.set_postfix_str(f"å¤±è´¥: {pdf_file.name[:20]}...")
                    except Exception as e:
                        failed_files.append(pdf_file.name)
                        pbar.set_postfix_str(f"é”™è¯¯: {pdf_file.name[:20]}...")
                    finally:
                        pbar.update(1)
        
        # æŒ‰ä¼˜å…ˆçº§å’Œç±»åˆ«ç»„ç»‡ç»“æœ
        categorized_results = defaultdict(list)
        priority_results = defaultdict(list)
        
        for result in results:
            category = result["pdf_info"]["category"]
            priority = result["pdf_info"]["priority"]
            
            categorized_results[category].append(result)
            priority_results[priority].append(result)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        total_hexagrams = sum(r.get("statistics", {}).get("hexagram_count", 0) for r in results)
        total_yao_ci = sum(r.get("statistics", {}).get("yao_ci_count", 0) for r in results)
        total_annotations = sum(r.get("statistics", {}).get("annotation_count", 0) for r in results)
        total_cases = sum(r.get("statistics", {}).get("case_count", 0) for r in results)
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            "total_files": len(pdf_files),
            "processed_successfully": len(results),
            "failed_files": len(failed_files),
            "categories": {cat: len(files) for cat, files in categorized_results.items()},
            "priorities": {f"ä¼˜å…ˆçº§{p}": len(files) for p, files in priority_results.items()},
            "content_statistics": {
                "total_hexagrams": total_hexagrams,
                "total_yao_ci": total_yao_ci,
                "total_annotations": total_annotations,
                "total_cases": total_cases,
                "avg_per_file": {
                    "hexagrams": total_hexagrams / max(len(results), 1),
                    "yao_ci": total_yao_ci / max(len(results), 1),
                    "annotations": total_annotations / max(len(results), 1),
                    "cases": total_cases / max(len(results), 1)
                }
            },
            "failed_file_list": failed_files,
            "processing_time": datetime.now().isoformat()
        }
        
        return {
            "statistics": stats,
            "results": results,
            "by_category": dict(categorized_results),
            "by_priority": dict(priority_results)
        }
    
    def save_results(self, results: Dict[str, Any]):
        """ä¿å­˜å¤„ç†ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        full_results_file = self.output_dir / "structured_data" / f"complete_results_{timestamp}.json"
        with open(full_results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # æŒ‰ç±»åˆ«ä¿å­˜
        for category, files in results["by_category"].items():
            category_dir = self.output_dir / "categories" / category
            category_dir.mkdir(exist_ok=True)
            
            category_file = category_dir / f"{category}_{timestamp}.json"
            with open(category_file, 'w', encoding='utf-8') as f:
                json.dump({
                    "category": category,
                    "file_count": len(files),
                    "files": files
                }, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats_file = self.output_dir / "structured_data" / f"statistics_{timestamp}.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(results["statistics"], f, ensure_ascii=False, indent=2)
        
        # ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
        self.generate_summary_report(results, timestamp)
        
        print(f"ç»“æœå·²ä¿å­˜åˆ°: {self.output_dir}")
        return full_results_file
    
    def generate_summary_report(self, results: Dict[str, Any], timestamp: str):
        """ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š"""
        stats = results["statistics"]
        
        report = f"""
# ğŸ“‹ PDFå¤„ç†æ‘˜è¦æŠ¥å‘Š
**æ—¶é—´**: {timestamp}

## ğŸ“Š å¤„ç†ç»Ÿè®¡
- **æ€»æ–‡ä»¶æ•°**: {stats['total_files']}
- **æˆåŠŸå¤„ç†**: {stats['processed_successfully']}
- **å¤±è´¥æ•°é‡**: {stats['failed_files']}
- **æˆåŠŸç‡**: {stats['processed_successfully']/max(stats['total_files'], 1)*100:.1f}%

## ğŸ“š åˆ†ç±»ç»Ÿè®¡
"""
        
        for category, count in sorted(stats["categories"].items(), key=lambda x: x[1], reverse=True):
            percentage = count / max(stats['processed_successfully'], 1) * 100
            report += f"- **{category}**: {count} ä¸ªæ–‡ä»¶ ({percentage:.1f}%)\n"
        
        report += "\n## â­ ä¼˜å…ˆçº§åˆ†å¸ƒ\n"
        for priority in sorted(stats["priorities"].keys()):
            count = stats["priorities"][priority]
            report += f"- **{priority}**: {count} ä¸ªæ–‡ä»¶\n"
        
        # å†…å®¹ç»Ÿè®¡
        if "content_statistics" in stats:
            content_stats = stats["content_statistics"]
            report += f"""
## ğŸ“– å†…å®¹æå–ç»Ÿè®¡
- **æ€»å¦è±¡**: {content_stats['total_hexagrams']} ä¸ª
- **æ€»çˆ»è¾**: {content_stats['total_yao_ci']} ä¸ª
- **æ€»æ³¨è§£**: {content_stats['total_annotations']} ä¸ª
- **æ€»æ¡ˆä¾‹**: {content_stats['total_cases']} ä¸ª

### å¹³å‡æ¯æ–‡ä»¶
- **å¦è±¡**: {content_stats['avg_per_file']['hexagrams']:.1f} ä¸ª
- **çˆ»è¾**: {content_stats['avg_per_file']['yao_ci']:.1f} ä¸ª
- **æ³¨è§£**: {content_stats['avg_per_file']['annotations']:.1f} ä¸ª
- **æ¡ˆä¾‹**: {content_stats['avg_per_file']['cases']:.1f} ä¸ª
"""
        
        if stats["failed_files"] > 0:
            report += f"\n## âŒ å¤±è´¥æ–‡ä»¶ ({stats['failed_files']} ä¸ª)\n"
            for failed_file in stats["failed_file_list"][:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                report += f"- {failed_file}\n"
            if len(stats["failed_file_list"]) > 10:
                report += f"- ... è¿˜æœ‰ {len(stats['failed_file_list']) - 10} ä¸ªæ–‡ä»¶\n"
        
        report += f"""
## ğŸ’¾ è¾“å‡ºæ–‡ä»¶
- å®Œæ•´ç»“æœ: `structured_data/complete_results_{timestamp}.json`
- ç»Ÿè®¡ä¿¡æ¯: `structured_data/statistics_{timestamp}.json`
- åˆ†ç±»ç»“æœ: `categories/` ç›®å½•ä¸‹æŒ‰ç±»åˆ«ä¿å­˜

## ğŸ“ ä½¿ç”¨å»ºè®®
1. ä¼˜å…ˆçº§1çš„æ–‡ä»¶åŒ…å«æœ€é‡è¦çš„æ˜“å­¦å†…å®¹
2. æŒ‰ç±»åˆ«æŸ¥çœ‹ä¸“é—¨çš„JSONæ–‡ä»¶
3. å¤±è´¥çš„æ–‡ä»¶å¯èƒ½æ˜¯æ‰«æç‰ˆæˆ–æŸåçš„PDF
"""
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.output_dir / f"processing_report_{timestamp}.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(report)
        return report_file

def main():
    """ä¸»å‡½æ•°"""
    data_dir = "/mnt/d/desktop/appp/data"
    output_dir = "/mnt/d/desktop/appp/structured_data"
    
    print("ğŸš€ å¼€å§‹æ‰¹é‡å¤„ç†æ˜“å­¦PDFæ–‡ä»¶ (ä¿®å¤ç‰ˆ)")
    print(f"ğŸ“‚ æ•°æ®ç›®å½•: {data_dir}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    
    # åˆ›å»ºå¤„ç†å™¨
    processor = PDFProcessorFixed(data_dir, output_dir)
    
    # æ£€æŸ¥æ•°æ®ç›®å½•
    if not Path(data_dir).exists():
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        return
    
    # å¼€å§‹å¤„ç†
    start_time = datetime.now()
    print(f"â° å¼€å§‹æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    try:
        # ä½¿ç”¨6ä¸ªçº¿ç¨‹å¹¶è¡Œå¤„ç†
        print("ğŸ”„ å¼€å§‹å¤šçº¿ç¨‹å¤„ç†...")
        results = processor.process_all_pdfs(max_workers=6)
        
        if "error" in results:
            print(f"âŒ {results['error']}")
            return
        
        # ä¿å­˜ç»“æœ
        print("\nğŸ’¾ ä¿å­˜å¤„ç†ç»“æœ...")
        result_file = processor.save_results(results)
        
        end_time = datetime.now()
        duration = end_time - start_time
        
        print(f"\nâœ… å¤„ç†å®Œæˆ!")
        print(f"â° æ€»è€—æ—¶: {duration}")
        print(f"ğŸ“Š æˆåŠŸå¤„ç†: {results['statistics']['processed_successfully']}/{results['statistics']['total_files']}")
        print(f"ğŸ—‚ï¸ ä¸»è¦ç»“æœæ–‡ä»¶: {result_file}")
        
        # æ˜¾ç¤ºåˆ†ç±»ç»Ÿè®¡
        if results['statistics']['categories']:
            print(f"\nğŸ“š åˆ†ç±»ç»Ÿè®¡:")
            for category, count in results['statistics']['categories'].items():
                print(f"  - {category}: {count} ä¸ªæ–‡ä»¶")
            
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­å¤„ç†")
    except Exception as e:
        print(f"\nâŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")

if __name__ == "__main__":
    main()