# ğŸ“Š æ˜“å¦åº”ç”¨æ•°æ®å¤„ç†å®Œæ•´æ–¹æ¡ˆ

## ğŸ¯ ç›®æ ‡
å°†3.7GBçš„æ˜“å­¦èµ„æ–™è½¬åŒ–ä¸ºï¼š
- **æ ¸å¿ƒæ•°æ®åŒ…**ï¼š5-10MBï¼ˆå†…ç½®åº”ç”¨ï¼‰
- **æ‰©å±•æ•°æ®åŒ…**ï¼š50-100MBï¼ˆæŒ‰éœ€ä¸‹è½½ï¼‰
- **äº‘ç«¯æ•°æ®åº“**ï¼šå®Œæ•´çŸ¥è¯†å›¾è°±ï¼ˆAPIè®¿é—®ï¼‰

## ğŸ“ æ•°æ®é›†åˆ†æ

### ç°æœ‰æ•°æ®ç±»å‹
```
æ€»å¤§å°ï¼š3.7GB
æ–‡ä»¶ç±»å‹ï¼š
â”œâ”€â”€ PDFæ–‡æ¡£ (90%)ï¼šå¤ç±ã€æ³¨è§£ã€æ¡ˆä¾‹
â”œâ”€â”€ å›¾ç‰‡æ–‡ä»¶ (5%)ï¼šå¦è±¡å›¾ã€ç¤ºæ„å›¾
â”œâ”€â”€ æ–‡æœ¬æ–‡ä»¶ (3%)ï¼šç¬”è®°ã€æ•´ç†ç¨¿
â””â”€â”€ å…¶ä»–æ–‡ä»¶ (2%)ï¼šè§†é¢‘ã€éŸ³é¢‘
```

### æ•°æ®ä»·å€¼åˆ†çº§
```
ğŸ”´ æ ¸å¿ƒæ•°æ®ï¼ˆå¿…éœ€ï¼‰
â”œâ”€â”€ 64å¦åŸºç¡€ä¿¡æ¯
â”œâ”€â”€ 384çˆ»è¾
â”œâ”€â”€ äº”è¡Œç”Ÿå…‹å…³ç³»
â”œâ”€â”€ å¤©å¹²åœ°æ”¯
â””â”€â”€ åŸºç¡€èµ·å¦è§„åˆ™

ğŸŸ¡ é‡è¦æ•°æ®ï¼ˆæ¨èï¼‰
â”œâ”€â”€ å†ä»£åå®¶æ³¨è§£
â”œâ”€â”€ ç»å…¸æ¡ˆä¾‹100ä¾‹
â”œâ”€â”€ æ¢…èŠ±æ˜“æ•°å£è¯€
â”œâ”€â”€ å…­çˆ»æ–­è¯­é›†
â””â”€â”€ å¸¸ç”¨ç¥ç…

ğŸŸ¢ æ‰©å±•æ•°æ®ï¼ˆå¯é€‰ï¼‰
â”œâ”€â”€ å®Œæ•´å¤ç±åŸæ–‡
â”œâ”€â”€ å­¦æœ¯è®ºæ–‡
â”œâ”€â”€ ç°ä»£æ¡ˆä¾‹
â””â”€â”€ éŸ³è§†é¢‘æ•™ç¨‹
```

## ğŸ”§ æ•°æ®å¤„ç†æµç¨‹

### Phase 1ï¼šæ•°æ®æå–ä¸æ¸…æ´—

#### 1.1 PDFæ–‡æœ¬æå–
```python
# extract_pdf.py
import PyPDF2
import pdfplumber
import os
import json
from pathlib import Path

class PDFExtractor:
    def __init__(self, input_dir, output_dir):
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
    def extract_all(self):
        """æ‰¹é‡æå–æ‰€æœ‰PDF"""
        results = []
        pdf_files = list(self.input_dir.glob("*.pdf"))
        
        for pdf_file in pdf_files:
            print(f"å¤„ç†: {pdf_file.name}")
            try:
                data = self.extract_single(pdf_file)
                results.append(data)
                
                # ä¿å­˜å•ä¸ªæ–‡ä»¶çš„æå–ç»“æœ
                output_file = self.output_dir / f"{pdf_file.stem}.json"
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                    
            except Exception as e:
                print(f"é”™è¯¯: {pdf_file.name} - {e}")
                
        return results
    
    def extract_single(self, pdf_path):
        """æå–å•ä¸ªPDF"""
        with pdfplumber.open(pdf_path) as pdf:
            return {
                'filename': pdf_path.name,
                'metadata': self.classify_book(pdf_path.name),
                'pages': len(pdf.pages),
                'content': self.extract_content(pdf),
                'tables': self.extract_tables(pdf),
                'images_count': self.count_images(pdf)
            }
    
    def classify_book(self, filename):
        """è‡ªåŠ¨åˆ†ç±»ä¹¦ç±"""
        categories = {
            'å…­çˆ»': ['å…­çˆ»', 'åœç­®', 'ç«ç æ—', 'å¢åˆ åœæ˜“'],
            'æ¢…èŠ±': ['æ¢…èŠ±', 'æ˜“æ•°', 'è§‚æ¢…'],
            'å…«å­—': ['å…«å­—', 'å››æŸ±', 'å‘½ç†', 'å­å¹³'],
            'å¥‡é—¨': ['å¥‡é—¨', 'éç”²'],
            'å…­å£¬': ['å…­å£¬', 'å¤§å…­å£¬', 'å£¬å '],
            'ç´«å¾®': ['ç´«å¾®', 'æ–—æ•°'],
            'é£æ°´': ['é£æ°´', 'å ªèˆ†', 'ç„ç©º'],
            'ç»¼åˆ': ['æ˜“ç»', 'å‘¨æ˜“', 'æ˜“å­¦']
        }
        
        for category, keywords in categories.items():
            if any(kw in filename for kw in keywords):
                return {'category': category, 'importance': self.get_importance(filename)}
        
        return {'category': 'å…¶ä»–', 'importance': 'low'}
    
    def get_importance(self, filename):
        """åˆ¤æ–­é‡è¦æ€§"""
        core_books = ['å¢åˆ åœæ˜“', 'æ¢…èŠ±æ˜“æ•°', 'å‘¨æ˜“', 'æ˜“ç»', 'ç«ç æ—']
        important_books = ['å…­çˆ»', 'åƒé‡Œå‘½ç¨¿', 'ä¸‰å‘½é€šä¼š', 'ç´«å¾®æ–—æ•°å…¨ä¹¦']
        
        if any(book in filename for book in core_books):
            return 'core'
        elif any(book in filename for book in important_books):
            return 'important'
        return 'extended'
    
    def extract_content(self, pdf):
        """æå–æ–‡æœ¬å†…å®¹"""
        content = []
        for i, page in enumerate(pdf.pages[:10]):  # å…ˆæå–å‰10é¡µä½œä¸ºæ ·æœ¬
            text = page.extract_text()
            if text:
                content.append({
                    'page': i + 1,
                    'text': text[:1000]  # æˆªå–å‰1000å­—ç¬¦
                })
        return content
    
    def extract_tables(self, pdf):
        """æå–è¡¨æ ¼æ•°æ®"""
        tables = []
        for i, page in enumerate(pdf.pages[:5]):
            page_tables = page.extract_tables()
            if page_tables:
                tables.append({
                    'page': i + 1,
                    'count': len(page_tables)
                })
        return tables
    
    def count_images(self, pdf):
        """ç»Ÿè®¡å›¾ç‰‡æ•°é‡"""
        # ç®€åŒ–ç‰ˆï¼šç»Ÿè®¡é¡µé¢ä¸­çš„å›¾ç‰‡æ•°é‡
        return sum(1 for page in pdf.pages if page.images)

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    extractor = PDFExtractor("../data", "./extracted")
    results = extractor.extract_all()
    print(f"å¤„ç†å®Œæˆ: {len(results)} ä¸ªæ–‡ä»¶")
```

#### 1.2 ç»“æ„åŒ–çŸ¥è¯†æå–
```python
# structure_knowledge.py
import json
import re
from typing import Dict, List

class KnowledgeStructurer:
    def __init__(self):
        self.hexagrams = {}
        self.interpretations = {}
        self.cases = []
        
    def process_yijing_text(self, text):
        """å¤„ç†æ˜“ç»åŸæ–‡ï¼Œæå–64å¦"""
        hexagram_pattern = r'(ç¬¬\d+å¦|[\u4e00-\u9fa5]{1,2}å¦)'
        
        # 64å¦åŸºç¡€ä¿¡æ¯
        self.hexagrams = {
            'ä¹¾': {'number': 1, 'symbol': 'â˜°â˜°', 'element': 'å¤©', 'attribute': 'åˆšå¥'},
            'å¤': {'number': 2, 'symbol': 'â˜·â˜·', 'element': 'åœ°', 'attribute': 'æŸ”é¡º'},
            'å±¯': {'number': 3, 'symbol': 'â˜µâ˜³', 'element': 'æ°´é›·', 'attribute': 'å§‹ç”Ÿ'},
            'è’™': {'number': 4, 'symbol': 'â˜¶â˜µ', 'element': 'å±±æ°´', 'attribute': 'å¯è’™'},
            # ... ç»§ç»­æ·»åŠ å…¶ä»–60å¦
        }
        
        # æå–å¦è¾å’Œçˆ»è¾
        for name, info in self.hexagrams.items():
            info['judgment'] = self.extract_judgment(text, name)
            info['lines'] = self.extract_line_texts(text, name)
            
    def extract_judgment(self, text, hexagram_name):
        """æå–å¦è¾"""
        pattern = f'{hexagram_name}[ï¼š:]([^ã€‚]+ã€‚)'
        match = re.search(pattern, text)
        return match.group(1) if match else ""
    
    def extract_line_texts(self, text, hexagram_name):
        """æå–çˆ»è¾"""
        lines = []
        positions = ['åˆ', 'äºŒ', 'ä¸‰', 'å››', 'äº”', 'ä¸Š']
        for pos in positions:
            pattern = f'{pos}[ä¹å…­][ï¼š:]([^ã€‚]+ã€‚)'
            match = re.search(pattern, text)
            if match:
                lines.append({
                    'position': pos,
                    'text': match.group(1)
                })
        return lines
    
    def extract_cases(self, text, source):
        """æå–å åœæ¡ˆä¾‹"""
        case_pattern = r'(ä¾‹|æ¡ˆ|å å¾—?)[ï¼š:](.+?)(?=ä¾‹|æ¡ˆ|$)'
        matches = re.findall(case_pattern, text, re.DOTALL)
        
        for match in matches:
            case = self.parse_case(match[1])
            if case:
                case['source'] = source
                self.cases.append(case)
    
    def parse_case(self, case_text):
        """è§£æå•ä¸ªæ¡ˆä¾‹"""
        return {
            'question': self.extract_question(case_text),
            'hexagram': self.extract_hexagram(case_text),
            'analysis': self.extract_analysis(case_text),
            'result': self.extract_result(case_text)
        }
    
    def extract_question(self, text):
        """æå–é—®é¢˜"""
        patterns = [r'é—®[ï¼š:](.+?)[ã€‚ï¼Ÿ]', r'å (.+?)[ã€‚ï¼Œ]', r'æµ‹(.+?)[ã€‚ï¼Œ]']
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(1)
        return ""
    
    def extract_hexagram(self, text):
        """æå–å¦è±¡"""
        # è¯†åˆ«å¦å
        for name in self.hexagrams.keys():
            if name in text:
                return name
        return ""
    
    def extract_analysis(self, text):
        """æå–åˆ†æè¿‡ç¨‹"""
        pattern = r'(æ|è§£|æ–­)[ï¼š:](.+?)(?=åº”éªŒ|ç»“æœ|$)'
        match = re.search(pattern, text, re.DOTALL)
        return match.group(2) if match else text[:200]
    
    def extract_result(self, text):
        """æå–ç»“æœ"""
        pattern = r'(æœ|éªŒ|åæ¥)[ï¼š:](.+?)$'
        match = re.search(pattern, text)
        return match.group(2) if match else ""
    
    def save_structured_data(self, output_dir):
        """ä¿å­˜ç»“æ„åŒ–æ•°æ®"""
        # 1. ä¿å­˜æ ¸å¿ƒæ•°æ®ï¼ˆç»™åº”ç”¨ä½¿ç”¨ï¼‰
        core_data = {
            'hexagrams': self.hexagrams,
            'basic_interpretations': self.get_basic_interpretations()
        }
        
        with open(f'{output_dir}/core_data.json', 'w', encoding='utf-8') as f:
            json.dump(core_data, f, ensure_ascii=False, indent=2)
        
        # 2. ä¿å­˜æ‰©å±•æ•°æ®ï¼ˆæŒ‰éœ€åŠ è½½ï¼‰
        extended_data = {
            'detailed_interpretations': self.interpretations,
            'cases': self.cases[:100]  # ç²¾é€‰100ä¸ªæ¡ˆä¾‹
        }
        
        with open(f'{output_dir}/extended_data.json', 'w', encoding='utf-8') as f:
            json.dump(extended_data, f, ensure_ascii=False, indent=2)
    
    def get_basic_interpretations(self):
        """è·å–åŸºç¡€è§£é‡Šï¼ˆç²¾ç®€ç‰ˆï¼‰"""
        # æ¯ä¸ªå¦åªä¿ç•™100å­—çš„æ ¸å¿ƒè§£é‡Š
        basic = {}
        for hex_name, interp in self.interpretations.items():
            basic[hex_name] = interp[:100] if len(interp) > 100 else interp
        return basic
```

### Phase 2ï¼šæ•°æ®åˆ†çº§å­˜å‚¨

#### 2.1 SQLiteæ•°æ®åº“è®¾è®¡
```python
# create_database.py
import sqlite3
import json
from pathlib import Path

class DatabaseBuilder:
    def __init__(self, db_path='yigua.db'):
        self.conn = sqlite3.connect(db_path)
        self.cursor = self.conn.cursor()
        self.create_tables()
    
    def create_tables(self):
        """åˆ›å»ºæ•°æ®è¡¨"""
        
        # 1. å¦è±¡è¡¨ï¼ˆæ ¸å¿ƒï¼‰
        self.cursor.execute('''
        CREATE TABLE IF NOT EXISTS hexagrams (
            id INTEGER PRIMARY KEY,
            name TEXT NOT NULL UNIQUE,
            number INTEGER,
            symbol TEXT,
            upper_trigram TEXT,
            lower_trigram TEXT,
            element TEXT,
            attribute TEXT,
            judgment TEXT,
            image TEXT,
            size_bytes INTEGER DEFAULT 0
        )''')
        
        # 2. çˆ»è¾è¡¨ï¼ˆæ ¸å¿ƒï¼‰
        self.cursor.execute('''
        CREATE TABLE IF NOT EXISTS line_texts (
            id INTEGER PRIMARY KEY,
            hexagram_id INTEGER,
            position INTEGER,
            yin_yang TEXT,
            text TEXT,
            interpretation TEXT,
            FOREIGN KEY (hexagram_id) REFERENCES hexagrams(id)
        )''')
        
        # 3. è§£é‡Šè¡¨ï¼ˆæ‰©å±•ï¼‰
        self.cursor.execute('''
        CREATE TABLE IF NOT EXISTS interpretations (
            id INTEGER PRIMARY KEY,
            hexagram_id INTEGER,
            type TEXT,  -- 'classic', 'modern', 'expert'
            author TEXT,
            content TEXT,
            source TEXT,
            importance INTEGER DEFAULT 0,
            FOREIGN KEY (hexagram_id) REFERENCES hexagrams(id)
        )''')
        
        # 4. æ¡ˆä¾‹è¡¨ï¼ˆæ‰©å±•ï¼‰
        self.cursor.execute('''
        CREATE TABLE IF NOT EXISTS cases (
            id INTEGER PRIMARY KEY,
            title TEXT,
            category TEXT,
            hexagram_id INTEGER,
            question TEXT,
            date TEXT,
            method TEXT,
            analysis TEXT,
            result TEXT,
            accuracy INTEGER,
            source TEXT,
            tags TEXT,
            FOREIGN KEY (hexagram_id) REFERENCES hexagrams(id)
        )''')
        
        # 5. çŸ¥è¯†å›¾è°±å…³ç³»è¡¨
        self.cursor.execute('''
        CREATE TABLE IF NOT EXISTS relations (
            id INTEGER PRIMARY KEY,
            source_type TEXT,
            source_id INTEGER,
            target_type TEXT,  
            target_id INTEGER,
            relation_type TEXT,
            weight REAL DEFAULT 1.0
        )''')
        
        # 6. åˆ›å»ºå…¨æ–‡æœç´¢ç´¢å¼•
        self.cursor.execute('''
        CREATE VIRTUAL TABLE IF NOT EXISTS search_index USING fts5(
            title,
            content,
            tags,
            tokenize='unicode61'
        )''')
        
        self.conn.commit()
    
    def import_core_data(self, json_path):
        """å¯¼å…¥æ ¸å¿ƒæ•°æ®"""
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # å¯¼å…¥64å¦
        for name, info in data['hexagrams'].items():
            self.cursor.execute('''
                INSERT OR REPLACE INTO hexagrams 
                (name, number, symbol, element, attribute, judgment)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (name, info['number'], info['symbol'], 
                  info['element'], info['attribute'], info.get('judgment', '')))
            
            # å¯¼å…¥çˆ»è¾
            hexagram_id = self.cursor.lastrowid
            for line in info.get('lines', []):
                self.cursor.execute('''
                    INSERT INTO line_texts
                    (hexagram_id, position, text)
                    VALUES (?, ?, ?)
                ''', (hexagram_id, line['position'], line['text']))
        
        self.conn.commit()
        print(f"å¯¼å…¥{len(data['hexagrams'])}ä¸ªå¦è±¡")
    
    def analyze_data_size(self):
        """åˆ†ææ•°æ®å¤§å°"""
        tables = ['hexagrams', 'line_texts', 'interpretations', 'cases']
        total_size = 0
        
        for table in tables:
            self.cursor.execute(f"SELECT COUNT(*) FROM {table}")
            count = self.cursor.fetchone()[0]
            
            # ä¼°ç®—æ¯ä¸ªè¡¨çš„å¤§å°
            self.cursor.execute(f"SELECT * FROM {table} LIMIT 1")
            if self.cursor.fetchone():
                # ç®€å•ä¼°ç®—ï¼šæ¯è¡Œçº¦500å­—èŠ‚
                size = count * 500
                total_size += size
                print(f"{table}: {count}æ¡è®°å½•, çº¦{size/1024:.1f}KB")
        
        print(f"æ€»å¤§å°: çº¦{total_size/1024/1024:.1f}MB")
        return total_size
    
    def export_for_app(self, output_dir):
        """å¯¼å‡ºåº”ç”¨æ‰€éœ€çš„æ•°æ®"""
        output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True)
        
        # 1. å¯¼å‡ºæ ¸å¿ƒæ•°æ®ï¼ˆå†…ç½®åˆ°APKï¼‰
        self.cursor.execute('''
            SELECT name, number, symbol, element, attribute, judgment
            FROM hexagrams
            ORDER BY number
        ''')
        
        hexagrams = []
        for row in self.cursor.fetchall():
            hexagrams.append({
                'name': row[0],
                'number': row[1],
                'symbol': row[2],
                'element': row[3],
                'attribute': row[4],
                'judgment': row[5][:100]  # åªä¿ç•™100å­—
            })
        
        core_db = {
            'version': '1.0',
            'hexagrams': hexagrams,
            'size_kb': len(json.dumps(hexagrams)) / 1024
        }
        
        with open(output_dir / 'core.json', 'w', encoding='utf-8') as f:
            json.dump(core_db, f, ensure_ascii=False)
        
        print(f"æ ¸å¿ƒæ•°æ®: {core_db['size_kb']:.1f}KB")
        
        # 2. å¯¼å‡ºæ‰©å±•æ•°æ®åŒ…ï¼ˆæŒ‰éœ€ä¸‹è½½ï¼‰
        packages = {
            'interpretations': self.export_interpretations(),
            'cases': self.export_cases(),
            'advanced': self.export_advanced()
        }
        
        for name, data in packages.items():
            with open(output_dir / f'{name}.json', 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False)
            size = len(json.dumps(data)) / 1024
            print(f"{name}åŒ…: {size:.1f}KB")
    
    def export_interpretations(self):
        """å¯¼å‡ºè§£é‡ŠåŒ…"""
        self.cursor.execute('''
            SELECT h.name, i.type, i.author, i.content
            FROM interpretations i
            JOIN hexagrams h ON i.hexagram_id = h.id
            WHERE i.importance > 5
            LIMIT 1000
        ''')
        
        return [
            {
                'hexagram': row[0],
                'type': row[1],
                'author': row[2],
                'content': row[3][:500]  # é™åˆ¶é•¿åº¦
            }
            for row in self.cursor.fetchall()
        ]
    
    def export_cases(self):
        """å¯¼å‡ºæ¡ˆä¾‹åŒ…"""
        self.cursor.execute('''
            SELECT c.title, h.name, c.question, c.analysis, c.result
            FROM cases c
            JOIN hexagrams h ON c.hexagram_id = h.id
            WHERE c.accuracy > 7
            LIMIT 100
        ''')
        
        return [
            {
                'title': row[0],
                'hexagram': row[1],
                'question': row[2],
                'analysis': row[3][:300],
                'result': row[4]
            }
            for row in self.cursor.fetchall()
        ]
    
    def export_advanced(self):
        """å¯¼å‡ºé«˜çº§åŠŸèƒ½åŒ…"""
        # å¯¼å‡ºçŸ¥è¯†å›¾è°±å…³ç³»ç­‰é«˜çº§æ•°æ®
        return {
            'relations': [],
            'advanced_algorithms': [],
            'expert_rules': []
        }
    
    def close(self):
        self.conn.close()

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    builder = DatabaseBuilder()
    builder.import_core_data('./extracted/core_data.json')
    builder.analyze_data_size()
    builder.export_for_app('./app_data')
    builder.close()
```

### Phase 3ï¼šäº‘ç«¯APIè®¾è®¡

#### 3.1 Serverless API (ä½¿ç”¨Vercel)
```javascript
// api/knowledge.js
import { createClient } from '@supabase/supabase-js'

const supabase = createClient(
  process.env.SUPABASE_URL,
  process.env.SUPABASE_KEY
)

export default async function handler(req, res) {
  const { method, query, body } = req
  
  switch (method) {
    case 'GET':
      return handleGet(query, res)
    case 'POST':
      return handlePost(body, res)
    default:
      res.status(405).json({ error: 'Method not allowed' })
  }
}

async function handleGet(query, res) {
  const { type, id, search } = query
  
  if (search) {
    // å…¨æ–‡æœç´¢
    const { data, error } = await supabase
      .from('knowledge')
      .select('*')
      .textSearch('content', search)
      .limit(20)
    
    return res.status(200).json({ results: data })
  }
  
  if (type === 'hexagram' && id) {
    // è·å–å¦è±¡è¯¦ç»†ä¿¡æ¯
    const { data } = await supabase
      .from('hexagrams')
      .select(`
        *,
        interpretations (
          type,
          content,
          author
        ),
        cases (
          title,
          question,
          analysis
        )
      `)
      .eq('id', id)
      .single()
    
    return res.status(200).json(data)
  }
  
  // è·å–æ•°æ®åŒ…ä¿¡æ¯
  const packages = {
    core: { size: '5MB', version: '1.0', free: true },
    cases: { size: '15MB', version: '1.0', free: false },
    advanced: { size: '30MB', version: '1.0', free: false }
  }
  
  res.status(200).json({ packages })
}

async function handlePost(body, res) {
  const { action, data } = body
  
  if (action === 'ai_interpret') {
    // AIè§£è¯»ï¼ˆè°ƒç”¨OpenAIæˆ–æœ¬åœ°æ¨¡å‹ï¼‰
    const interpretation = await getAIInterpretation(data)
    return res.status(200).json({ interpretation })
  }
  
  if (action === 'save_record') {
    // ä¿å­˜ç”¨æˆ·å åœè®°å½•
    const { data: record } = await supabase
      .from('user_records')
      .insert(data)
      .select()
      .single()
    
    return res.status(201).json(record)
  }
  
  res.status(400).json({ error: 'Invalid action' })
}
```

## ğŸ“± åº”ç”¨é›†æˆæ–¹æ¡ˆ

### Flutterç«¯æ•°æ®æœåŠ¡
```dart
// lib/services/data_service_v2.dart
class DataServiceV2 {
  static const String baseUrl = 'https://your-api.vercel.app/api';
  
  // æœ¬åœ°æ•°æ®åº“
  late Database localDB;
  
  // æ•°æ®ç­–ç•¥
  final DataStrategy strategy = DataStrategy();
  
  Future<void> initialize() async {
    // 1. åˆå§‹åŒ–æœ¬åœ°æ•°æ®åº“
    localDB = await openDatabase(
      'yigua_local.db',
      version: 1,
      onCreate: (db, version) {
        // åˆ›å»ºæœ¬åœ°è¡¨
      },
    );
    
    // 2. åŠ è½½æ ¸å¿ƒæ•°æ®
    await loadCoreData();
    
    // 3. æ£€æŸ¥æ›´æ–°
    checkForUpdates();
  }
  
  Future<void> loadCoreData() async {
    // ä»assetsåŠ è½½æ ¸å¿ƒæ•°æ®
    final String jsonString = await rootBundle.loadString(
      'assets/data/core.json'
    );
    final coreData = json.decode(jsonString);
    
    // å­˜å…¥æœ¬åœ°æ•°æ®åº“
    for (var hexagram in coreData['hexagrams']) {
      await localDB.insert('hexagrams', hexagram);
    }
  }
  
  Future<HexagramData> getHexagram(String name) async {
    // ä¼˜å…ˆæœ¬åœ°
    var data = await localDB.query(
      'hexagrams',
      where: 'name = ?',
      whereArgs: [name],
    );
    
    if (data.isEmpty && hasNetwork()) {
      // ä»äº‘ç«¯è·å–
      data = await fetchFromCloud('/hexagram/$name');
      // ç¼“å­˜åˆ°æœ¬åœ°
      await localDB.insert('hexagrams', data);
    }
    
    return HexagramData.fromMap(data.first);
  }
  
  Future<List<Case>> searchCases(String keyword) async {
    // æœç´¢éœ€è¦ç½‘ç»œ
    if (!hasNetwork()) {
      return getCachedCases(keyword);
    }
    
    final response = await http.get(
      Uri.parse('$baseUrl/knowledge?search=$keyword'),
    );
    
    final cases = json.decode(response.body)['results'];
    return cases.map((c) => Case.fromMap(c)).toList();
  }
  
  Future<void> downloadPackage(String packageId) async {
    // ä¸‹è½½æ‰©å±•åŒ…
    final url = '$baseUrl/packages/$packageId';
    final response = await http.get(Uri.parse(url));
    
    if (response.statusCode == 200) {
      final data = json.decode(response.body);
      
      // å­˜å‚¨åˆ°æœ¬åœ°
      await savePackageData(packageId, data);
      
      // æ›´æ–°é…ç½®
      await updatePackageStatus(packageId, 'downloaded');
    }
  }
}
```

## ğŸš€ å®æ–½è®¡åˆ’

### Week 1: æ•°æ®æå–
- [ ] è¿è¡ŒPDFæå–è„šæœ¬
- [ ] æ•´ç†åˆ†ç±»æ•°æ®
- [ ] ç”Ÿæˆç»“æ„åŒ–JSON

### Week 2: æ•°æ®åº“æ„å»º
- [ ] åˆ›å»ºSQLiteæ•°æ®åº“
- [ ] å¯¼å…¥æ ¸å¿ƒæ•°æ®
- [ ] ç”Ÿæˆæ•°æ®åŒ…

### Week 3: äº‘ç«¯éƒ¨ç½²
- [ ] éƒ¨ç½²Supabase/Vercel
- [ ] ä¸Šä¼ æ‰©å±•æ•°æ®
- [ ] æµ‹è¯•APIæ¥å£

### Week 4: åº”ç”¨é›†æˆ
- [ ] é›†æˆæœ¬åœ°æ•°æ®
- [ ] å®ç°äº‘ç«¯åŒæ­¥
- [ ] æµ‹è¯•ç¦»çº¿åŠŸèƒ½

## ğŸ’¾ é¢„æœŸæˆæœ

### æ•°æ®åŒ…å¤§å°
```
æ ¸å¿ƒåŒ…ï¼š5MBï¼ˆå¿…éœ€ï¼Œå†…ç½®APKï¼‰
â”œâ”€â”€ 64å¦åŸºç¡€ï¼š1MB
â”œâ”€â”€ 384çˆ»è¾ï¼š2MB
â”œâ”€â”€ äº”è¡Œå…³ç³»ï¼š0.5MB
â””â”€â”€ åŸºç¡€ç®—æ³•ï¼š1.5MB

æ‰©å±•åŒ…1ï¼š15MBï¼ˆå…­çˆ»ä¸“ä¸šï¼‰
â”œâ”€â”€ å†ä»£æ³¨è§£ï¼š8MB
â”œâ”€â”€ ç»å…¸æ¡ˆä¾‹ï¼š5MB
â””â”€â”€ æ–­è¯­é›†ï¼š2MB

æ‰©å±•åŒ…2ï¼š20MBï¼ˆæ¢…èŠ±æ˜“æ•°ï¼‰
æ‰©å±•åŒ…3ï¼š25MBï¼ˆå…«å­—å‘½ç†ï¼‰
```

### æ€§èƒ½æŒ‡æ ‡
- ç¦»çº¿æŸ¥è¯¢ï¼š<10ms
- åœ¨çº¿æœç´¢ï¼š<500ms
- æ•°æ®åŒæ­¥ï¼šå¢é‡æ›´æ–°
- å­˜å‚¨å ç”¨ï¼š30-100MBï¼ˆå«ç¼“å­˜ï¼‰

è¿™ä¸ªæ–¹æ¡ˆå¯ä»¥è®©ä½ çš„3.7GBæ•°æ®å˜æˆé«˜æ•ˆå¯ç”¨çš„çŸ¥è¯†åº“ç³»ç»Ÿï¼