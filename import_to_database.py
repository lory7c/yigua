#!/usr/bin/env python3
"""
å°†å¤„ç†ç»“æœå¯¼å…¥æ•°æ®åº“ - çŸ¥è¯†åº“æ„å»º
å°†PDFæå–ç»“æœå¯¼å…¥SQLiteæ•°æ®åº“ï¼Œå®ŒæˆçŸ¥è¯†åº“æ„å»º
"""

import json
import sqlite3
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any
import hashlib

class DatabaseImporter:
    """æ•°æ®åº“å¯¼å…¥å™¨"""
    
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.conn = None
        self.logger = self._setup_logging()
        
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        logger = logging.getLogger("DatabaseImporter")
        logger.setLevel(logging.INFO)
        
        # æ¸…é™¤ç°æœ‰handler
        for handler in logger.handlers:
            logger.removeHandler(handler)
        
        # æ§åˆ¶å°handler
        console_handler = logging.StreamHandler()
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        console_handler.setFormatter(formatter)
        logger.addHandler(console_handler)
        
        return logger
    
    def connect_database(self):
        """è¿æ¥æ•°æ®åº“"""
        try:
            self.conn = sqlite3.connect(self.db_path)
            self.conn.row_factory = sqlite3.Row  # æ”¯æŒå­—å…¸è®¿é—®
            self.logger.info(f"å·²è¿æ¥æ•°æ®åº“: {self.db_path}")
            return True
        except Exception as e:
            self.logger.error(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            return False
    
    def create_tables(self):
        """åˆ›å»ºæ•°æ®è¡¨"""
        try:
            cursor = self.conn.cursor()
            
            # åˆ é™¤ç°æœ‰è¡¨
            tables_to_drop = [
                'pdf_documents', 'extracted_content', 'hexagrams', 
                'yao_content', 'cases', 'keywords', 'processing_stats'
            ]
            
            for table in tables_to_drop:
                cursor.execute(f"DROP TABLE IF EXISTS {table}")
            
            # 1. PDFæ–‡æ¡£åŸºç¡€ä¿¡æ¯è¡¨
            cursor.execute("""
            CREATE TABLE pdf_documents (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                file_name TEXT NOT NULL,
                file_path TEXT NOT NULL,
                file_size INTEGER,
                file_hash TEXT,
                category TEXT,
                method_used TEXT,
                processing_time REAL,
                processed_at TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
            """)
            
            # 2. æå–å†…å®¹è¡¨
            cursor.execute("""
            CREATE TABLE extracted_content (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                document_id INTEGER,
                raw_text TEXT,
                text_length INTEGER,
                word_count INTEGER,
                line_count INTEGER,
                FOREIGN KEY (document_id) REFERENCES pdf_documents (id)
            )
            """)
            
            # 3. å¦è±¡ä¿¡æ¯è¡¨
            cursor.execute("""
            CREATE TABLE hexagrams (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                document_id INTEGER,
                hexagram_name TEXT,
                description TEXT,
                position INTEGER,
                FOREIGN KEY (document_id) REFERENCES pdf_documents (id)
            )
            """)
            
            # 4. çˆ»è¾å†…å®¹è¡¨
            cursor.execute("""
            CREATE TABLE yao_content (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                document_id INTEGER,
                position TEXT,
                yao_type TEXT,
                description TEXT,
                location INTEGER,
                FOREIGN KEY (document_id) REFERENCES pdf_documents (id)
            )
            """)
            
            # 5. æ¡ˆä¾‹è¡¨
            cursor.execute("""
            CREATE TABLE cases (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                document_id INTEGER,
                case_content TEXT,
                case_length INTEGER,
                FOREIGN KEY (document_id) REFERENCES pdf_documents (id)
            )
            """)
            
            # 6. å…³é”®è¯è¡¨
            cursor.execute("""
            CREATE TABLE keywords (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                document_id INTEGER,
                keyword TEXT,
                FOREIGN KEY (document_id) REFERENCES pdf_documents (id)
            )
            """)
            
            # 7. å¤„ç†ç»Ÿè®¡è¡¨
            cursor.execute("""
            CREATE TABLE processing_stats (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                batch_id TEXT,
                total_files INTEGER,
                successful_files INTEGER,
                failed_files INTEGER,
                success_rate REAL,
                processing_time_minutes REAL,
                files_per_minute REAL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
            """)
            
            # åˆ›å»ºç´¢å¼•
            indexes = [
                "CREATE INDEX idx_documents_category ON pdf_documents (category)",
                "CREATE INDEX idx_documents_filename ON pdf_documents (file_name)",
                "CREATE INDEX idx_hexagrams_name ON hexagrams (hexagram_name)",
                "CREATE INDEX idx_keywords_keyword ON keywords (keyword)",
                "CREATE INDEX idx_yao_position ON yao_content (position, yao_type)"
            ]
            
            for index_sql in indexes:
                cursor.execute(index_sql)
            
            self.conn.commit()
            self.logger.info("æ•°æ®è¡¨åˆ›å»ºå®Œæˆ")
            return True
            
        except Exception as e:
            self.logger.error(f"åˆ›å»ºæ•°æ®è¡¨å¤±è´¥: {e}")
            return False
    
    def import_processing_results(self, results_file: str) -> bool:
        """å¯¼å…¥å¤„ç†ç»“æœ"""
        try:
            with open(results_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            cursor = self.conn.cursor()
            
            # å¯¼å…¥æ‰¹æ¬¡ç»Ÿè®¡ä¿¡æ¯
            batch_id = f"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            summary = data['summary']
            
            cursor.execute("""
            INSERT INTO processing_stats 
            (batch_id, total_files, successful_files, failed_files, success_rate, 
             processing_time_minutes, files_per_minute)
            VALUES (?, ?, ?, ?, ?, ?, ?)
            """, (
                batch_id,
                summary['total_files'],
                summary['successful_files'],
                summary['failed_files'],
                summary['success_rate'],
                summary['processing_time_minutes'],
                summary['files_per_minute']
            ))
            
            batch_stats_id = cursor.lastrowid
            self.logger.info(f"å¯¼å…¥æ‰¹æ¬¡ç»Ÿè®¡: {batch_id}")
            
            # å¯¼å…¥æ–‡æ¡£æ•°æ®
            successful_docs = 0
            failed_docs = 0
            
            for result in data['results']:
                if result['status'] != 'success':
                    failed_docs += 1
                    continue
                
                try:
                    # æ’å…¥æ–‡æ¡£åŸºç¡€ä¿¡æ¯
                    cursor.execute("""
                    INSERT INTO pdf_documents 
                    (file_name, file_path, file_size, category, method_used, 
                     processing_time, processed_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                    """, (
                        result['file_name'],
                        result['file_path'],
                        result.get('file_size', 0),
                        result['category'],
                        result['method_used'],
                        result['processing_time'],
                        result['processed_at']
                    ))
                    
                    doc_id = cursor.lastrowid
                    
                    # æ’å…¥æå–å†…å®¹
                    stats = result['statistics']
                    cursor.execute("""
                    INSERT INTO extracted_content 
                    (document_id, raw_text, text_length, word_count, line_count)
                    VALUES (?, ?, ?, ?, ?)
                    """, (
                        doc_id,
                        result.get('raw_text', ''),
                        stats['text_length'],
                        stats['word_count'],
                        stats['line_count']
                    ))
                    
                    # æ’å…¥ç»“æ„åŒ–å†…å®¹
                    structured = result['structured_content']
                    
                    # æ’å…¥å¦è±¡ä¿¡æ¯
                    for hexagram in structured.get('hexagrams', []):
                        cursor.execute("""
                        INSERT INTO hexagrams (document_id, hexagram_name, description, position)
                        VALUES (?, ?, ?, ?)
                        """, (
                            doc_id,
                            hexagram.get('name', ''),
                            hexagram.get('description', ''),
                            hexagram.get('position', 0)
                        ))
                    
                    # æ’å…¥çˆ»è¾ä¿¡æ¯
                    for yao in structured.get('yao_info', []):
                        cursor.execute("""
                        INSERT INTO yao_content (document_id, position, yao_type, description, location)
                        VALUES (?, ?, ?, ?, ?)
                        """, (
                            doc_id,
                            yao.get('position', ''),
                            yao.get('type', ''),
                            yao.get('description', ''),
                            yao.get('location', 0)
                        ))
                    
                    # æ’å…¥æ¡ˆä¾‹
                    for case in structured.get('cases', []):
                        if isinstance(case, str):
                            case_content = case
                        else:
                            case_content = case.get('content', str(case))
                        
                        cursor.execute("""
                        INSERT INTO cases (document_id, case_content, case_length)
                        VALUES (?, ?, ?)
                        """, (
                            doc_id,
                            case_content,
                            len(case_content)
                        ))
                    
                    # æ’å…¥å…³é”®è¯
                    for keyword in structured.get('keywords', []):
                        cursor.execute("""
                        INSERT INTO keywords (document_id, keyword)
                        VALUES (?, ?)
                        """, (doc_id, keyword))
                    
                    successful_docs += 1
                    
                except Exception as e:
                    self.logger.error(f"å¯¼å…¥æ–‡æ¡£å¤±è´¥ {result['file_name']}: {e}")
                    failed_docs += 1
                    continue
            
            self.conn.commit()
            
            self.logger.info(f"æ•°æ®å¯¼å…¥å®Œæˆ:")
            self.logger.info(f"  æˆåŠŸå¯¼å…¥: {successful_docs} ä¸ªæ–‡æ¡£")
            self.logger.info(f"  å¯¼å…¥å¤±è´¥: {failed_docs} ä¸ªæ–‡æ¡£")
            
            return True
            
        except Exception as e:
            self.logger.error(f"å¯¼å…¥å¤„ç†ç»“æœå¤±è´¥: {e}")
            return False
    
    def generate_database_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ•°æ®åº“æŠ¥å‘Š"""
        try:
            cursor = self.conn.cursor()
            
            # åŸºç¡€ç»Ÿè®¡
            cursor.execute("SELECT COUNT(*) FROM pdf_documents")
            total_documents = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(*) FROM extracted_content")
            total_content = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(*) FROM hexagrams")
            total_hexagrams = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(*) FROM yao_content")
            total_yao = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(*) FROM cases")
            total_cases = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(*) FROM keywords")
            total_keywords = cursor.fetchone()[0]
            
            # åˆ†ç±»ç»Ÿè®¡
            cursor.execute("""
            SELECT category, COUNT(*) as count 
            FROM pdf_documents 
            GROUP BY category 
            ORDER BY count DESC
            """)
            category_stats = dict(cursor.fetchall())
            
            # æ–¹æ³•ç»Ÿè®¡
            cursor.execute("""
            SELECT method_used, COUNT(*) as count 
            FROM pdf_documents 
            GROUP BY method_used 
            ORDER BY count DESC
            """)
            method_stats = dict(cursor.fetchall())
            
            # çƒ­é—¨å…³é”®è¯
            cursor.execute("""
            SELECT keyword, COUNT(*) as frequency 
            FROM keywords 
            GROUP BY keyword 
            ORDER BY frequency DESC 
            LIMIT 20
            """)
            top_keywords = dict(cursor.fetchall())
            
            # å†…å®¹è´¨é‡ç»Ÿè®¡
            cursor.execute("""
            SELECT 
                AVG(text_length) as avg_text_length,
                AVG(word_count) as avg_word_count,
                MAX(text_length) as max_text_length,
                MIN(text_length) as min_text_length
            FROM extracted_content
            """)
            quality_stats = cursor.fetchone()
            
            report = {
                "database_overview": {
                    "total_documents": total_documents,
                    "total_extracted_content": total_content,
                    "total_hexagrams": total_hexagrams,
                    "total_yao_content": total_yao,
                    "total_cases": total_cases,
                    "total_keywords": total_keywords
                },
                "category_distribution": category_stats,
                "method_distribution": method_stats,
                "top_keywords": top_keywords,
                "content_quality": {
                    "average_text_length": round(quality_stats[0] or 0, 2),
                    "average_word_count": round(quality_stats[1] or 0, 2),
                    "max_text_length": quality_stats[2] or 0,
                    "min_text_length": quality_stats[3] or 0
                },
                "generated_at": datetime.now().isoformat()
            }
            
            return report
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆæ•°æ®åº“æŠ¥å‘Šå¤±è´¥: {e}")
            return {}
    
    def close_connection(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.conn:
            self.conn.close()
            self.logger.info("æ•°æ®åº“è¿æ¥å·²å…³é—­")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ—„ï¸ å¼€å§‹å¯¼å…¥å¤„ç†ç»“æœåˆ°æ•°æ®åº“")
    print("=" * 50)
    
    # é…ç½®è·¯å¾„
    results_file = "/mnt/d/desktop/appp/structured_data/quick_results_20250808_080059.json"
    db_path = "/mnt/d/desktop/appp/database/yixue_knowledge_base.db"
    
    # ç¡®ä¿æ•°æ®åº“ç›®å½•å­˜åœ¨
    Path(db_path).parent.mkdir(parents=True, exist_ok=True)
    
    # æ£€æŸ¥ç»“æœæ–‡ä»¶
    if not Path(results_file).exists():
        print(f"âŒ ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {results_file}")
        return
    
    # åˆ›å»ºå¯¼å…¥å™¨å¹¶æ‰§è¡Œ
    importer = DatabaseImporter(db_path)
    
    try:
        # è¿æ¥æ•°æ®åº“
        if not importer.connect_database():
            print("âŒ æ•°æ®åº“è¿æ¥å¤±è´¥")
            return
        
        # åˆ›å»ºæ•°æ®è¡¨
        print("ğŸ“‹ åˆ›å»ºæ•°æ®è¡¨...")
        if not importer.create_tables():
            print("âŒ åˆ›å»ºæ•°æ®è¡¨å¤±è´¥")
            return
        
        # å¯¼å…¥æ•°æ®
        print("ğŸ“¥ å¯¼å…¥å¤„ç†ç»“æœ...")
        if not importer.import_processing_results(results_file):
            print("âŒ å¯¼å…¥æ•°æ®å¤±è´¥")
            return
        
        # ç”Ÿæˆæ•°æ®åº“æŠ¥å‘Š
        print("ğŸ“Š ç”Ÿæˆæ•°æ®åº“æŠ¥å‘Š...")
        report = importer.generate_database_report()
        
        if report:
            # ä¿å­˜æŠ¥å‘Š
            report_file = "/mnt/d/desktop/appp/database/database_report.json"
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            overview = report['database_overview']
            print(f"\nâœ… æ•°æ®åº“å¯¼å…¥å®Œæˆ!")
            print(f"ğŸ“„ æ–‡æ¡£æ€»æ•°: {overview['total_documents']}")
            print(f"ğŸ“ æå–å†…å®¹: {overview['total_extracted_content']} æ¡")
            print(f"ğŸ”® å¦è±¡ä¿¡æ¯: {overview['total_hexagrams']} ä¸ª")
            print(f"ğŸ“¿ çˆ»è¾å†…å®¹: {overview['total_yao_content']} æ¡")
            print(f"ğŸ“‹ æ¡ˆä¾‹è®°å½•: {overview['total_cases']} ä¸ª")
            print(f"ğŸ·ï¸ å…³é”®è¯: {overview['total_keywords']} ä¸ª")
            
            print(f"\nğŸ“š åˆ†ç±»åˆ†å¸ƒ:")
            for category, count in report['category_distribution'].items():
                print(f"   {category}: {count} ä¸ªæ–‡æ¡£")
            
            print(f"\nğŸ”¤ çƒ­é—¨å…³é”®è¯:")
            for keyword, freq in list(report['top_keywords'].items())[:10]:
                print(f"   {keyword}: å‡ºç° {freq} æ¬¡")
            
            print(f"\nğŸ“Š æ•°æ®åº“æ–‡ä»¶: {db_path}")
            print(f"ğŸ“„ æŠ¥å‘Šæ–‡ä»¶: {report_file}")
        
        print(f"\nğŸ‰ çŸ¥è¯†åº“æ„å»ºå®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        importer.close_connection()

if __name__ == "__main__":
    main()