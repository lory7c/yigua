#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®æ—¶æ€§èƒ½ç›‘æ§ç³»ç»Ÿ
ç›‘æ§CPUã€å†…å­˜ã€I/Oã€å“åº”æ—¶é—´ç­‰å…³é”®æŒ‡æ ‡
æä¾›å®æ—¶å‘Šè­¦å’Œæ€§èƒ½è¶‹åŠ¿åˆ†æ
"""

import asyncio
import psutil
import time
import json
import logging
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from collections import deque
import threading
import statistics
from pathlib import Path

from performance_optimizer import PerformanceOptimizer
from cache_strategy import MultiLevelCacheManager, CacheConfig


@dataclass
class SystemMetrics:
    """ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡"""
    timestamp: datetime
    cpu_percent: float
    memory_percent: float
    memory_used_mb: float
    memory_available_mb: float
    disk_usage_percent: float
    disk_read_mb_per_sec: float
    disk_write_mb_per_sec: float
    network_sent_mb_per_sec: float
    network_recv_mb_per_sec: float
    load_average: Optional[List[float]] = None


@dataclass
class DatabaseMetrics:
    """æ•°æ®åº“æ€§èƒ½æŒ‡æ ‡"""
    timestamp: datetime
    query_count: int
    avg_response_time_ms: float
    slow_query_count: int
    cache_hit_rate: float
    connection_count: int
    index_usage_count: int


@dataclass
class AlertConfig:
    """å‘Šè­¦é…ç½®"""
    cpu_threshold: float = 80.0
    memory_threshold: float = 85.0
    disk_threshold: float = 90.0
    response_time_threshold: float = 50.0
    cache_hit_rate_threshold: float = 0.7
    consecutive_alerts: int = 3  # è¿ç»­å‡ æ¬¡è¶…é˜ˆå€¼æ‰å‘Šè­¦


class RealTimeMonitor:
    """å®æ—¶æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self, db_path: str, alert_config: AlertConfig = None):
        self.db_path = db_path
        self.alert_config = alert_config or AlertConfig()
        self.logger = logging.getLogger(__name__)
        
        # æ€§èƒ½ä¼˜åŒ–å™¨
        self.db_optimizer = PerformanceOptimizer(db_path, enable_monitoring=True)
        
        # ç¼“å­˜ç®¡ç†å™¨
        cache_config = CacheConfig(l2_enabled=False)  # ä»…ä½¿ç”¨å†…å­˜ç¼“å­˜
        self.cache_manager = MultiLevelCacheManager(cache_config)
        
        # æ•°æ®å­˜å‚¨
        self.system_metrics = deque(maxlen=1440)  # 24å°æ—¶æ•°æ®ï¼ˆæ¯åˆ†é’Ÿä¸€ä¸ªç‚¹ï¼‰
        self.database_metrics = deque(maxlen=1440)
        self.alerts_history = deque(maxlen=1000)
        
        # ç›‘æ§çŠ¶æ€
        self.monitoring = False
        self.monitor_thread = None
        
        # å‘Šè­¦çŠ¶æ€è·Ÿè¸ª
        self.alert_counters = {
            'cpu': 0, 'memory': 0, 'disk': 0, 
            'response_time': 0, 'cache_hit_rate': 0
        }
        
        # åŸºçº¿æ•°æ®ï¼ˆç”¨äºå¼‚å¸¸æ£€æµ‹ï¼‰
        self.baselines = {}
        
    def start_monitoring(self, interval_seconds: int = 60):
        """å¯åŠ¨ç›‘æ§"""
        if self.monitoring:
            self.logger.warning("ç›‘æ§å·²åœ¨è¿è¡Œ")
            return
        
        self.monitoring = True
        self.monitor_thread = threading.Thread(
            target=self._monitor_loop,
            args=(interval_seconds,),
            daemon=True
        )
        self.monitor_thread.start()
        
        self.logger.info(f"å®æ—¶ç›‘æ§å·²å¯åŠ¨ï¼Œé‡‡é›†é—´éš”: {interval_seconds}ç§’")
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=5)
        
        self.logger.info("å®æ—¶ç›‘æ§å·²åœæ­¢")
    
    def _monitor_loop(self, interval_seconds: int):
        """ç›‘æ§å¾ªç¯"""
        last_disk_io = None
        last_network_io = None
        
        while self.monitoring:
            try:
                # æ”¶é›†ç³»ç»ŸæŒ‡æ ‡
                system_metrics = self._collect_system_metrics(last_disk_io, last_network_io)
                self.system_metrics.append(system_metrics)
                
                # æ›´æ–°I/OåŸºçº¿
                if system_metrics.disk_read_mb_per_sec > 0:
                    last_disk_io = {
                        'read_bytes': psutil.disk_io_counters().read_bytes,
                        'write_bytes': psutil.disk_io_counters().write_bytes,
                        'timestamp': system_metrics.timestamp
                    }
                
                if system_metrics.network_sent_mb_per_sec > 0:
                    network_io = psutil.net_io_counters()
                    last_network_io = {
                        'bytes_sent': network_io.bytes_sent,
                        'bytes_recv': network_io.bytes_recv,
                        'timestamp': system_metrics.timestamp
                    }
                
                # æ”¶é›†æ•°æ®åº“æŒ‡æ ‡
                db_metrics = self._collect_database_metrics()
                self.database_metrics.append(db_metrics)
                
                # æ£€æŸ¥å‘Šè­¦
                self._check_alerts(system_metrics, db_metrics)
                
                # æ›´æ–°åŸºçº¿
                self._update_baselines(system_metrics, db_metrics)
                
                time.sleep(interval_seconds)
                
            except Exception as e:
                self.logger.error(f"ç›‘æ§å¾ªç¯å¼‚å¸¸: {e}")
                time.sleep(interval_seconds)
    
    def _collect_system_metrics(self, last_disk_io: Optional[Dict] = None, 
                               last_network_io: Optional[Dict] = None) -> SystemMetrics:
        """æ”¶é›†ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡"""
        
        # CPUå’Œå†…å­˜
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        
        # ç£ç›˜ä½¿ç”¨
        disk = psutil.disk_usage('/')
        disk_usage_percent = (disk.used / disk.total) * 100
        
        # I/Oé€Ÿç‡è®¡ç®—
        disk_read_rate = 0.0
        disk_write_rate = 0.0
        
        try:
            current_disk_io = psutil.disk_io_counters()
            if last_disk_io and current_disk_io:
                time_diff = (datetime.now() - last_disk_io['timestamp']).total_seconds()
                if time_diff > 0:
                    disk_read_rate = (current_disk_io.read_bytes - last_disk_io['read_bytes']) / time_diff / 1024 / 1024
                    disk_write_rate = (current_disk_io.write_bytes - last_disk_io['write_bytes']) / time_diff / 1024 / 1024
        except:
            pass
        
        # ç½‘ç»œé€Ÿç‡è®¡ç®—
        network_sent_rate = 0.0
        network_recv_rate = 0.0
        
        try:
            current_network_io = psutil.net_io_counters()
            if last_network_io and current_network_io:
                time_diff = (datetime.now() - last_network_io['timestamp']).total_seconds()
                if time_diff > 0:
                    network_sent_rate = (current_network_io.bytes_sent - last_network_io['bytes_sent']) / time_diff / 1024 / 1024
                    network_recv_rate = (current_network_io.bytes_recv - last_network_io['bytes_recv']) / time_diff / 1024 / 1024
        except:
            pass
        
        # è´Ÿè½½å‡å€¼ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        load_avg = None
        if hasattr(psutil, 'getloadavg'):
            try:
                load_avg = list(psutil.getloadavg())
            except:
                pass
        
        return SystemMetrics(
            timestamp=datetime.now(),
            cpu_percent=cpu_percent,
            memory_percent=memory.percent,
            memory_used_mb=memory.used / 1024 / 1024,
            memory_available_mb=memory.available / 1024 / 1024,
            disk_usage_percent=disk_usage_percent,
            disk_read_mb_per_sec=max(0, disk_read_rate),
            disk_write_mb_per_sec=max(0, disk_write_rate),
            network_sent_mb_per_sec=max(0, network_sent_rate),
            network_recv_mb_per_sec=max(0, network_recv_rate),
            load_average=load_avg
        )
    
    def _collect_database_metrics(self) -> DatabaseMetrics:
        """æ”¶é›†æ•°æ®åº“æ€§èƒ½æŒ‡æ ‡"""
        
        # è·å–æ•°æ®åº“æ€§èƒ½æ•°æ®
        dashboard_data = self.db_optimizer.get_performance_dashboard_data()
        
        if 'error' in dashboard_data:
            # æ²¡æœ‰æ•°æ®æ—¶è¿”å›é»˜è®¤å€¼
            return DatabaseMetrics(
                timestamp=datetime.now(),
                query_count=0,
                avg_response_time_ms=0.0,
                slow_query_count=0,
                cache_hit_rate=0.0,
                connection_count=1,
                index_usage_count=0
            )
        
        summary = dashboard_data.get('summary', {})
        cache_stats = dashboard_data.get('cache_stats', {})
        
        return DatabaseMetrics(
            timestamp=datetime.now(),
            query_count=summary.get('total_queries', 0),
            avg_response_time_ms=summary.get('average_query_time_ms', 0.0),
            slow_query_count=summary.get('slow_queries_count', 0),
            cache_hit_rate=cache_stats.get('hit_rate', 0.0),
            connection_count=1,  # SQLiteå•è¿æ¥
            index_usage_count=len(summary.get('index_usage', []))
        )
    
    def _check_alerts(self, sys_metrics: SystemMetrics, db_metrics: DatabaseMetrics):
        """æ£€æŸ¥å‘Šè­¦æ¡ä»¶"""
        
        alerts_triggered = []
        
        # CPUå‘Šè­¦æ£€æŸ¥
        if sys_metrics.cpu_percent > self.alert_config.cpu_threshold:
            self.alert_counters['cpu'] += 1
        else:
            self.alert_counters['cpu'] = 0
        
        if self.alert_counters['cpu'] >= self.alert_config.consecutive_alerts:
            alerts_triggered.append({
                'type': 'cpu_high',
                'message': f'CPUä½¿ç”¨ç‡æŒç»­è¿‡é«˜: {sys_metrics.cpu_percent:.1f}%',
                'severity': 'warning',
                'timestamp': sys_metrics.timestamp,
                'value': sys_metrics.cpu_percent,
                'threshold': self.alert_config.cpu_threshold
            })
            self.alert_counters['cpu'] = 0  # é‡ç½®è®¡æ•°å™¨
        
        # å†…å­˜å‘Šè­¦æ£€æŸ¥
        if sys_metrics.memory_percent > self.alert_config.memory_threshold:
            self.alert_counters['memory'] += 1
        else:
            self.alert_counters['memory'] = 0
        
        if self.alert_counters['memory'] >= self.alert_config.consecutive_alerts:
            alerts_triggered.append({
                'type': 'memory_high',
                'message': f'å†…å­˜ä½¿ç”¨ç‡æŒç»­è¿‡é«˜: {sys_metrics.memory_percent:.1f}%',
                'severity': 'warning',
                'timestamp': sys_metrics.timestamp,
                'value': sys_metrics.memory_percent,
                'threshold': self.alert_config.memory_threshold
            })
            self.alert_counters['memory'] = 0
        
        # ç£ç›˜å‘Šè­¦æ£€æŸ¥
        if sys_metrics.disk_usage_percent > self.alert_config.disk_threshold:
            self.alert_counters['disk'] += 1
        else:
            self.alert_counters['disk'] = 0
        
        if self.alert_counters['disk'] >= self.alert_config.consecutive_alerts:
            alerts_triggered.append({
                'type': 'disk_full',
                'message': f'ç£ç›˜ä½¿ç”¨ç‡æŒç»­è¿‡é«˜: {sys_metrics.disk_usage_percent:.1f}%',
                'severity': 'critical',
                'timestamp': sys_metrics.timestamp,
                'value': sys_metrics.disk_usage_percent,
                'threshold': self.alert_config.disk_threshold
            })
            self.alert_counters['disk'] = 0
        
        # æ•°æ®åº“å“åº”æ—¶é—´å‘Šè­¦
        if db_metrics.avg_response_time_ms > self.alert_config.response_time_threshold:
            self.alert_counters['response_time'] += 1
        else:
            self.alert_counters['response_time'] = 0
        
        if self.alert_counters['response_time'] >= self.alert_config.consecutive_alerts:
            alerts_triggered.append({
                'type': 'slow_response',
                'message': f'æ•°æ®åº“å“åº”æ—¶é—´æŒç»­è¿‡æ…¢: {db_metrics.avg_response_time_ms:.1f}ms',
                'severity': 'warning',
                'timestamp': db_metrics.timestamp,
                'value': db_metrics.avg_response_time_ms,
                'threshold': self.alert_config.response_time_threshold
            })
            self.alert_counters['response_time'] = 0
        
        # ç¼“å­˜å‘½ä¸­ç‡å‘Šè­¦
        if db_metrics.cache_hit_rate < self.alert_config.cache_hit_rate_threshold:
            self.alert_counters['cache_hit_rate'] += 1
        else:
            self.alert_counters['cache_hit_rate'] = 0
        
        if self.alert_counters['cache_hit_rate'] >= self.alert_config.consecutive_alerts:
            alerts_triggered.append({
                'type': 'low_cache_hit',
                'message': f'ç¼“å­˜å‘½ä¸­ç‡æŒç»­è¿‡ä½: {db_metrics.cache_hit_rate:.1%}',
                'severity': 'info',
                'timestamp': db_metrics.timestamp,
                'value': db_metrics.cache_hit_rate,
                'threshold': self.alert_config.cache_hit_rate_threshold
            })
            self.alert_counters['cache_hit_rate'] = 0
        
        # è®°å½•è§¦å‘çš„å‘Šè­¦
        for alert in alerts_triggered:
            self.alerts_history.append(alert)
            self._log_alert(alert)
    
    def _log_alert(self, alert: Dict[str, Any]):
        """è®°å½•å‘Šè­¦"""
        severity_map = {
            'info': self.logger.info,
            'warning': self.logger.warning,
            'critical': self.logger.critical
        }
        
        log_func = severity_map.get(alert['severity'], self.logger.info)
        log_func(f"ğŸš¨ {alert['message']}")
    
    def _update_baselines(self, sys_metrics: SystemMetrics, db_metrics: DatabaseMetrics):
        """æ›´æ–°æ€§èƒ½åŸºçº¿"""
        
        # åªä¿ç•™æœ€è¿‘1å°æ—¶çš„æ•°æ®ç”¨äºåŸºçº¿è®¡ç®—
        recent_sys_metrics = [m for m in self.system_metrics if 
                             (datetime.now() - m.timestamp).total_seconds() < 3600]
        recent_db_metrics = [m for m in self.database_metrics if 
                           (datetime.now() - m.timestamp).total_seconds() < 3600]
        
        if len(recent_sys_metrics) >= 10:  # è‡³å°‘10ä¸ªæ•°æ®ç‚¹
            self.baselines['cpu_avg'] = statistics.mean([m.cpu_percent for m in recent_sys_metrics])
            self.baselines['memory_avg'] = statistics.mean([m.memory_percent for m in recent_sys_metrics])
            self.baselines['disk_io_avg'] = statistics.mean([
                m.disk_read_mb_per_sec + m.disk_write_mb_per_sec for m in recent_sys_metrics
            ])
        
        if len(recent_db_metrics) >= 10:
            response_times = [m.avg_response_time_ms for m in recent_db_metrics if m.avg_response_time_ms > 0]
            if response_times:
                self.baselines['response_time_avg'] = statistics.mean(response_times)
                self.baselines['response_time_p95'] = statistics.quantiles(response_times, n=20)[18] if len(response_times) >= 20 else max(response_times)
    
    def get_current_status(self) -> Dict[str, Any]:
        """è·å–å½“å‰çŠ¶æ€"""
        
        current_sys = self.system_metrics[-1] if self.system_metrics else None
        current_db = self.database_metrics[-1] if self.database_metrics else None
        
        # æœ€è¿‘1å°æ—¶çš„å‘Šè­¦
        recent_alerts = [alert for alert in self.alerts_history if 
                        (datetime.now() - alert['timestamp']).total_seconds() < 3600]
        
        status = {
            'timestamp': datetime.now().isoformat(),
            'monitoring_active': self.monitoring,
            'data_points': {
                'system_metrics': len(self.system_metrics),
                'database_metrics': len(self.database_metrics),
                'alerts_history': len(self.alerts_history)
            },
            'current_metrics': {
                'system': asdict(current_sys) if current_sys else None,
                'database': asdict(current_db) if current_db else None
            },
            'recent_alerts': recent_alerts,
            'baselines': self.baselines,
            'health_status': self._calculate_health_status()
        }
        
        return status
    
    def _calculate_health_status(self) -> Dict[str, str]:
        """è®¡ç®—ç³»ç»Ÿå¥åº·çŠ¶å†µ"""
        
        if not self.system_metrics or not self.database_metrics:
            return {'overall': 'unknown', 'reason': 'æ•°æ®ä¸è¶³'}
        
        current_sys = self.system_metrics[-1]
        current_db = self.database_metrics[-1]
        
        # å„å­ç³»ç»Ÿå¥åº·çŠ¶å†µ
        cpu_health = 'good' if current_sys.cpu_percent < 70 else 'warning' if current_sys.cpu_percent < 90 else 'critical'
        memory_health = 'good' if current_sys.memory_percent < 80 else 'warning' if current_sys.memory_percent < 95 else 'critical'
        disk_health = 'good' if current_sys.disk_usage_percent < 85 else 'warning' if current_sys.disk_usage_percent < 95 else 'critical'
        
        db_health = 'good'
        if current_db.avg_response_time_ms > 50:
            db_health = 'critical'
        elif current_db.avg_response_time_ms > 20:
            db_health = 'warning'
        
        cache_health = 'good'
        if current_db.cache_hit_rate < 0.5:
            cache_health = 'warning'
        elif current_db.cache_hit_rate < 0.3:
            cache_health = 'critical'
        
        # ç»¼åˆå¥åº·çŠ¶å†µ
        all_healths = [cpu_health, memory_health, disk_health, db_health, cache_health]
        
        if 'critical' in all_healths:
            overall = 'critical'
        elif 'warning' in all_healths:
            overall = 'warning'
        else:
            overall = 'good'
        
        return {
            'overall': overall,
            'cpu': cpu_health,
            'memory': memory_health,
            'disk': disk_health,
            'database': db_health,
            'cache': cache_health
        }
    
    def get_performance_trends(self, hours: int = 24) -> Dict[str, Any]:
        """è·å–æ€§èƒ½è¶‹åŠ¿åˆ†æ"""
        
        cutoff_time = datetime.now() - timedelta(hours=hours)
        
        # ç­›é€‰æ—¶é—´èŒƒå›´å†…çš„æ•°æ®
        sys_data = [m for m in self.system_metrics if m.timestamp > cutoff_time]
        db_data = [m for m in self.database_metrics if m.timestamp > cutoff_time]
        
        if not sys_data or not db_data:
            return {'error': f'æœ€è¿‘{hours}å°æ—¶å†…æ²¡æœ‰è¶³å¤Ÿæ•°æ®'}
        
        # ç³»ç»Ÿæ€§èƒ½è¶‹åŠ¿
        sys_trends = {
            'cpu': {
                'current': sys_data[-1].cpu_percent,
                'average': statistics.mean([m.cpu_percent for m in sys_data]),
                'peak': max([m.cpu_percent for m in sys_data]),
                'trend': self._calculate_trend([m.cpu_percent for m in sys_data[-10:]])
            },
            'memory': {
                'current': sys_data[-1].memory_percent,
                'average': statistics.mean([m.memory_percent for m in sys_data]),
                'peak': max([m.memory_percent for m in sys_data]),
                'trend': self._calculate_trend([m.memory_percent for m in sys_data[-10:]])
            }
        }
        
        # æ•°æ®åº“æ€§èƒ½è¶‹åŠ¿
        response_times = [m.avg_response_time_ms for m in db_data if m.avg_response_time_ms > 0]
        cache_rates = [m.cache_hit_rate for m in db_data if m.cache_hit_rate > 0]
        
        db_trends = {}
        if response_times:
            db_trends['response_time'] = {
                'current': response_times[-1],
                'average': statistics.mean(response_times),
                'p95': statistics.quantiles(response_times, n=20)[18] if len(response_times) >= 20 else max(response_times),
                'trend': self._calculate_trend(response_times[-10:])
            }
        
        if cache_rates:
            db_trends['cache_hit_rate'] = {
                'current': cache_rates[-1],
                'average': statistics.mean(cache_rates),
                'trend': self._calculate_trend(cache_rates[-10:])
            }
        
        return {
            'period_hours': hours,
            'data_points': len(sys_data),
            'system_trends': sys_trends,
            'database_trends': db_trends,
            'alerts_count': len([a for a in self.alerts_history if (datetime.now() - a['timestamp']).total_seconds() < hours * 3600])
        }
    
    def _calculate_trend(self, values: List[float]) -> str:
        """è®¡ç®—æ•°æ®è¶‹åŠ¿"""
        if len(values) < 3:
            return 'stable'
        
        # ç®€å•çš„çº¿æ€§è¶‹åŠ¿è®¡ç®—
        x = list(range(len(values)))
        y = values
        
        n = len(values)
        sum_x = sum(x)
        sum_y = sum(y)
        sum_xy = sum(xi * yi for xi, yi in zip(x, y))
        sum_x2 = sum(xi * xi for xi in x)
        
        if n * sum_x2 - sum_x * sum_x == 0:
            return 'stable'
        
        slope = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x * sum_x)
        
        # åˆ¤æ–­è¶‹åŠ¿
        if abs(slope) < 0.1:
            return 'stable'
        elif slope > 0:
            return 'increasing'
        else:
            return 'decreasing'
    
    def export_metrics(self, filepath: str, hours: int = 24):
        """å¯¼å‡ºæ€§èƒ½æŒ‡æ ‡æ•°æ®"""
        
        cutoff_time = datetime.now() - timedelta(hours=hours)
        
        export_data = {
            'export_info': {
                'timestamp': datetime.now().isoformat(),
                'period_hours': hours,
                'total_data_points': len(self.system_metrics) + len(self.database_metrics)
            },
            'system_metrics': [
                asdict(m) for m in self.system_metrics 
                if m.timestamp > cutoff_time
            ],
            'database_metrics': [
                asdict(m) for m in self.database_metrics 
                if m.timestamp > cutoff_time
            ],
            'alerts_history': [
                alert for alert in self.alerts_history 
                if alert['timestamp'] > cutoff_time
            ],
            'baselines': self.baselines
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, indent=2, ensure_ascii=False, default=str)
        
        self.logger.info(f"æ€§èƒ½æ•°æ®å·²å¯¼å‡º: {filepath}")
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self.stop_monitoring()
        
        if self.db_optimizer:
            self.db_optimizer.close()


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•
async def run_monitoring_demo(db_path: str, duration_minutes: int = 5):
    """è¿è¡Œç›‘æ§æ¼”ç¤º"""
    
    print(f"ğŸ” å¯åŠ¨å®æ—¶æ€§èƒ½ç›‘æ§æ¼”ç¤º (æŒç»­ {duration_minutes} åˆ†é’Ÿ)")
    
    # åˆ›å»ºå‘Šè­¦é…ç½®
    alert_config = AlertConfig(
        cpu_threshold=75.0,
        memory_threshold=80.0,
        response_time_threshold=20.0,
        cache_hit_rate_threshold=0.8,
        consecutive_alerts=2
    )
    
    monitor = RealTimeMonitor(db_path, alert_config)
    
    try:
        # å¯åŠ¨ç›‘æ§ (1åˆ†é’Ÿé—´éš”)
        monitor.start_monitoring(interval_seconds=10)  # æ¼”ç¤ºç”¨è¾ƒçŸ­é—´éš”
        
        # æ¨¡æ‹Ÿä¸€äº›æ•°æ®åº“æ´»åŠ¨
        for i in range(duration_minutes * 6):  # æ¯10ç§’ä¸€æ¬¡ï¼ŒæŒç»­æŒ‡å®šåˆ†é’Ÿæ•°
            
            # æ‰§è¡Œä¸€äº›æµ‹è¯•æŸ¥è¯¢
            try:
                monitor.db_optimizer.execute_query("SELECT COUNT(*) FROM test_data", (), "demo_query")
                monitor.db_optimizer.execute_query("SELECT * FROM test_data LIMIT 10", (), "demo_select")
            except:
                pass
            
            # æ˜¾ç¤ºå½“å‰çŠ¶æ€
            if i % 6 == 0:  # æ¯åˆ†é’Ÿæ˜¾ç¤ºä¸€æ¬¡
                status = monitor.get_current_status()
                current = status['current_metrics']
                
                if current['system'] and current['database']:
                    sys_metrics = current['system']
                    db_metrics = current['database']
                    
                    print(f"\nâ° {datetime.now().strftime('%H:%M:%S')} ç³»ç»ŸçŠ¶æ€:")
                    print(f"   CPU: {sys_metrics['cpu_percent']:.1f}%")
                    print(f"   å†…å­˜: {sys_metrics['memory_percent']:.1f}%")
                    print(f"   æ•°æ®åº“å“åº”: {db_metrics['avg_response_time_ms']:.2f}ms")
                    print(f"   ç¼“å­˜å‘½ä¸­ç‡: {db_metrics['cache_hit_rate']:.1%}")
                    
                    health = status['health_status']
                    health_emoji = {'good': 'âœ…', 'warning': 'âš ï¸', 'critical': 'âŒ'}
                    print(f"   ç³»ç»Ÿå¥åº·: {health_emoji.get(health['overall'], 'â“')} {health['overall']}")
                    
                    # æ˜¾ç¤ºæœ€è¿‘å‘Šè­¦
                    recent_alerts = status['recent_alerts']
                    if recent_alerts:
                        print(f"   æœ€è¿‘å‘Šè­¦: {len(recent_alerts)} æ¡")
                        for alert in recent_alerts[-2:]:  # æ˜¾ç¤ºæœ€è¿‘2æ¡
                            print(f"     - {alert['type']}: {alert['message']}")
            
            await asyncio.sleep(10)  # 10ç§’é—´éš”
        
        # è·å–æ€§èƒ½è¶‹åŠ¿
        print(f"\nğŸ“Š æ€§èƒ½è¶‹åŠ¿åˆ†æ:")
        trends = monitor.get_performance_trends(hours=1)  # æœ€è¿‘1å°æ—¶
        
        if 'system_trends' in trends:
            sys_trends = trends['system_trends']
            print(f"   CPUè¶‹åŠ¿: {sys_trends['cpu']['trend']} (å¹³å‡: {sys_trends['cpu']['average']:.1f}%)")
            print(f"   å†…å­˜è¶‹åŠ¿: {sys_trends['memory']['trend']} (å¹³å‡: {sys_trends['memory']['average']:.1f}%)")
        
        if 'database_trends' in trends:
            db_trends = trends['database_trends']
            if 'response_time' in db_trends:
                rt_trend = db_trends['response_time']
                print(f"   å“åº”æ—¶é—´è¶‹åŠ¿: {rt_trend['trend']} (å¹³å‡: {rt_trend['average']:.2f}ms)")
        
        # å¯¼å‡ºç›‘æ§æ•°æ®
        export_file = f"monitoring_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        monitor.export_metrics(export_file, hours=1)
        print(f"\nğŸ“ ç›‘æ§æ•°æ®å·²å¯¼å‡º: {export_file}")
        
    finally:
        monitor.cleanup()
        print("\nâœ… ç›‘æ§æ¼”ç¤ºå®Œæˆ")


if __name__ == "__main__":
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    # è¿è¡Œç›‘æ§æ¼”ç¤º
    db_path = "database/yijing_knowledge.db"
    
    if Path(db_path).exists():
        asyncio.run(run_monitoring_demo(db_path, duration_minutes=3))
    else:
        print(f"æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {db_path}")
        print("è¯·å…ˆåˆ›å»ºæˆ–æŒ‡å®šæ­£ç¡®çš„æ•°æ®åº“æ–‡ä»¶è·¯å¾„")