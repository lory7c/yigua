#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RAGç³»ç»Ÿé›†æˆæµ‹è¯•è„šæœ¬
æµ‹è¯•å‘é‡æ•°æ®åº“ã€LLMæœåŠ¡å’Œæ™ºèƒ½é—®ç­”åŠŸèƒ½
"""

import asyncio
import sys
import os
import json
import time
from pathlib import Path

# æ·»åŠ è·¯å¾„
sys.path.append(str(Path(__file__).parent))
sys.path.append(str(Path(__file__).parent / "knowledge_graph"))
sys.path.append(str(Path(__file__).parent / "config"))

from knowledge_graph.rag_engine import YixueQAEngine
from config.rag_config import create_config


class RAGSystemTester:
    """RAGç³»ç»Ÿæµ‹è¯•å™¨"""
    
    def __init__(self):
        self.test_results = []
        
    async def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("=" * 60)
        print("        æ˜“å­¦RAGé—®ç­”ç³»ç»Ÿ - é›†æˆæµ‹è¯•")
        print("=" * 60)
        
        # æµ‹è¯•ç”¨ä¾‹
        test_cases = [
            ("é…ç½®ç³»ç»Ÿæµ‹è¯•", self.test_config_system),
            ("æ•°æ®åº“è¿æ¥æµ‹è¯•", self.test_database_connection),
            ("å‘é‡å¼•æ“æµ‹è¯•", self.test_vector_engine),
            ("çŸ¥è¯†å›¾è°±æµ‹è¯•", self.test_knowledge_graph),
            ("é—®ç­”å¼•æ“æµ‹è¯•", self.test_qa_engine),
            ("æ€§èƒ½åŸºå‡†æµ‹è¯•", self.test_performance),
            ("å¤šè½®å¯¹è¯æµ‹è¯•", self.test_multi_turn_qa)
        ]
        
        for test_name, test_func in test_cases:
            print(f"\nğŸš€ å¼€å§‹ {test_name}...")
            try:
                start_time = time.time()
                result = await test_func()
                duration = time.time() - start_time
                
                if result['success']:
                    print(f"âœ… {test_name} é€šè¿‡ ({duration:.2f}s)")
                else:
                    print(f"âŒ {test_name} å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                
                result['duration'] = duration
                result['test_name'] = test_name
                self.test_results.append(result)
                
            except Exception as e:
                print(f"ğŸ’¥ {test_name} å¼‚å¸¸: {str(e)}")
                self.test_results.append({
                    'test_name': test_name,
                    'success': False,
                    'error': str(e),
                    'duration': 0
                })
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        self.generate_report()
    
    async def test_config_system(self) -> dict:
        """æµ‹è¯•é…ç½®ç³»ç»Ÿ"""
        try:
            # æµ‹è¯•ä¸åŒé…ç½®æ¨¡æ¿
            configs = {
                'development': create_config('development'),
                'production': create_config('production'),
                'local': create_config('local')
            }
            
            for name, config in configs.items():
                validation = config.validate_config()
                print(f"  {name}é…ç½®: {'âœ“' if validation['valid'] else 'âœ—'}")
                if validation['issues']:
                    for issue in validation['issues']:
                        print(f"    - {issue}")
            
            return {'success': True, 'configs_tested': len(configs)}
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    async def test_database_connection(self) -> dict:
        """æµ‹è¯•æ•°æ®åº“è¿æ¥"""
        try:
            config = create_config('development')
            db_path = config.get('database.path')
            
            if not Path(db_path).exists():
                return {'success': False, 'error': f'æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {db_path}'}
            
            # å°è¯•è¿æ¥æ•°æ®åº“
            import sqlite3
            with sqlite3.connect(db_path) as conn:
                cursor = conn.execute("SELECT COUNT(*) FROM hexagrams")
                hexagram_count = cursor.fetchone()[0]
                
                cursor = conn.execute("SELECT COUNT(*) FROM lines")
                line_count = cursor.fetchone()[0]
                
                cursor = conn.execute("SELECT COUNT(*) FROM interpretations")
                interpretation_count = cursor.fetchone()[0]
            
            print(f"  æ•°æ®ç»Ÿè®¡: å¦è±¡{hexagram_count}ä¸ª, çˆ»ä½{line_count}ä¸ª, æ³¨è§£{interpretation_count}æ¡")
            
            return {
                'success': True,
                'hexagrams': hexagram_count,
                'lines': line_count,
                'interpretations': interpretation_count
            }
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    async def test_vector_engine(self) -> dict:
        """æµ‹è¯•å‘é‡å¼•æ“"""
        try:
            config = create_config('development', {
                'vector_engine.qdrant.enabled': False  # å¼€å‘ç¯å¢ƒä¸éœ€è¦Qdrant
            })
            
            from knowledge_graph.vector_engine import YixueVectorEngine
            
            vector_config = config.get_vector_engine_config()
            vector_engine = YixueVectorEngine(
                db_path=config.get('database.path'),
                **vector_config
            )
            
            # æå–å’Œå‘é‡åŒ–æ–‡æ¡£
            print("  æ­£åœ¨æå–æ–‡æ¡£...")
            vector_engine.extract_documents()
            doc_count = len(vector_engine.documents)
            
            print("  æ­£åœ¨å‘é‡åŒ–æ–‡æ¡£...")
            vector_engine.vectorize_documents()
            
            print("  æ­£åœ¨æ„å»ºç´¢å¼•...")
            vector_engine.build_vector_index()
            
            # æµ‹è¯•æœç´¢
            print("  æµ‹è¯•è¯­ä¹‰æœç´¢...")
            results = vector_engine.semantic_search("ä¹¾å¦çš„å«ä¹‰", top_k=3)
            
            print(f"  æ–‡æ¡£æ•°é‡: {doc_count}")
            print(f"  æœç´¢ç»“æœ: {len(results)}ä¸ª")
            print(f"  å‘é‡ç»´åº¦: {vector_engine.embedding_dim}")
            print(f"  ä½¿ç”¨æ¨¡å‹: {'transformer' if vector_engine.use_transformer else 'tfidf'}")
            
            return {
                'success': True,
                'documents': doc_count,
                'search_results': len(results),
                'embedding_dim': vector_engine.embedding_dim,
                'model_type': 'transformer' if vector_engine.use_transformer else 'tfidf'
            }
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    async def test_knowledge_graph(self) -> dict:
        """æµ‹è¯•çŸ¥è¯†å›¾è°±"""
        try:
            config = create_config('development')
            
            from knowledge_graph.graph_builder import YixueKnowledgeGraphBuilder
            
            graph_builder = YixueKnowledgeGraphBuilder(
                config.get('database.path'),
                config.get('knowledge_graph.output_dir')
            )
            
            # æ„å»ºå›¾è°±
            print("  æ­£åœ¨æ„å»ºçŸ¥è¯†å›¾è°±...")
            graph_files = graph_builder.build_complete_graph()
            
            node_count = graph_builder.graph.number_of_nodes()
            edge_count = graph_builder.graph.number_of_edges()
            
            # æµ‹è¯•æŸ¥è¯¢
            print("  æµ‹è¯•å›¾è°±æŸ¥è¯¢...")
            sample_entities = list(graph_builder.entities.keys())[:3]
            query_results = []
            
            for entity_id in sample_entities:
                neighbors = graph_builder.query_neighbors(entity_id, max_depth=1)
                query_results.append(len(neighbors))
            
            print(f"  å›¾è°±èŠ‚ç‚¹: {node_count}ä¸ª")
            print(f"  å›¾è°±è¾¹: {edge_count}æ¡")
            print(f"  æŸ¥è¯¢æµ‹è¯•: {len(query_results)}ä¸ªå®ä½“")
            
            return {
                'success': True,
                'nodes': node_count,
                'edges': edge_count,
                'files_generated': len(graph_files),
                'sample_queries': query_results
            }
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    async def test_qa_engine(self) -> dict:
        """æµ‹è¯•é—®ç­”å¼•æ“"""
        try:
            # ä½¿ç”¨æ¨¡æ¿æ¨¡å¼è¿›è¡Œæµ‹è¯•ï¼ˆä¸ä¾èµ–LLM APIï¼‰
            config = create_config('development', {
                'llm.use_llm': False,
                'llm.default_provider': 'template'
            })
            
            qa_engine = YixueQAEngine(
                config.get('database.path'),
                config.get('knowledge_graph.output_dir'),
                config.get_llm_config()
            )
            
            # æ„å»ºç³»ç»Ÿ
            print("  æ­£åœ¨æ„å»ºé—®ç­”ç³»ç»Ÿ...")
            system_files = await qa_engine.build_system()
            
            # æµ‹è¯•é—®ç­”
            test_questions = [
                "ä¹¾å¦çš„å«ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ",
                "äº”è¡Œç›¸ç”Ÿçš„é¡ºåºæ˜¯æ€æ ·çš„ï¼Ÿ",
                "ä»€ä¹ˆæ˜¯é˜´é˜³ï¼Ÿ",
                "å…«å¦éƒ½æœ‰å“ªäº›ï¼Ÿ"
            ]
            
            responses = []
            print(f"  æµ‹è¯• {len(test_questions)} ä¸ªé—®é¢˜...")
            
            for i, question in enumerate(test_questions, 1):
                print(f"    é—®é¢˜{i}: {question}")
                response = await qa_engine.query(question)
                responses.append(response)
                print(f"    ç½®ä¿¡åº¦: {response.confidence:.3f}")
                print(f"    ç­”æ¡ˆé•¿åº¦: {len(response.answer)}å­—ç¬¦")
            
            # è·å–ç»Ÿè®¡ä¿¡æ¯
            stats = qa_engine.get_stats()
            
            return {
                'success': True,
                'system_files': len(system_files),
                'questions_tested': len(test_questions),
                'avg_confidence': stats['avg_confidence'],
                'success_rate': stats['success_rate'],
                'responses': [r.to_dict() for r in responses]
            }
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    async def test_performance(self) -> dict:
        """æµ‹è¯•æ€§èƒ½"""
        try:
            config = create_config('development', {
                'llm.use_llm': False,
                'retrieval.top_k': 5  # å‡å°‘æ£€ç´¢æ•°é‡ä»¥åŠ å¿«æµ‹è¯•
            })
            
            qa_engine = YixueQAEngine(
                config.get('database.path'),
                config.get('knowledge_graph.output_dir'),
                config.get_llm_config()
            )
            
            # å¿«é€Ÿæ„å»ºï¼ˆå¦‚æœè¿˜æœªæ„å»ºï¼‰
            if not qa_engine.vector_engine.documents:
                await qa_engine.build_system()
            
            # æ€§èƒ½æµ‹è¯•
            test_questions = [
                "ä¹¾å¦", "äº”è¡Œ", "é˜´é˜³", "å…«å¦", "å¤ªæ"
            ]
            
            start_time = time.time()
            for question in test_questions:
                await qa_engine.query(question)
            total_time = time.time() - start_time
            
            avg_time = total_time / len(test_questions)
            qps = len(test_questions) / total_time
            
            stats = qa_engine.get_stats()
            
            print(f"  æ€»è€—æ—¶: {total_time:.2f}ç§’")
            print(f"  å¹³å‡å“åº”æ—¶é—´: {avg_time:.3f}ç§’")
            print(f"  QPS: {qps:.2f}")
            
            return {
                'success': True,
                'total_time': total_time,
                'avg_response_time': avg_time,
                'qps': qps,
                'stats': stats
            }
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    async def test_multi_turn_qa(self) -> dict:
        """æµ‹è¯•å¤šè½®å¯¹è¯"""
        try:
            config = create_config('development', {
                'llm.use_llm': False
            })
            
            qa_engine = YixueQAEngine(
                config.get('database.path'),
                config.get('knowledge_graph.output_dir'),
                config.get_llm_config()
            )
            
            # å¦‚æœç³»ç»Ÿæœªæ„å»ºï¼Œå…ˆæ„å»º
            if not qa_engine.vector_engine.documents:
                await qa_engine.build_system()
            
            # æ¨¡æ‹Ÿå¤šè½®å¯¹è¯
            conversation = [
                "ä»€ä¹ˆæ˜¯ä¹¾å¦ï¼Ÿ",
                "å®ƒæœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ",
                "åœ¨å®é™…ç”Ÿæ´»ä¸­å¦‚ä½•åº”ç”¨ï¼Ÿ"
            ]
            
            responses = []
            for question in conversation:
                response = await qa_engine.query(question)
                responses.append({
                    'question': question,
                    'answer': response.answer[:100] + "..." if len(response.answer) > 100 else response.answer,
                    'confidence': response.confidence
                })
            
            print(f"  å®Œæˆ {len(conversation)} è½®å¯¹è¯")
            for i, r in enumerate(responses, 1):
                print(f"    ç¬¬{i}è½® - ç½®ä¿¡åº¦: {r['confidence']:.3f}")
            
            return {
                'success': True,
                'turns': len(conversation),
                'responses': responses,
                'avg_confidence': sum(r['confidence'] for r in responses) / len(responses)
            }
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def generate_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "=" * 60)
        print("                    æµ‹è¯•æŠ¥å‘Š")
        print("=" * 60)
        
        passed = sum(1 for r in self.test_results if r['success'])
        total = len(self.test_results)
        total_time = sum(r['duration'] for r in self.test_results)
        
        print(f"\nğŸ“Š æ€»ä½“ç»“æœ:")
        print(f"   é€šè¿‡: {passed}/{total} ({passed/total*100:.1f}%)")
        print(f"   æ€»è€—æ—¶: {total_time:.2f}ç§’")
        
        print(f"\nğŸ“‹ è¯¦ç»†ç»“æœ:")
        for result in self.test_results:
            status = "âœ…" if result['success'] else "âŒ"
            print(f"   {status} {result['test_name']} ({result['duration']:.2f}s)")
            if not result['success']:
                print(f"      é”™è¯¯: {result.get('error', 'æœªçŸ¥')}")
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_file = Path(__file__).parent / "test_results" / f"rag_test_report_{int(time.time())}.json"
        report_file.parent.mkdir(exist_ok=True)
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump({
                'summary': {
                    'total_tests': total,
                    'passed_tests': passed,
                    'success_rate': passed / total,
                    'total_duration': total_time
                },
                'results': self.test_results,
                'timestamp': time.time()
            }, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"\nğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        if passed == total:
            print(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼RAGç³»ç»Ÿå·²æˆåŠŸé›†æˆã€‚")
        else:
            print(f"\nâš ï¸  {total - passed} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³é…ç½®ã€‚")


async def main():
    """ä¸»å‡½æ•°"""
    tester = RAGSystemTester()
    await tester.run_all_tests()


if __name__ == "__main__":
    asyncio.run(main())